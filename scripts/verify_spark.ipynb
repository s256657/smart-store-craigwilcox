{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4603db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: py4j in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "! pip install pyspark\n",
    "! pip install py4j\n",
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21a19125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x112e29550>\n"
     ]
    }
   ],
   "source": [
    "# Verify Spark works\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SmartSales\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eeb447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:30:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/04/11 09:30:51 DEBUG SparkSession: Configurations that might not take effect:\n",
      "  spark.app.name=sales_data_preparedcsv\n",
      "25/04/11 09:30:51 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "25/04/11 09:30:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:30:51 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:30:51 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#659\n",
      "25/04/11 09:30:51 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#666\n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#659, None)) > 0)\n",
      "25/04/11 09:30:51 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 351.9 KiB, free 365.9 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_29 locally took 1 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_29 without replication took 1 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.9 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_29_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.3 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_29_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_29_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_29_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_29_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 29 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:30:51 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 76 took 0.000131 seconds\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Got job 18 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Final stage: ResultStage 18 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitStage(ResultStage 18 (name=load at NativeMethodAccessorImpl.java:0;jobs=18))\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[76] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitMissingTasks(ResultStage 18)\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 13.5 KiB, free 365.9 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_30 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_30 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.9 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_30_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on macbookpro.lan:57375 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_30_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_30_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_30_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_30_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[76] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Epoch for TaskSet 18.0: 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Valid locality levels for TaskSet 18.0: NO_PREF, ANY\n",
      "25/04/11 09:30:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_18.0, runningTasks: 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:30:51 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (18, 0) -> 1\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_30\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_30 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:30:51 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_29\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_29 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 1666 bytes result sent to driver\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (18, 0) -> 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 7 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:30:51 INFO DAGScheduler: ResultStage 18 (load at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: After removal of stage 18, remaining stages = 0\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 18 finished: load at NativeMethodAccessorImpl.java:0, took 0.013932 s\n",
      "25/04/11 09:30:51 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 351.9 KiB, free 365.5 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_31 locally took 1 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_31 without replication took 1 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.5 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_31_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_31_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_31_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_31_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_31_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 31 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$inferFromDataset$2\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$2\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$2) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$3\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$3) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:30:51 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 82 took 0.000089 seconds\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Got job 19 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Final stage: ResultStage 19 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitStage(ResultStage 19 (name=load at NativeMethodAccessorImpl.java:0;jobs=19))\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[82] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitMissingTasks(ResultStage 19)\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 28.2 KiB, free 365.5 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_32 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_32 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 365.5 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_32_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on macbookpro.lan:57375 (size: 13.0 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_32_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_32_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_32_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_32_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[82] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Epoch for TaskSet 19.0: 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Valid locality levels for TaskSet 19.0: NO_PREF, ANY\n",
      "25/04/11 09:30:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_19.0, runningTasks: 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:30:51 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (19, 0) -> 1\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_32\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_32 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:30:51 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:30:51 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_31\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_31 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1656 bytes result sent to driver\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (19, 0) -> 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 16 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:30:51 INFO DAGScheduler: ResultStage 19 (load at NativeMethodAccessorImpl.java:0) finished in 0.020 s\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: After removal of stage 19, remaining stages = 0\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 19 finished: load at NativeMethodAccessorImpl.java:0, took 0.023818 s\n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:30:51 INFO FileSourceStrategy: Post-Scan Filters: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "|transactionid|saledate|customerid|productid|storeid|campaignid|saleamount|loyaltypercentage|billtype|\n",
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "|          550|  1/6/24|      1008|      102|    404|         0|      39.1|                5|    Paid|\n",
      "|          551|  1/6/24|      1009|      105|    403|         0|     19.78|                5|    Paid|\n",
      "|          552| 1/16/24|      1004|      107|    404|         0|     335.1|                5|    Paid|\n",
      "|          553| 1/16/24|      1006|      102|    406|         0|     195.5|                5| Invoice|\n",
      "|          554| 1/25/24|      1005|      102|    405|         0|     117.3|                5| Invoice|\n",
      "|          555| 1/25/24|      1001|      101|    401|         0|   2379.36|               20| Invoice|\n",
      "|          556| 1/29/24|      1009|      104|    403|         0|     172.4|                5|    Paid|\n",
      "|          557| 1/29/24|      1010|      101|    402|         0|   3172.48|               20|  Credit|\n",
      "|          558|  2/6/24|      1002|      102|    402|         0|     312.8|                5| Invoice|\n",
      "|          559|  2/6/24|      1001|      106|    401|         0|    622.86|               10|  Credit|\n",
      "|          560|  2/6/24|      1010|      101|    402|         0|   6344.96|               20|  Credit|\n",
      "|          561|  2/6/24|      1005|      107|    405|         0|    469.14|                5|  Credit|\n",
      "|          562|  2/8/24|      1003|      108|    403|         0|     12.56|                5|    Paid|\n",
      "|          563|  2/8/24|      1006|      107|    406|         0|     67.02|                5| Invoice|\n",
      "|          564|  2/9/24|      1009|      107|    403|         0|    469.14|                5|    Paid|\n",
      "|          565|  2/9/24|      1002|      105|    402|         0|    138.46|                5| Invoice|\n",
      "|          566| 2/24/24|      1007|      103|    405|         0|    204.84|                5|    Paid|\n",
      "|          567| 2/24/24|      1007|      106|    405|         0|     444.9|                5|    Paid|\n",
      "|          568| 2/24/24|      1011|      107|    401|         0|    603.18|               10|    Paid|\n",
      "|          569| 2/24/24|      1010|      101|    402|         0|    3965.6|               20| Invoice|\n",
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:30:51 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       // common sub-expressions\n",
      "/* 029 */\n",
      "/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 031 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 032 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 033 */       UTF8String project_value_0;\n",
      "/* 034 */       if (inputadapter_isNull_0) {\n",
      "/* 035 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 036 */       } else {\n",
      "/* 037 */         project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));\n",
      "/* 038 */       }\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 042 */       UTF8String project_value_2;\n",
      "/* 043 */       if (inputadapter_isNull_1) {\n",
      "/* 044 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_2 = inputadapter_value_1;\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 049 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 050 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 051 */       UTF8String project_value_4;\n",
      "/* 052 */       if (inputadapter_isNull_2) {\n",
      "/* 053 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_4 = UTF8String.fromString(String.valueOf(inputadapter_value_2));\n",
      "/* 056 */       }\n",
      "/* 057 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 058 */       int inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 059 */       -1 : (inputadapter_row_0.getInt(3));\n",
      "/* 060 */       UTF8String project_value_6;\n",
      "/* 061 */       if (inputadapter_isNull_3) {\n",
      "/* 062 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 063 */       } else {\n",
      "/* 064 */         project_value_6 = UTF8String.fromString(String.valueOf(inputadapter_value_3));\n",
      "/* 065 */       }\n",
      "/* 066 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n",
      "/* 067 */       int inputadapter_value_4 = inputadapter_isNull_4 ?\n",
      "/* 068 */       -1 : (inputadapter_row_0.getInt(4));\n",
      "/* 069 */       UTF8String project_value_8;\n",
      "/* 070 */       if (inputadapter_isNull_4) {\n",
      "/* 071 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 072 */       } else {\n",
      "/* 073 */         project_value_8 = UTF8String.fromString(String.valueOf(inputadapter_value_4));\n",
      "/* 074 */       }\n",
      "/* 075 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n",
      "/* 076 */       int inputadapter_value_5 = inputadapter_isNull_5 ?\n",
      "/* 077 */       -1 : (inputadapter_row_0.getInt(5));\n",
      "/* 078 */       UTF8String project_value_10;\n",
      "/* 079 */       if (inputadapter_isNull_5) {\n",
      "/* 080 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 081 */       } else {\n",
      "/* 082 */         project_value_10 = UTF8String.fromString(String.valueOf(inputadapter_value_5));\n",
      "/* 083 */       }\n",
      "/* 084 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n",
      "/* 085 */       double inputadapter_value_6 = inputadapter_isNull_6 ?\n",
      "/* 086 */       -1.0 : (inputadapter_row_0.getDouble(6));\n",
      "/* 087 */       UTF8String project_value_12;\n",
      "/* 088 */       if (inputadapter_isNull_6) {\n",
      "/* 089 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 090 */       } else {\n",
      "/* 091 */         project_value_12 = UTF8String.fromString(String.valueOf(inputadapter_value_6));\n",
      "/* 092 */       }\n",
      "/* 093 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n",
      "/* 094 */       int inputadapter_value_7 = inputadapter_isNull_7 ?\n",
      "/* 095 */       -1 : (inputadapter_row_0.getInt(7));\n",
      "/* 096 */       UTF8String project_value_14;\n",
      "/* 097 */       if (inputadapter_isNull_7) {\n",
      "/* 098 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 099 */       } else {\n",
      "/* 100 */         project_value_14 = UTF8String.fromString(String.valueOf(inputadapter_value_7));\n",
      "/* 101 */       }\n",
      "/* 102 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n",
      "/* 103 */       UTF8String inputadapter_value_8 = inputadapter_isNull_8 ?\n",
      "/* 104 */       null : (inputadapter_row_0.getUTF8String(8));\n",
      "/* 105 */       UTF8String project_value_16;\n",
      "/* 106 */       if (inputadapter_isNull_8) {\n",
      "/* 107 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 108 */       } else {\n",
      "/* 109 */         project_value_16 = inputadapter_value_8;\n",
      "/* 110 */       }\n",
      "/* 111 */       project_mutableStateArray_0[0].reset();\n",
      "/* 112 */\n",
      "/* 113 */       project_mutableStateArray_0[0].write(0, project_value_0);\n",
      "/* 114 */\n",
      "/* 115 */       project_mutableStateArray_0[0].write(1, project_value_2);\n",
      "/* 116 */\n",
      "/* 117 */       project_mutableStateArray_0[0].write(2, project_value_4);\n",
      "/* 118 */\n",
      "/* 119 */       project_mutableStateArray_0[0].write(3, project_value_6);\n",
      "/* 120 */\n",
      "/* 121 */       project_mutableStateArray_0[0].write(4, project_value_8);\n",
      "/* 122 */\n",
      "/* 123 */       project_mutableStateArray_0[0].write(5, project_value_10);\n",
      "/* 124 */\n",
      "/* 125 */       project_mutableStateArray_0[0].write(6, project_value_12);\n",
      "/* 126 */\n",
      "/* 127 */       project_mutableStateArray_0[0].write(7, project_value_14);\n",
      "/* 128 */\n",
      "/* 129 */       project_mutableStateArray_0[0].write(8, project_value_16);\n",
      "/* 130 */       append((project_mutableStateArray_0[0].getRow()));\n",
      "/* 131 */       if (shouldStop()) return;\n",
      "/* 132 */     }\n",
      "/* 133 */   }\n",
      "/* 134 */\n",
      "/* 135 */ }\n",
      "\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 351.8 KiB, free 365.1 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_33 locally took 1 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_33 without replication took 1 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 365.1 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_33_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_33_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_33_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_33_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_33_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 33 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:30:51 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:30:51 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 86 took 0.000124 seconds\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Got job 20 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Final stage: ResultStage 20 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitStage(ResultStage 20 (name=showString at NativeMethodAccessorImpl.java:0;jobs=20))\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[86] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: submitMissingTasks(ResultStage 20)\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 18.8 KiB, free 365.1 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_34 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_34 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 365.1 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_34_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:30:51 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on macbookpro.lan:57375 (size: 8.3 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:30:51 DEBUG BlockManagerMaster: Updated info of block broadcast_34_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Told master about block broadcast_34_piece0\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Put block broadcast_34_piece0 locally took 0 ms\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Putting block broadcast_34_piece0 without replication took 0 ms\n",
      "25/04/11 09:30:51 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[86] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Epoch for TaskSet 20.0: 0\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: Valid locality levels for TaskSet 20.0: NO_PREF, ANY\n",
      "25/04/11 09:30:51 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_20.0, runningTasks: 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:30:51 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:30:51 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (20, 0) -> 1\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_34\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_34 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:30:51 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true],input[2, int, true],input[3, int, true],input[4, int, true],input[5, int, true],input[6, double, true],input[7, int, true],input[8, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */     writeFields_0_0(i);\n",
      "/* 031 */     writeFields_0_1(i);\n",
      "/* 032 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 033 */   }\n",
      "/* 034 */\n",
      "/* 035 */\n",
      "/* 036 */   private void writeFields_0_1(InternalRow i) {\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_5 = i.isNullAt(5);\n",
      "/* 039 */     int value_5 = isNull_5 ?\n",
      "/* 040 */     -1 : (i.getInt(5));\n",
      "/* 041 */     if (isNull_5) {\n",
      "/* 042 */       mutableStateArray_0[0].setNullAt(5);\n",
      "/* 043 */     } else {\n",
      "/* 044 */       mutableStateArray_0[0].write(5, value_5);\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     boolean isNull_6 = i.isNullAt(6);\n",
      "/* 048 */     double value_6 = isNull_6 ?\n",
      "/* 049 */     -1.0 : (i.getDouble(6));\n",
      "/* 050 */     if (isNull_6) {\n",
      "/* 051 */       mutableStateArray_0[0].setNullAt(6);\n",
      "/* 052 */     } else {\n",
      "/* 053 */       mutableStateArray_0[0].write(6, value_6);\n",
      "/* 054 */     }\n",
      "/* 055 */\n",
      "/* 056 */     boolean isNull_7 = i.isNullAt(7);\n",
      "/* 057 */     int value_7 = isNull_7 ?\n",
      "/* 058 */     -1 : (i.getInt(7));\n",
      "/* 059 */     if (isNull_7) {\n",
      "/* 060 */       mutableStateArray_0[0].setNullAt(7);\n",
      "/* 061 */     } else {\n",
      "/* 062 */       mutableStateArray_0[0].write(7, value_7);\n",
      "/* 063 */     }\n",
      "/* 064 */\n",
      "/* 065 */     boolean isNull_8 = i.isNullAt(8);\n",
      "/* 066 */     UTF8String value_8 = isNull_8 ?\n",
      "/* 067 */     null : (i.getUTF8String(8));\n",
      "/* 068 */     if (isNull_8) {\n",
      "/* 069 */       mutableStateArray_0[0].setNullAt(8);\n",
      "/* 070 */     } else {\n",
      "/* 071 */       mutableStateArray_0[0].write(8, value_8);\n",
      "/* 072 */     }\n",
      "/* 073 */\n",
      "/* 074 */   }\n",
      "/* 075 */\n",
      "/* 076 */\n",
      "/* 077 */   private void writeFields_0_0(InternalRow i) {\n",
      "/* 078 */\n",
      "/* 079 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 080 */     int value_0 = isNull_0 ?\n",
      "/* 081 */     -1 : (i.getInt(0));\n",
      "/* 082 */     if (isNull_0) {\n",
      "/* 083 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 084 */     } else {\n",
      "/* 085 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 089 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 090 */     null : (i.getUTF8String(1));\n",
      "/* 091 */     if (isNull_1) {\n",
      "/* 092 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 093 */     } else {\n",
      "/* 094 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 098 */     int value_2 = isNull_2 ?\n",
      "/* 099 */     -1 : (i.getInt(2));\n",
      "/* 100 */     if (isNull_2) {\n",
      "/* 101 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 102 */     } else {\n",
      "/* 103 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     boolean isNull_3 = i.isNullAt(3);\n",
      "/* 107 */     int value_3 = isNull_3 ?\n",
      "/* 108 */     -1 : (i.getInt(3));\n",
      "/* 109 */     if (isNull_3) {\n",
      "/* 110 */       mutableStateArray_0[0].setNullAt(3);\n",
      "/* 111 */     } else {\n",
      "/* 112 */       mutableStateArray_0[0].write(3, value_3);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     boolean isNull_4 = i.isNullAt(4);\n",
      "/* 116 */     int value_4 = isNull_4 ?\n",
      "/* 117 */     -1 : (i.getInt(4));\n",
      "/* 118 */     if (isNull_4) {\n",
      "/* 119 */       mutableStateArray_0[0].setNullAt(4);\n",
      "/* 120 */     } else {\n",
      "/* 121 */       mutableStateArray_0[0].write(4, value_4);\n",
      "/* 122 */     }\n",
      "/* 123 */\n",
      "/* 124 */   }\n",
      "/* 125 */\n",
      "/* 126 */ }\n",
      "\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Getting local block broadcast_33\n",
      "25/04/11 09:30:51 DEBUG BlockManager: Level for block broadcast_33 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:30:51 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2495 bytes result sent to driver\n",
      "25/04/11 09:30:51 DEBUG ExecutorMetricsPoller: stageTCMP: (20, 0) -> 0\n",
      "25/04/11 09:30:51 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 16 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:30:51 INFO DAGScheduler: ResultStage 20 (showString at NativeMethodAccessorImpl.java:0) finished in 0.022 s\n",
      "25/04/11 09:30:51 DEBUG DAGScheduler: After removal of stage 20, remaining stages = 0\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:30:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "25/04/11 09:30:51 INFO DAGScheduler: Job 20 finished: showString at NativeMethodAccessorImpl.java:0, took 0.025254 s\n",
      "25/04/11 09:30:51 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, StructField(toprettystring(transactionid),StringType,false), StructField(toprettystring(saledate),StringType,false), StructField(toprettystring(customerid),StringType,false), StructField(toprettystring(productid),StringType,false), StructField(toprettystring(storeid),StringType,false), StructField(toprettystring(campaignid),StringType,false), StructField(toprettystring(saleamount),StringType,false), StructField(toprettystring(loyaltypercentage),StringType,false), StructField(toprettystring(billtype),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[9];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 076 */     boolean isNull_17 = true;\n",
      "/* 077 */     java.lang.String value_17 = null;\n",
      "/* 078 */     isNull_17 = false;\n",
      "/* 079 */     if (!isNull_17) {\n",
      "/* 080 */\n",
      "/* 081 */       Object funcResult_8 = null;\n",
      "/* 082 */       funcResult_8 = value_18.toString();\n",
      "/* 083 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 084 */\n",
      "/* 085 */     }\n",
      "/* 086 */     if (isNull_17) {\n",
      "/* 087 */       values_0[8] = null;\n",
      "/* 088 */     } else {\n",
      "/* 089 */       values_0[8] = value_17;\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */\n",
      "/* 095 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 098 */     boolean isNull_7 = true;\n",
      "/* 099 */     java.lang.String value_7 = null;\n",
      "/* 100 */     isNull_7 = false;\n",
      "/* 101 */     if (!isNull_7) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_3 = null;\n",
      "/* 104 */       funcResult_3 = value_8.toString();\n",
      "/* 105 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_7) {\n",
      "/* 109 */       values_0[3] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[3] = value_7;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 115 */     boolean isNull_9 = true;\n",
      "/* 116 */     java.lang.String value_9 = null;\n",
      "/* 117 */     isNull_9 = false;\n",
      "/* 118 */     if (!isNull_9) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_4 = null;\n",
      "/* 121 */       funcResult_4 = value_10.toString();\n",
      "/* 122 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_9) {\n",
      "/* 126 */       values_0[4] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[4] = value_9;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 132 */     boolean isNull_11 = true;\n",
      "/* 133 */     java.lang.String value_11 = null;\n",
      "/* 134 */     isNull_11 = false;\n",
      "/* 135 */     if (!isNull_11) {\n",
      "/* 136 */\n",
      "/* 137 */       Object funcResult_5 = null;\n",
      "/* 138 */       funcResult_5 = value_12.toString();\n",
      "/* 139 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 140 */\n",
      "/* 141 */     }\n",
      "/* 142 */     if (isNull_11) {\n",
      "/* 143 */       values_0[5] = null;\n",
      "/* 144 */     } else {\n",
      "/* 145 */       values_0[5] = value_11;\n",
      "/* 146 */     }\n",
      "/* 147 */\n",
      "/* 148 */   }\n",
      "/* 149 */\n",
      "/* 150 */\n",
      "/* 151 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 154 */     boolean isNull_1 = true;\n",
      "/* 155 */     java.lang.String value_1 = null;\n",
      "/* 156 */     isNull_1 = false;\n",
      "/* 157 */     if (!isNull_1) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_0 = null;\n",
      "/* 160 */       funcResult_0 = value_2.toString();\n",
      "/* 161 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_1) {\n",
      "/* 165 */       values_0[0] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[0] = value_1;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 171 */     boolean isNull_3 = true;\n",
      "/* 172 */     java.lang.String value_3 = null;\n",
      "/* 173 */     isNull_3 = false;\n",
      "/* 174 */     if (!isNull_3) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_1 = null;\n",
      "/* 177 */       funcResult_1 = value_4.toString();\n",
      "/* 178 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_3) {\n",
      "/* 182 */       values_0[1] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[1] = value_3;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 188 */     boolean isNull_5 = true;\n",
      "/* 189 */     java.lang.String value_5 = null;\n",
      "/* 190 */     isNull_5 = false;\n",
      "/* 191 */     if (!isNull_5) {\n",
      "/* 192 */\n",
      "/* 193 */       Object funcResult_2 = null;\n",
      "/* 194 */       funcResult_2 = value_6.toString();\n",
      "/* 195 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 196 */\n",
      "/* 197 */     }\n",
      "/* 198 */     if (isNull_5) {\n",
      "/* 199 */       values_0[2] = null;\n",
      "/* 200 */     } else {\n",
      "/* 201 */       values_0[2] = value_5;\n",
      "/* 202 */     }\n",
      "/* 203 */\n",
      "/* 204 */   }\n",
      "/* 205 */\n",
      "/* 206 */ }\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:30:55 DEBUG ExecutorMetricsPoller: removing (18, 0) from stageTCMP\n",
      "25/04/11 09:30:55 DEBUG ExecutorMetricsPoller: removing (19, 0) from stageTCMP\n",
      "25/04/11 09:30:55 DEBUG ExecutorMetricsPoller: removing (20, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "# Sales Table\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sales_data_preparedcsv\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_path = \"/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_sales = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "# Show data\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3283aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:32:28 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "25/04/11 09:32:28 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:32:28 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:32:28 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#771\n",
      "25/04/11 09:32:28 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#778\n",
      "25/04/11 09:32:28 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:32:28 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#771, None)) > 0)\n",
      "25/04/11 09:32:28 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 351.9 KiB, free 365.5 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_39 locally took 1 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_39 without replication took 1 ms\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.5 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_39_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:32:28 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMaster: Updated info of block broadcast_39_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Told master about block broadcast_39_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_39_piece0 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_39_piece0 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO SparkContext: Created broadcast 39 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:32:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:32:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 100 took 0.000065 seconds\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Got job 23 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Final stage: ResultStage 23 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: submitStage(ResultStage 23 (name=load at NativeMethodAccessorImpl.java:0;jobs=23))\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[100] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: submitMissingTasks(ResultStage 23)\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 13.5 KiB, free 365.5 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_40 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_40 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.5 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_40_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:32:28 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on macbookpro.lan:57375 (size: 6.4 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMaster: Updated info of block broadcast_40_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Told master about block broadcast_40_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_40_piece0 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_40_piece0 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[100] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Epoch for TaskSet 23.0: 0\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Valid locality levels for TaskSet 23.0: NO_PREF, ANY\n",
      "25/04/11 09:32:28 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_23.0, runningTasks: 0\n",
      "25/04/11 09:32:28 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:32:28 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)\n",
      "25/04/11 09:32:28 DEBUG ExecutorMetricsPoller: stageTCMP: (23, 0) -> 1\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Getting local block broadcast_40\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Level for block broadcast_40 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:32:28 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:32:28 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Getting local block broadcast_39\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Level for block broadcast_39 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:32:28 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 1633 bytes result sent to driver\n",
      "25/04/11 09:32:28 DEBUG ExecutorMetricsPoller: stageTCMP: (23, 0) -> 0\n",
      "25/04/11 09:32:28 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 7 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:32:28 INFO DAGScheduler: ResultStage 23 (load at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: After removal of stage 23, remaining stages = 0\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Job 23 finished: load at NativeMethodAccessorImpl.java:0, took 0.013005 s\n",
      "25/04/11 09:32:28 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:32:28 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:32:28 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 351.9 KiB, free 365.1 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_41 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_41 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 365.1 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_41_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:32:28 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMaster: Updated info of block broadcast_41_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Told master about block broadcast_41_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_41_piece0 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_41_piece0 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO SparkContext: Created broadcast 41 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:32:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$inferFromDataset$2\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$2\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$2) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$3\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$3) is now cleaned +++\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:32:28 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:32:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 106 took 0.000061 seconds\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Got job 24 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Final stage: ResultStage 24 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: submitStage(ResultStage 24 (name=load at NativeMethodAccessorImpl.java:0;jobs=24))\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[106] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: submitMissingTasks(ResultStage 24)\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 28.2 KiB, free 365.1 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_42 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_42 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 365.1 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_42_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:32:28 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on macbookpro.lan:57375 (size: 12.9 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:32:28 DEBUG BlockManagerMaster: Updated info of block broadcast_42_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Told master about block broadcast_42_piece0\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Put block broadcast_42_piece0 locally took 0 ms\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Putting block broadcast_42_piece0 without replication took 0 ms\n",
      "25/04/11 09:32:28 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[106] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Epoch for TaskSet 24.0: 0\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: Valid locality levels for TaskSet 24.0: NO_PREF, ANY\n",
      "25/04/11 09:32:28 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_24.0, runningTasks: 0\n",
      "25/04/11 09:32:28 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:32:28 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:32:28 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)\n",
      "25/04/11 09:32:28 DEBUG ExecutorMetricsPoller: stageTCMP: (24, 0) -> 1\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Getting local block broadcast_42\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Level for block broadcast_42 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:32:28 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:32:28 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:32:28 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Getting local block broadcast_41\n",
      "25/04/11 09:32:28 DEBUG BlockManager: Level for block broadcast_41 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:32:28 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 1588 bytes result sent to driver\n",
      "25/04/11 09:32:28 DEBUG ExecutorMetricsPoller: stageTCMP: (24, 0) -> 0\n",
      "25/04/11 09:32:28 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 12 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:32:28 INFO DAGScheduler: ResultStage 24 (load at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "25/04/11 09:32:28 DEBUG DAGScheduler: After removal of stage 24, remaining stages = 0\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:32:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "25/04/11 09:32:28 INFO DAGScheduler: Job 24 finished: load at NativeMethodAccessorImpl.java:0, took 0.019650 s\n",
      "25/04/11 09:32:28 DEBUG SparkSqlParser: Parsing command: customer\n"
     ]
    }
   ],
   "source": [
    "# Customers Tables\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "\n",
    "# Path to your customer CSV file\n",
    "customer_csv_path = \"/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv\"\n",
    "\n",
    "# Load customer data into DataFrame\n",
    "df_customer = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customer_csv_path)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df_customer.createOrReplaceTempView(\"customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e548fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:37:21 DEBUG SparkSqlParser: Parsing command: sales\n",
      "25/04/11 09:37:21 DEBUG SparkSqlParser: Parsing command: customer\n",
      "25/04/11 09:37:21 DEBUG SparkSqlParser: Parsing command: \n",
      "    SELECT c.Name, SUM(s.saleamount) AS total_spent\n",
      "    FROM sales s\n",
      "    JOIN customer c ON s.customerid = c.CustomerID\n",
      "    GROUP BY c.Name\n",
      "    ORDER BY total_spent DESC\n",
      "\n",
      "25/04/11 09:37:21 DEBUG Analyzer$ResolveReferences: Resolving 's.customerid to customerid#678\n",
      "25/04/11 09:37:21 DEBUG Analyzer$ResolveReferences: Resolving 'c.CustomerID to CustomerID#788\n",
      "25/04/11 09:37:21 DEBUG ResolveReferencesInAggregate: Resolving 'c.Name to Name#789\n",
      "25/04/11 09:37:21 DEBUG ResolveReferencesInAggregate: Resolving 'c.Name to Name#789\n",
      "25/04/11 09:37:21 DEBUG ResolveReferencesInAggregate: Resolving 's.saleamount to saleamount#682\n",
      "25/04/11 09:37:21 DEBUG ResolveReferencesInSort: Resolving 'total_spent to total_spent#804\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#678)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: TakeOrderedAndProject(limit=21, orderBy=[total_spent#804 DESC NULLS LAST], output=[toprettystring(Name)#810,toprettystring(total_spent)#811])\n",
      "+- HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "   +- HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "      +- Project [saleamount#682, Name#789]\n",
      "         +- BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "            :- Project [customerid#678, saleamount#682]\n",
      "            :  +- Filter isnotnull(customerid#678)\n",
      "            :     +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "            +- Project [CustomerID#788, Name#789]\n",
      "               +- Filter isnotnull(CustomerID#788)\n",
      "                  +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:21 DEBUG BroadcastQueryStageExec: Materialize query stage BroadcastQueryStageExec: 0\n",
      "25/04/11 09:37:21 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:37:21 INFO CodeGenerator: Code generated in 9.267821 ms\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 351.8 KiB, free 364.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_43 locally took 1 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_43 without replication took 1 ms\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 364.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_43_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:21 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMaster: Updated info of block broadcast_43_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Told master about block broadcast_43_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_43_piece0 locally took 0 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_43_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:21 INFO SparkContext: Created broadcast 43 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:37:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:37:21 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:37:21 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 110 took 0.000054 seconds\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: submitStage(ResultStage 25 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=25))\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[110] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: submitMissingTasks(ResultStage 25)\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 15.5 KiB, free 364.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_44 locally took 0 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_44 without replication took 0 ms\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 364.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_44_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:21 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on macbookpro.lan:57375 (size: 7.6 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMaster: Updated info of block broadcast_44_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Told master about block broadcast_44_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_44_piece0 locally took 0 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_44_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:21 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[110] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:21 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:21 DEBUG TaskSetManager: Epoch for TaskSet 25.0: 0\n",
      "25/04/11 09:37:21 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:21 DEBUG TaskSetManager: Valid locality levels for TaskSet 25.0: NO_PREF, ANY\n",
      "25/04/11 09:37:21 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_25.0, runningTasks: 0\n",
      "25/04/11 09:37:21 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:37:21 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:37:21 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)\n",
      "25/04/11 09:37:21 DEBUG ExecutorMetricsPoller: stageTCMP: (25, 0) -> 1\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Getting local block broadcast_44\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Level for block broadcast_44 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:37:21 INFO CodeGenerator: Code generated in 8.647102 ms\n",
      "25/04/11 09:37:21 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:37:21 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:21 INFO CodeGenerator: Code generated in 9.716127 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Getting local block broadcast_43\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Level for block broadcast_43 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:21 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:37:21 INFO CodeGenerator: Code generated in 7.95969 ms\n",
      "25/04/11 09:37:21 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 1862 bytes result sent to driver\n",
      "25/04/11 09:37:21 DEBUG ExecutorMetricsPoller: stageTCMP: (25, 0) -> 0\n",
      "25/04/11 09:37:21 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 60 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:21 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:21 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.063 s\n",
      "25/04/11 09:37:21 DEBUG DAGScheduler: After removal of stage 25, remaining stages = 0\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:37:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/04/11 09:37:21 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.065663 s\n",
      "25/04/11 09:37:21 DEBUG TaskMemoryManager: Task 0 acquired 1024.3 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@101bb800\n",
      "25/04/11 09:37:21 DEBUG GenerateUnsafeProjection: code for cast(input[0, int, false] as bigint):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     int value_1 = i.getInt(0);\n",
      "/* 032 */     boolean isNull_0 = false;\n",
      "/* 033 */     long value_0 = -1L;\n",
      "/* 034 */     if (!false) {\n",
      "/* 035 */       value_0 = (long) value_1;\n",
      "/* 036 */     }\n",
      "/* 037 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */ }\n",
      "\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     int value_1 = i.getInt(0);\n",
      "/* 032 */     boolean isNull_0 = false;\n",
      "/* 033 */     long value_0 = -1L;\n",
      "/* 034 */     if (!false) {\n",
      "/* 035 */       value_0 = (long) value_1;\n",
      "/* 036 */     }\n",
      "/* 037 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */ }\n",
      "\n",
      "25/04/11 09:37:21 INFO CodeGenerator: Code generated in 6.403872 ms\n",
      "25/04/11 09:37:21 DEBUG TaskMemoryManager: Task 0 acquired 512.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@101bb800\n",
      "25/04/11 09:37:21 DEBUG TaskMemoryManager: Task 0 release 256.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@101bb800\n",
      "25/04/11 09:37:21 DEBUG TaskMemoryManager: Task 0 acquired 88.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@101bb800\n",
      "25/04/11 09:37:21 DEBUG TaskMemoryManager: Task 0 release 512.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@101bb800\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 1024.1 KiB, free 363.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_45 locally took 0 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_45 without replication took 0 ms\n",
      "25/04/11 09:37:21 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 509.0 B, free 363.7 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_45_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:21 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on macbookpro.lan:57375 (size: 509.0 B, free: 366.1 MiB)\n",
      "25/04/11 09:37:21 DEBUG BlockManagerMaster: Updated info of block broadcast_45_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Told master about block broadcast_45_piece0\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Put block broadcast_45_piece0 locally took 0 ms\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Putting block broadcast_45_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:21 INFO SparkContext: Created broadcast 45 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:21 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:37:21 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#678)\n",
      "25/04/11 09:37:21 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 1\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Getting local block broadcast_45\n",
      "25/04/11 09:37:21 DEBUG BlockManager: Level for block broadcast_45 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:21 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:37:21 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 56.959002 ms\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 351.8 KiB, free 363.3 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_46 locally took 2 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_46 without replication took 2 ms\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 363.3 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_46_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_46_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_46_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_46_piece0 locally took 0 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_46_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:22 INFO SparkContext: Created broadcast 46 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:37:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1434)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1434\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1434\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1321)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1321\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1321\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1211)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1211\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1211\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1430)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1430\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1430\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1436)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1436\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1436\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1224)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1224\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1224\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1444)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1444\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1444\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1240)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1240\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1240\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1475)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1475\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1475\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1334)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1334\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1334\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1207)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1207\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1207\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1329)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1329\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1329\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1259)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1259\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1259\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1249)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1249\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1249\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1201)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1201\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1201\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1244)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1244\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1244\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1268)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1268\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1268\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1425)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1425\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1425\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1454)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1454\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1454\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1231)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1231\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1231\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1413)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1413\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1413\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1490)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1490\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1490\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1282)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1282\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1282\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1505)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1505\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1505\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1264)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1264\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1264\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1504)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1504\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1504\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1503)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1503\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1503\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1252)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1252\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1252\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1311)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1311\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1311\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1495)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1495\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1495\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1303)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1303\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1303\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1441)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1441\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1441\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1426)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1426\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1426\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1502)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1502\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1502\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1437)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1437\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1437\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1214)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1214\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1214\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1218)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1218\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1218\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1229)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1229\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1229\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1237)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1237\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1237\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1223)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1223\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1223\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(40)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 40\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 40\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 40\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 40\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_40_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_40_piece0 of size 6542 dropped from memory (free 380943136)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_40_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_40_piece0 on macbookpro.lan:57375 in memory (size: 6.4 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_40_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_40_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_40\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_40 of size 13792 dropped from memory (free 380956928)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 40, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 40\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1304)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1304\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1304\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1276)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1276\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1276\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1280)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1280\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1280\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1428)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1428\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1428\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1270)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1270\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1270\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1339)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1339\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1339\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1273)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1273\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1273\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(44)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 44\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 44\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 44\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 44\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_44_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_44_piece0 of size 7816 dropped from memory (free 380964744)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_44_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_44_piece0 on macbookpro.lan:57375 in memory (size: 7.6 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_44_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_44_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_44\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_44 of size 15896 dropped from memory (free 380980640)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 44, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 44\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1324)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1324\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1324\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1235)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1235\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1235\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1317)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1317\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1317\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1228)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1228\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1228\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1242)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1242\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1242\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1295)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1295\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1295\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1232)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1232\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1232\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1414)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1414\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1414\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1474)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1474\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1474\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1297)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1297\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1297\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1217)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1217\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1217\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1243)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1243\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1243\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(37)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 37\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 37\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 37\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 37\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_37_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_37_piece0 of size 35646 dropped from memory (free 381016286)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_37_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_37_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_37_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_37_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_37\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_37 of size 360328 dropped from memory (free 381376614)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 37, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 37\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1287)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1287\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1287\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1452)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1452\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1452\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1318)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1318\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1318\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1498)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1498\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1498\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1238)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1238\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1238\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1446)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1446\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1446\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1221)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1221\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1221\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1279)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1279\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1279\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1420)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1420\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1420\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1496)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1496\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1496\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1419)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1419\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1419\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1488)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1488\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1488\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1494)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1494\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1494\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1222)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1222\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1222\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1322)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1322\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1322\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1493)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1493\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1493\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1241)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1241\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1241\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1204)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1204\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1204\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1310)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1310\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1310\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1423)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1423\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1423\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1462)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1462\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1462\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1306)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1306\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1306\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1251)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1251\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1251\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1500)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1500\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1500\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1246)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1246\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1246\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1335)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1335\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1335\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1312)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1312\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1312\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1258)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1258\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1258\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1254)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1254\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1254\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(41)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 41\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 41\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 41\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 41\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_41\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_41 of size 360304 dropped from memory (free 381736918)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_41_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_41_piece0 of size 35622 dropped from memory (free 381772540)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_41_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_41_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_41_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_41_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 41, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 41\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1300)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1300\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1300\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1257)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1257\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1257\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1336)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1336\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1336\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1269)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1269\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1269\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1456)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1456\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1456\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1212)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1212\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1212\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1260)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1260\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1260\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1497)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1497\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1497\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1302)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1302\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1302\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1319)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1319\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1319\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1261)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1261\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1261\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1323)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1323\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1323\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1337)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1337\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1337\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1226)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1226\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1226\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1445)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1445\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1445\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1438)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1438\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1438\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1266)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1266\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1266\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1291)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1291\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1291\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1250)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1250\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1250\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1484)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1484\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1484\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1424)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1424\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1424\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1453)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1453\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1453\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1247)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1247\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1247\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1422)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1422\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1422\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1253)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1253\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1253\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1256)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1256\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1256\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1480)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1480\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1480\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1412)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1412\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1412\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1275)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1275\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1275\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1325)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1325\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1325\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1421)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1421\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1421\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1286)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1286\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1286\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1457)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1457\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1457\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1333)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1333\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1333\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1463)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1463\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1463\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1203)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1203\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1203\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1294)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1294\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1294\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1338)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1338\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1338\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1281)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1281\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1281\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1435)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1435\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1435\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1491)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1491\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1491\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1206)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1206\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1206\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1461)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1461\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1461\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1277)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1277\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1277\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1443)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1443\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1443\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1202)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1202\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1202\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1482)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1482\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1482\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1272)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1272\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1272\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1296)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1296\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1296\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1326)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1326\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1326\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1479)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1479\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1479\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1208)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1208\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1208\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1487)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1487\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1487\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1239)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1239\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1239\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1499)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1499\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1499\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1213)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1213\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1213\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1271)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1271\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1271\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1293)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1293\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1293\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1472)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1472\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1472\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1220)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1220\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1220\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1485)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1485\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1485\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1305)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1305\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1305\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1451)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1451\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1451\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1331)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1331\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1331\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1314)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1314\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1314\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1313)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1313\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1313\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1416)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1416\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1416\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1440)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1440\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1440\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1298)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1298\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1298\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1289)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1289\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1289\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1288)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1288\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1288\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1209)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1209\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1209\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1301)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1301\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1301\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1219)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1219\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1219\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1215)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1215\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1215\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1292)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1292\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1292\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1205)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1205\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1205\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1442)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1442\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1442\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1431)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1431\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1431\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(38)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 38\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 38\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 38\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 38\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_38\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_38 of size 28864 dropped from memory (free 381801404)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_38_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_38_piece0 of size 13274 dropped from memory (free 381814678)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_38_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_38_piece0 on macbookpro.lan:57375 in memory (size: 13.0 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_38_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_38_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 38, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 38\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1332)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1332\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1332\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1486)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1486\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1486\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1328)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1328\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1328\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1309)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1309\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1309\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1439)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1439\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1439\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1299)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1299\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1299\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1210)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1210\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1210\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1233)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1233\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1233\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1248)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1248\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1248\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1506)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1506\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1506\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1460)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1460\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1460\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1225)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1225\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1225\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1427)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1427\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1427\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1278)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1278\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1278\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1283)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1283\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1283\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1234)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1234\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1234\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1473)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1473\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1473\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1255)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1255\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1255\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1448)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1448\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1448\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1230)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1230\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1230\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1464)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1464\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1464\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(39)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 39\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 39\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 39\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 39\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_39\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_39 of size 360304 dropped from memory (free 382174982)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_39_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_39_piece0 of size 35622 dropped from memory (free 382210604)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_39_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_39_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_39_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_39_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 39, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 39\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1450)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1450\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1450\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1274)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1274\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1274\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1308)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1308\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1308\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1415)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1415\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1415\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1432)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1432\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1432\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1265)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1265\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1265\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1481)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1481\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1481\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1477)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1477\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1477\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1267)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1267\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1267\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1459)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1459\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1459\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1455)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1455\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1455\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1290)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1290\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1290\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1433)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1433\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1433\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1285)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1285\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1285\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1478)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1478\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1478\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1316)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1316\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1316\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1411)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1411\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1411\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1492)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1492\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1492\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1245)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1245\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1245\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1236)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1236\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1236\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1327)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1327\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1327\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1501)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1501\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1501\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1489)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1489\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1489\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1410)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1410\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1410\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1284)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1284\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1284\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1458)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1458\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1458\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1447)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1447\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1447\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1330)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1330\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1330\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1216)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1216\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1216\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1418)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1418\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1418\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1307)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1307\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1307\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1417)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1417\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1417\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1227)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1227\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1227\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1483)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1483\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1483\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1262)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1262\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1262\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(42)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning broadcast 42\n",
      "25/04/11 09:37:22 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 42\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: removing broadcast 42\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing broadcast 42\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_42_piece0\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_42_piece0 of size 13253 dropped from memory (free 382223857)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_42_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Removed broadcast_42_piece0 on macbookpro.lan:57375 in memory (size: 12.9 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_42_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_42_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Removing block broadcast_42\n",
      "25/04/11 09:37:22 DEBUG MemoryStore: Block broadcast_42 of size 28856 dropped from memory (free 382252713)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 42, response is 0\n",
      "25/04/11 09:37:22 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned broadcast 42\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1263)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1263\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1263\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1429)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1429\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1429\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1449)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1449\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1449\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1476)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1476\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1476\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1315)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1315\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1315\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Got cleaning task CleanAccum(1320)\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaning accumulator 1320\n",
      "25/04/11 09:37:22 DEBUG ContextCleaner: Cleaned accumulator 1320\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 114 took 0.000052 seconds\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Registering RDD 114 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Got map stage job 26 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: submitStage(ShuffleMapStage 26 (name=showString at NativeMethodAccessorImpl.java:0;jobs=26))\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[114] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 26)\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 50.3 KiB, free 364.5 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_47 locally took 0 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_47 without replication took 0 ms\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 364.5 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_47_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on macbookpro.lan:57375 (size: 23.2 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_47_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_47_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_47_piece0 locally took 0 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_47_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:22 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[114] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:22 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Epoch for TaskSet 26.0: 0\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Valid locality levels for TaskSet 26.0: NO_PREF, ANY\n",
      "25/04/11 09:37:22 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_26.0, runningTasks: 0\n",
      "25/04/11 09:37:22 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:37:22 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)\n",
      "25/04/11 09:37:22 DEBUG ExecutorMetricsPoller: stageTCMP: (26, 0) -> 1\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Getting local block broadcast_47\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Level for block broadcast_47 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 34.770413 ms\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 200;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 200;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 9.097169 ms\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 4.904824 ms\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 26 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@6bd0a5ae\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 6.94234 ms\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 26 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@f27d316\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 6.327473 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Getting local block broadcast_46\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Level for block broadcast_46 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:22 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 26 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@f27d316\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 26 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@6bd0a5ae\n",
      "25/04/11 09:37:22 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 26 with length 200\n",
      "25/04/11 09:37:22 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 26: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,76,0,0,0,0,0,0,0,0,0,83,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:37:22 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 3552 bytes result sent to driver\n",
      "25/04/11 09:37:22 DEBUG ExecutorMetricsPoller: stageTCMP: (26, 0) -> 0\n",
      "25/04/11 09:37:22 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 193 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:22 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:37:22 INFO DAGScheduler: ShuffleMapStage 26 (showString at NativeMethodAccessorImpl.java:0) finished in 0.206 s\n",
      "25/04/11 09:37:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:37:22 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:37:22 DEBUG MapOutputTrackerMaster: Increasing epoch to 1\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: After removal of stage 26, remaining stages = 0\n",
      "25/04/11 09:37:22 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "+- ShuffleQueryStage 1\n",
      "   +- Exchange hashpartitioning(Name#789, 200), ENSURE_REQUIREMENTS, [plan_id=455]\n",
      "      +- *(2) HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "         +- *(2) Project [saleamount#682, Name#789]\n",
      "            +- *(2) BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "               :- *(2) Filter isnotnull(customerid#678)\n",
      "               :  +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "               +- BroadcastQueryStage 0\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=400]\n",
      "                     +- *(1) Filter isnotnull(CustomerID#788)\n",
      "                        +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:22 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:37:22 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 15.380278 ms\n",
      "25/04/11 09:37:22 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:37:22 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 13.68569 ms\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$takeOrdered$2$adapted\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$takeOrdered$2$adapted) is now cleaned +++\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$takeOrdered$3\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$takeOrdered$3) is now cleaned +++\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:37:22 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:37:22 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 118 took 0.000096 seconds\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Got job 27 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Final stage: ResultStage 28 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: submitStage(ResultStage 28 (name=showString at NativeMethodAccessorImpl.java:0;jobs=27))\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[118] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: submitMissingTasks(ResultStage 28)\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 51.9 KiB, free 364.4 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_48 locally took 0 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_48 without replication took 0 ms\n",
      "25/04/11 09:37:22 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 364.4 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_48_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:22 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on macbookpro.lan:57375 (size: 24.1 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:22 DEBUG BlockManagerMaster: Updated info of block broadcast_48_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Told master about block broadcast_48_piece0\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Put block broadcast_48_piece0 locally took 0 ms\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Putting block broadcast_48_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:22 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[118] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:22 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Epoch for TaskSet 28.0: 1\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Adding pending tasks took 1 ms\n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: Valid locality levels for TaskSet 28.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:37:22 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_28.0, runningTasks: 0\n",
      "25/04/11 09:37:22 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 27) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:37:22 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:37:22 INFO Executor: Running task 0.0 in stage 28.0 (TID 27)\n",
      "25/04/11 09:37:22 DEBUG ExecutorMetricsPoller: stageTCMP: (28, 0) -> 1\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Getting local block broadcast_48\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Level for block broadcast_48 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:22 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 8.148351 ms\n",
      "25/04/11 09:37:22 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0\n",
      "25/04/11 09:37:22 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 0, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:37:22 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:37:22 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:37:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "25/04/11 09:37:22 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_0_26_22_181,0)\n",
      "25/04/11 09:37:22 DEBUG BlockManager: Getting local shuffle block shuffle_0_26_22_181\n",
      "25/04/11 09:37:22 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 13 ms\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 12.269145 ms\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 27 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@61884da2\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 27 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@61884da2\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 27 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@61884da2\n",
      "25/04/11 09:37:22 DEBUG TaskMemoryManager: Task 27 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@61884da2\n",
      "25/04/11 09:37:22 INFO Executor: Finished task 0.0 in stage 28.0 (TID 27). 6473 bytes result sent to driver\n",
      "25/04/11 09:37:22 DEBUG ExecutorMetricsPoller: stageTCMP: (28, 0) -> 0\n",
      "25/04/11 09:37:22 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 27) in 104 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:22 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:22 INFO DAGScheduler: ResultStage 28 (showString at NativeMethodAccessorImpl.java:0) finished in 0.111 s\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: After removal of stage 28, remaining stages = 1\n",
      "25/04/11 09:37:22 DEBUG DAGScheduler: After removal of stage 27, remaining stages = 0\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:37:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "25/04/11 09:37:22 INFO DAGScheduler: Job 27 finished: showString at NativeMethodAccessorImpl.java:0, took 0.122017 s\n",
      "25/04/11 09:37:22 DEBUG GenerateUnsafeProjection: code for toprettystring(input[0, string, true], Some(America/Chicago)),toprettystring(input[1, double, true], Some(America/Chicago)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     UTF8String value_0;\n",
      "/* 035 */     if (isNull_1) {\n",
      "/* 036 */       value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 037 */     } else {\n",
      "/* 038 */       value_0 = value_1;\n",
      "/* 039 */     }\n",
      "/* 040 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 041 */\n",
      "/* 042 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 043 */     double value_3 = isNull_3 ?\n",
      "/* 044 */     -1.0 : (i.getDouble(1));\n",
      "/* 045 */     UTF8String value_2;\n",
      "/* 046 */     if (isNull_3) {\n",
      "/* 047 */       value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 048 */     } else {\n",
      "/* 049 */       value_2 = UTF8String.fromString(String.valueOf(value_3));\n",
      "/* 050 */     }\n",
      "/* 051 */     mutableStateArray_0[0].write(1, value_2);\n",
      "/* 052 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 053 */   }\n",
      "/* 054 */\n",
      "/* 055 */\n",
      "/* 056 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     UTF8String value_0;\n",
      "/* 035 */     if (isNull_1) {\n",
      "/* 036 */       value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 037 */     } else {\n",
      "/* 038 */       value_0 = value_1;\n",
      "/* 039 */     }\n",
      "/* 040 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 041 */\n",
      "/* 042 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 043 */     double value_3 = isNull_3 ?\n",
      "/* 044 */     -1.0 : (i.getDouble(1));\n",
      "/* 045 */     UTF8String value_2;\n",
      "/* 046 */     if (isNull_3) {\n",
      "/* 047 */       value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 048 */     } else {\n",
      "/* 049 */       value_2 = UTF8String.fromString(String.valueOf(value_3));\n",
      "/* 050 */     }\n",
      "/* 051 */     mutableStateArray_0[0].write(1, value_2);\n",
      "/* 052 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 053 */   }\n",
      "/* 054 */\n",
      "/* 055 */\n",
      "/* 056 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 5.464617 ms\n",
      "25/04/11 09:37:22 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "TakeOrderedAndProject(limit=21, orderBy=[total_spent#804 DESC NULLS LAST], output=[toprettystring(Name)#810,toprettystring(total_spent)#811])\n",
      "+- *(3) HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 1\n",
      "         +- Exchange hashpartitioning(Name#789, 200), ENSURE_REQUIREMENTS, [plan_id=455]\n",
      "            +- *(2) HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "               +- *(2) Project [saleamount#682, Name#789]\n",
      "                  +- *(2) BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "                     :- *(2) Filter isnotnull(customerid#678)\n",
      "                     :  +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                     +- BroadcastQueryStage 0\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=400]\n",
      "                           +- *(1) Filter isnotnull(CustomerID#788)\n",
      "                              +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:22 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, StructField(toprettystring(Name),StringType,false), StructField(toprettystring(total_spent),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[2];\n",
      "/* 024 */\n",
      "/* 025 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 026 */     boolean isNull_1 = true;\n",
      "/* 027 */     java.lang.String value_1 = null;\n",
      "/* 028 */     isNull_1 = false;\n",
      "/* 029 */     if (!isNull_1) {\n",
      "/* 030 */\n",
      "/* 031 */       Object funcResult_0 = null;\n",
      "/* 032 */       funcResult_0 = value_2.toString();\n",
      "/* 033 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 034 */\n",
      "/* 035 */     }\n",
      "/* 036 */     if (isNull_1) {\n",
      "/* 037 */       values_0[0] = null;\n",
      "/* 038 */     } else {\n",
      "/* 039 */       values_0[0] = value_1;\n",
      "/* 040 */     }\n",
      "/* 041 */\n",
      "/* 042 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 043 */     boolean isNull_3 = true;\n",
      "/* 044 */     java.lang.String value_3 = null;\n",
      "/* 045 */     isNull_3 = false;\n",
      "/* 046 */     if (!isNull_3) {\n",
      "/* 047 */\n",
      "/* 048 */       Object funcResult_1 = null;\n",
      "/* 049 */       funcResult_1 = value_4.toString();\n",
      "/* 050 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 051 */\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_3) {\n",
      "/* 054 */       values_0[1] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[1] = value_3;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 060 */     if (false) {\n",
      "/* 061 */       mutableRow.setNullAt(0);\n",
      "/* 062 */     } else {\n",
      "/* 063 */\n",
      "/* 064 */       mutableRow.update(0, value_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     return mutableRow;\n",
      "/* 068 */   }\n",
      "/* 069 */\n",
      "/* 070 */\n",
      "/* 071 */ }\n",
      "\n",
      "25/04/11 09:37:22 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[2];\n",
      "/* 024 */\n",
      "/* 025 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 026 */     boolean isNull_1 = true;\n",
      "/* 027 */     java.lang.String value_1 = null;\n",
      "/* 028 */     isNull_1 = false;\n",
      "/* 029 */     if (!isNull_1) {\n",
      "/* 030 */\n",
      "/* 031 */       Object funcResult_0 = null;\n",
      "/* 032 */       funcResult_0 = value_2.toString();\n",
      "/* 033 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 034 */\n",
      "/* 035 */     }\n",
      "/* 036 */     if (isNull_1) {\n",
      "/* 037 */       values_0[0] = null;\n",
      "/* 038 */     } else {\n",
      "/* 039 */       values_0[0] = value_1;\n",
      "/* 040 */     }\n",
      "/* 041 */\n",
      "/* 042 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 043 */     boolean isNull_3 = true;\n",
      "/* 044 */     java.lang.String value_3 = null;\n",
      "/* 045 */     isNull_3 = false;\n",
      "/* 046 */     if (!isNull_3) {\n",
      "/* 047 */\n",
      "/* 048 */       Object funcResult_1 = null;\n",
      "/* 049 */       funcResult_1 = value_4.toString();\n",
      "/* 050 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 051 */\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_3) {\n",
      "/* 054 */       values_0[1] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[1] = value_3;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 060 */     if (false) {\n",
      "/* 061 */       mutableRow.setNullAt(0);\n",
      "/* 062 */     } else {\n",
      "/* 063 */\n",
      "/* 064 */       mutableRow.update(0, value_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     return mutableRow;\n",
      "/* 068 */   }\n",
      "/* 069 */\n",
      "/* 070 */\n",
      "/* 071 */ }\n",
      "\n",
      "25/04/11 09:37:22 INFO CodeGenerator: Code generated in 5.391403 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            Name|       total_spent|\n",
      "+----------------+------------------+\n",
      "|   William White|23752.520000000004|\n",
      "|Hermione Granger|          22822.54|\n",
      "|   Susan Johnson|           12422.6|\n",
      "|       Chewbacca|11813.439999999999|\n",
      "|   Tiffany James|          11715.82|\n",
      "| Hermione Grager|           8750.94|\n",
      "|    Wylie Coyote|           7434.44|\n",
      "|          Dr Who|4064.8599999999997|\n",
      "|       Dan Brown|2427.2999999999997|\n",
      "|    Jason Bourne|           1806.34|\n",
      "|      Tony Stark|           1545.54|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#678)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: Sort [total_spent#804 DESC NULLS LAST], true, 0\n",
      "+- HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "   +- HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "      +- Project [saleamount#682, Name#789]\n",
      "         +- BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "            :- Project [customerid#678, saleamount#682]\n",
      "            :  +- Filter isnotnull(customerid#678)\n",
      "            :     +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "            +- Project [CustomerID#788, Name#789]\n",
      "               +- Filter isnotnull(CustomerID#788)\n",
      "                  +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:23 DEBUG BroadcastQueryStageExec: Materialize query stage BroadcastQueryStageExec: 0\n",
      "25/04/11 09:37:23 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 351.8 KiB, free 364.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_49 locally took 1 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_49 without replication took 1 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 364.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_49_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_49_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_49_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_49_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_49_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 49 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:37:23 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 122 took 0.000108 seconds\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Got job 28 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Final stage: ResultStage 29 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitStage(ResultStage 29 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=28))\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[122] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitMissingTasks(ResultStage 29)\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 15.5 KiB, free 364.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_50 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_50 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 364.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_50_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on macbookpro.lan:57375 (size: 7.6 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_50_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_50_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_50_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_50_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[122] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Epoch for TaskSet 29.0: 1\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Valid locality levels for TaskSet 29.0: NO_PREF, ANY\n",
      "25/04/11 09:37:23 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_29.0, runningTasks: 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 28) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:37:23 INFO Executor: Running task 0.0 in stage 29.0 (TID 28)\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (29, 0) -> 1\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_50\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_50 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_49\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_49 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO Executor: Finished task 0.0 in stage 29.0 (TID 28). 1862 bytes result sent to driver\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (29, 0) -> 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 28) in 9 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:23 INFO DAGScheduler: ResultStage 29 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.012 s\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 29, remaining stages = 0\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Job 28 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.014285 s\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 0 acquired 1024.3 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@5398d672\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for cast(input[0, int, false] as bigint):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     int value_1 = i.getInt(0);\n",
      "/* 032 */     boolean isNull_0 = false;\n",
      "/* 033 */     long value_0 = -1L;\n",
      "/* 034 */     if (!false) {\n",
      "/* 035 */       value_0 = (long) value_1;\n",
      "/* 036 */     }\n",
      "/* 037 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 0 acquired 512.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@5398d672\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 0 release 256.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@5398d672\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 0 acquired 88.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@5398d672\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 0 release 512.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@5398d672\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 1024.1 KiB, free 363.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_51 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_51 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 509.0 B, free 363.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_51_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on macbookpro.lan:57375 (size: 509.0 B, free: 366.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_51_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_51_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_51_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_51_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 51 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#678 = CustomerID#788))\n",
      "25/04/11 09:37:23 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#678) | rightKeys:List(CustomerID#788)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:37:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#678)\n",
      "25/04/11 09:37:23 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 1\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_51\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_51 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 351.8 KiB, free 362.7 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_52 locally took 1 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_52 without replication took 1 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 362.6 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_52_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_52_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_52_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_52_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_52_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 52 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 126 took 0.000068 seconds\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Registering RDD 126 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) as input to shuffle 1\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Got map stage job 29 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitStage(ShuffleMapStage 30 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=29))\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[126] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 30)\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 50.5 KiB, free 362.6 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_53 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_53 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 362.5 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_53_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on macbookpro.lan:57375 (size: 23.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_53_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_53_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_53_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_53_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[126] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Epoch for TaskSet 30.0: 1\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Valid locality levels for TaskSet 30.0: NO_PREF, ANY\n",
      "25/04/11 09:37:23 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_30.0, runningTasks: 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 29) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:37:23 INFO Executor: Running task 0.0 in stage 30.0 (TID 29)\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (30, 0) -> 1\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_53\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_53 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 200;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 29 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@59d9c9d0\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 29 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@6c5b5fbd\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_52\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_52 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 29 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@6c5b5fbd\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 29 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@59d9c9d0\n",
      "25/04/11 09:37:23 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 29 with length 200\n",
      "25/04/11 09:37:23 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 29: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,76,0,0,0,0,0,0,0,0,0,83,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:37:23 INFO Executor: Finished task 0.0 in stage 30.0 (TID 29). 3509 bytes result sent to driver\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (30, 0) -> 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 29) in 42 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:37:23 INFO DAGScheduler: ShuffleMapStage 30 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.047 s\n",
      "25/04/11 09:37:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:37:23 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Increasing epoch to 2\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 30, remaining stages = 0\n",
      "25/04/11 09:37:23 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "+- ShuffleQueryStage 1\n",
      "   +- Exchange hashpartitioning(Name#789, 200), ENSURE_REQUIREMENTS, [plan_id=624]\n",
      "      +- *(2) HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "         +- *(2) Project [saleamount#682, Name#789]\n",
      "            +- *(2) BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "               :- *(2) Filter isnotnull(customerid#678)\n",
      "               :  +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "               +- BroadcastQueryStage 0\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=561]\n",
      "                     +- *(1) Filter isnotnull(CustomerID#788)\n",
      "                        +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:23 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:37:23 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 2\n",
      "25/04/11 09:37:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:37:23 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG GenerateOrdering: Generated Ordering by input[0, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG CodeGenerator: \n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO CodeGenerator: Code generated in 6.655907 ms\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rangeBounds$1\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rangeBounds$1) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$sketch$1$adapted\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$sketch$1$adapted) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:37:23 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:37:23 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 131 took 0.000049 seconds\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Got job 30 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Final stage: ResultStage 32 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitStage(ResultStage 32 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=30))\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[131] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitMissingTasks(ResultStage 32)\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 52.3 KiB, free 362.5 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_54 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_54 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 362.5 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_54_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on macbookpro.lan:57375 (size: 24.2 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_54_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_54_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_54_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_54_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[131] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Epoch for TaskSet 32.0: 2\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Valid locality levels for TaskSet 32.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:37:23 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_32.0, runningTasks: 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 30) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:37:23 INFO Executor: Running task 0.0 in stage 32.0 (TID 30)\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (32, 0) -> 1\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_54\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_54 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 1\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 1, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:37:23 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:37:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_1_29_22_181,0)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local shuffle block shuffle_1_29_22_181\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(1);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(1);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO CodeGenerator: Code generated in 4.398157 ms\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 30 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@bbd2baf\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 30 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@bbd2baf\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 30 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@bbd2baf\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 30 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@bbd2baf\n",
      "25/04/11 09:37:23 INFO Executor: Finished task 0.0 in stage 32.0 (TID 30). 6373 bytes result sent to driver\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (32, 0) -> 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 30) in 32 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:23 INFO DAGScheduler: ResultStage 32 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.036 s\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 32, remaining stages = 1\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 31, remaining stages = 0\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Job 30 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.039711 s\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 132 took 0.000144 seconds\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Registering RDD 132 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) as input to shuffle 2\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Got map stage job 31 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitStage(ShuffleMapStage 34 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=31))\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[132] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 34)\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 52.6 KiB, free 362.4 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_55 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_55 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 362.4 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_55_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:23 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on macbookpro.lan:57375 (size: 24.3 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:37:23 DEBUG BlockManagerMaster: Updated info of block broadcast_55_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Told master about block broadcast_55_piece0\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Put block broadcast_55_piece0 locally took 0 ms\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Putting block broadcast_55_piece0 without replication took 0 ms\n",
      "25/04/11 09:37:23 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[132] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Epoch for TaskSet 34.0: 2\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: Valid locality levels for TaskSet 34.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:37:23 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_34.0, runningTasks: 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 31) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9173 bytes) \n",
      "25/04/11 09:37:23 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:37:23 INFO Executor: Running task 0.0 in stage 34.0 (TID 31)\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (34, 0) -> 1\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local block broadcast_55\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Level for block broadcast_55 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:23 DEBUG GenerateOrdering: Generated Ordering by input[0, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG CodeGenerator: \n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:23 INFO CodeGenerator: Code generated in 7.211378 ms\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 1\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 1, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:37:23 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:37:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_1_29_22_181,0)\n",
      "25/04/11 09:37:23 DEBUG BlockManager: Getting local shuffle block shuffle_1_29_22_181\n",
      "25/04/11 09:37:23 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(1);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 31 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@75d2031c\n",
      "25/04/11 09:37:23 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 31 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@75d2031c\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 31 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@75d2031c\n",
      "25/04/11 09:37:23 DEBUG TaskMemoryManager: Task 31 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@75d2031c\n",
      "25/04/11 09:37:23 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 31 with length 11\n",
      "25/04/11 09:37:23 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 31: [84,84,84,84,84,84,84,76,83,84,84]\n",
      "25/04/11 09:37:23 INFO Executor: Finished task 0.0 in stage 34.0 (TID 31). 6056 bytes result sent to driver\n",
      "25/04/11 09:37:23 DEBUG ExecutorMetricsPoller: stageTCMP: (34, 0) -> 0\n",
      "25/04/11 09:37:23 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 31) in 34 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:23 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:37:23 INFO DAGScheduler: ShuffleMapStage 34 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.041 s\n",
      "25/04/11 09:37:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:37:23 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:37:23 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:37:23 DEBUG MapOutputTrackerMaster: Increasing epoch to 3\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 34, remaining stages = 1\n",
      "25/04/11 09:37:23 DEBUG DAGScheduler: After removal of stage 33, remaining stages = 0\n",
      "25/04/11 09:37:23 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: ShuffleQueryStage 2\n",
      "+- Exchange rangepartitioning(total_spent#804 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=658]\n",
      "   +- *(3) HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "      +- AQEShuffleRead coalesced\n",
      "         +- ShuffleQueryStage 1\n",
      "            +- Exchange hashpartitioning(Name#789, 200), ENSURE_REQUIREMENTS, [plan_id=624]\n",
      "               +- *(2) HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "                  +- *(2) Project [saleamount#682, Name#789]\n",
      "                     +- *(2) BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "                        :- *(2) Filter isnotnull(customerid#678)\n",
      "                        :  +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                        +- BroadcastQueryStage 0\n",
      "                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=561]\n",
      "                              +- *(1) Filter isnotnull(CustomerID#788)\n",
      "                                 +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:37:23 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:37:24 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=4\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean sort_needToSort_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n",
      "/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n",
      "/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     sort_needToSort_0 = true;\n",
      "/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n",
      "/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n",
      "/* 031 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 033 */\n",
      "/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n",
      "/* 035 */       // shouldStop check is eliminated\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */   protected void processNext() throws java.io.IOException {\n",
      "/* 041 */     if (sort_needToSort_0) {\n",
      "/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n",
      "/* 043 */       sort_addToSorter_0();\n",
      "/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n",
      "/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n",
      "/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n",
      "/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 049 */       sort_needToSort_0 = false;\n",
      "/* 050 */     }\n",
      "/* 051 */\n",
      "/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n",
      "/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n",
      "/* 054 */\n",
      "/* 055 */       append(sort_outputRow_0);\n",
      "/* 056 */\n",
      "/* 057 */       if (shouldStop()) return;\n",
      "/* 058 */     }\n",
      "/* 059 */   }\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:24 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=4\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean sort_needToSort_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n",
      "/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n",
      "/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     sort_needToSort_0 = true;\n",
      "/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n",
      "/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n",
      "/* 031 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 033 */\n",
      "/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n",
      "/* 035 */       // shouldStop check is eliminated\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */   protected void processNext() throws java.io.IOException {\n",
      "/* 041 */     if (sort_needToSort_0) {\n",
      "/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n",
      "/* 043 */       sort_addToSorter_0();\n",
      "/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n",
      "/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n",
      "/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n",
      "/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 049 */       sort_needToSort_0 = false;\n",
      "/* 050 */     }\n",
      "/* 051 */\n",
      "/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n",
      "/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n",
      "/* 054 */\n",
      "/* 055 */       append(sort_outputRow_0);\n",
      "/* 056 */\n",
      "/* 057 */       if (shouldStop()) return;\n",
      "/* 058 */     }\n",
      "/* 059 */   }\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:24 INFO CodeGenerator: Code generated in 8.793394 ms\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:37:24 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:37:24 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 135 took 0.000059 seconds\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Got job 32 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Final stage: ResultStage 37 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: submitStage(ResultStage 37 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=32))\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[135] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: submitMissingTasks(ResultStage 37)\n",
      "25/04/11 09:37:24 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 49.1 KiB, free 362.4 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Put block broadcast_56 locally took 0 ms\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Putting block broadcast_56 without replication took 0 ms\n",
      "25/04/11 09:37:24 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 362.3 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_56_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1626)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1626\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1626\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1792)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1792\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1792\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1975)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1975\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1975\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1856)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1856\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1856\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1651)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1651\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1651\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1562)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on macbookpro.lan:57375 (size: 23.2 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1562\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1562\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1904)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1904\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1904\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1362)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1362\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1362\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_56_piece0\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1822)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1822\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_56_piece0\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1822\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(54)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Put block broadcast_56_piece0 locally took 5 ms\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 54\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 54\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Putting block broadcast_56_piece0 without replication took 5 ms\n",
      "25/04/11 09:37:24 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 54\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 54\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_54\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[135] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:37:24 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_54 of size 53584 dropped from memory (free 379981985)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_54_piece0\n",
      "25/04/11 09:37:24 DEBUG TaskSetManager: Epoch for TaskSet 37.0: 3\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_54_piece0 of size 24789 dropped from memory (free 380006774)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_54_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:37:24 DEBUG TaskSetManager: Valid locality levels for TaskSet 37.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_54_piece0 on macbookpro.lan:57375 in memory (size: 24.2 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_54_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_54_piece0\n",
      "25/04/11 09:37:24 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_37.0, runningTasks: 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 54, response is 0\n",
      "25/04/11 09:37:24 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 32) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:37:24 INFO Executor: Running task 0.0 in stage 37.0 (TID 32)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 54\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1578)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1578\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1578\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1662)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1662\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1662\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1854)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1854\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1854\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1931)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1931\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1931\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1794)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1794\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1794\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1511)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1511\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1511\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1779)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1779\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1779\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1401)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1401\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1401\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1912)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1912\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1912\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1398)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1398\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1398\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1617)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1617\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1617\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1977)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1977\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1977\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1855)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1855\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1855\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1638)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1638\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1638\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1466)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1466\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1466\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1658)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1658\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1658\n",
      "25/04/11 09:37:24 DEBUG ExecutorMetricsPoller: stageTCMP: (37, 0) -> 1\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1642)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1642\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1642\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2007)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2007\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2007\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1573)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1573\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1573\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1516)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1516\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1516\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1744)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1744\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1744\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2025)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2025\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2025\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanShuffle(0)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Getting local block broadcast_56\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Level for block broadcast_56 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning shuffle 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing shuffle 0\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned shuffle 0\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1983)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1983\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1983\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1788)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1788\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1788\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1587)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1587\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1587\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1530)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1530\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1530\n",
      "25/04/11 09:37:24 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 2\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1938)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1938\n",
      "25/04/11 09:37:24 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 2, mappers 0-1, partitions 0-11\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1938\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1997)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1997\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1997\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1789)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1789\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1789\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1973)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1973\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1973\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1796)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1796\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1796\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1377)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1377\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1377\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2003)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2003\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2003\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2018)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2018\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2018\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1581)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1581\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1581\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1655)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1655\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1655\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1359)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1359\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1359\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1370)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1370\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1370\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1371)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1371\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1371\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1632)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1632\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1632\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1913)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1913\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1913\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2038)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2038\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2038\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2042)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2042\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2042\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1356)\n",
      "25/04/11 09:37:24 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1356\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1356\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2012)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2012\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2012\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1592)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1592\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1592\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1841)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1841\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1841\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1761)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1761\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1761\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1864)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1864\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1864\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1404)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1404\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1404\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1363)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1363\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1363\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1345)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1345\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1345\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1564)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1564\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1564\n",
      "25/04/11 09:37:24 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1550)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1550\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1550\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1851)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1851\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1851\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1774)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1774\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1774\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1571)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1571\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1571\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1846)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1846\n",
      "25/04/11 09:37:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1846\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(47)\n",
      "25/04/11 09:37:24 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_2_31_0_11,0)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 47\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Getting local shuffle block shuffle_2_31_0_11\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 47\n",
      "25/04/11 09:37:24 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 47\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 47\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_47\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_47 of size 51544 dropped from memory (free 380058318)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_47_piece0\n",
      "25/04/11 09:37:24 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=4\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean sort_needToSort_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n",
      "/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n",
      "/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     sort_needToSort_0 = true;\n",
      "/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n",
      "/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n",
      "/* 031 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 033 */\n",
      "/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n",
      "/* 035 */       // shouldStop check is eliminated\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */   protected void processNext() throws java.io.IOException {\n",
      "/* 041 */     if (sort_needToSort_0) {\n",
      "/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n",
      "/* 043 */       sort_addToSorter_0();\n",
      "/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n",
      "/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n",
      "/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n",
      "/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 049 */       sort_needToSort_0 = false;\n",
      "/* 050 */     }\n",
      "/* 051 */\n",
      "/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n",
      "/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n",
      "/* 054 */\n",
      "/* 055 */       append(sort_outputRow_0);\n",
      "/* 056 */\n",
      "/* 057 */       if (shouldStop()) return;\n",
      "/* 058 */     }\n",
      "/* 059 */   }\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_47_piece0 of size 23807 dropped from memory (free 380082125)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_47_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_47_piece0 on macbookpro.lan:57375 in memory (size: 23.2 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_47_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_47_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 47, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 47\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1853)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1853\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1853\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1387)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1387\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1387\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1557)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1557\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1557\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1619)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1619\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1619\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1519)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1519\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1519\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2037)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2037\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2037\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1907)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1907\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1907\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1618)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1618\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1618\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1381)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1381\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1381\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1539)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1539\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1539\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1824)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1824\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1824\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2015)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2015\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2015\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1596)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1596\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1596\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(48)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 48\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 48\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 48\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 48\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_48\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_48 of size 53120 dropped from memory (free 380135245)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_48_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_48_piece0 of size 24726 dropped from memory (free 380159971)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_48_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_48_piece0 on macbookpro.lan:57375 in memory (size: 24.1 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_48_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_48_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing shuffle 0, response is true\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: true to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 48, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 48\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1343)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1343\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1343\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1515)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1515\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1515\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1832)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1832\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1832\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(45)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 45\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 45\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 45\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 45\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_45\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_45 of size 1048688 dropped from memory (free 381208659)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_45_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_45_piece0 of size 509 dropped from memory (free 381209168)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_45_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_45_piece0 on macbookpro.lan:57375 in memory (size: 509.0 B, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_45_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_45_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 45, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 45\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1385)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1385\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1385\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1613)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1613\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1613\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1525)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1525\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1525\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1663)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1663\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1663\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1572)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1572\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1572\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1652)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1652\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1652\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1524)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1524\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1524\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1739)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1739\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1739\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1577)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1577\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1577\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1995)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1995\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1995\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(53)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 53\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 53\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 53\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 53\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_53\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_53 of size 51704 dropped from memory (free 381260872)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_53_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_53_piece0 of size 23894 dropped from memory (free 381284766)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_53_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_53_piece0 on macbookpro.lan:57375 in memory (size: 23.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_53_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_53_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 53, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 53\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2031)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2031\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2031\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1784)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1784\n",
      "25/04/11 09:37:24 INFO CodeGenerator: Code generated in 11.951374 ms\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1784\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2005)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2005\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2005\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1589)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1589\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1589\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1994)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1994\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1994\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2000)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2000\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2000\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1361)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1361\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1361\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1637)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1637\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1637\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1780)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1780\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1780\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1575)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1575\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1575\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1925)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1925\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1925\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2004)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2004\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2004\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2019)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2019\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2019\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1353)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1353\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1353\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1622)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1622\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1622\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1746)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1746\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1746\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1565)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1565\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1565\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1923)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1923\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1923\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1468)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1468\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1468\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1827)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1827\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1827\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1806)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1806\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1806\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2024)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2024\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2024\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1934)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1934\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1934\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1659)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1659\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1659\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1979)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1979\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1979\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1814)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1814\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1814\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1588)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1588\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1588\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1834)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1834\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1834\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1833)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1833\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1833\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1905)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1905\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1905\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1768)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1768\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1768\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1365)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1365\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1365\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1845)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1845\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1845\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1380)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1380\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1380\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1756)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1756\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1756\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1999)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1999\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1999\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1849)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1849\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1849\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1601)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1601\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1601\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1933)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1933\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1933\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1813)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1813\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1813\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1920)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1920\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1920\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1406)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1406\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1406\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1553)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1553\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1553\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1661)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1661\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1661\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1598)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1598\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1598\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1860)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1860\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1860\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1595)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1595\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1595\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1812)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1812\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1812\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1978)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1978\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1978\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1850)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1850\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1850\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1358)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1358\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1358\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1760)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1760\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1760\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2036)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2036\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2036\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1930)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1930\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1930\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1467)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1467\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1467\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1656)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1656\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1656\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1762)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1762\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1762\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2001)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2001\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2001\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1741)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1741\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1741\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2021)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2021\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2021\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1648)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1648\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1648\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1857)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1857\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1857\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1908)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1908\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1908\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1567)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1567\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1567\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1917)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1917\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1917\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1993)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1993\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1993\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1534)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1534\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1534\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2034)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2034\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2034\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1527)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1527\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1527\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1510)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1510\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1510\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1378)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1378\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1378\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1547)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1547\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1547\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1820)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1820\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1820\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1641)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1641\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1641\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1916)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1916\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1916\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1811)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1811\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1811\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1903)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1903\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1903\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1835)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1835\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1835\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1350)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1350\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1350\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1390)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1390\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1390\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1354)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1354\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1354\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1918)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1918\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1918\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1653)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1653\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1653\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1831)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1831\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1831\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1861)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1861\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1861\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1586)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1586\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1586\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1974)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1974\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1974\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1986)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1986\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1986\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1518)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1518\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1518\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1838)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1838\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1838\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2008)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2008\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2008\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1610)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1610\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1610\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1344)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1344\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1344\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1758)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1758\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1758\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1654)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1654\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1654\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1608)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1608\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1608\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1560)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1560\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1560\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1819)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1819\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1819\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1384)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1384\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1384\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1629)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1629\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1629\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1754)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1754\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1754\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1836)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1836\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1836\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1537)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1537\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1537\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1605)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1605\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1605\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1816)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1816\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1816\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1374)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1374\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1374\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1514)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1514\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1514\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2041)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2041\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2041\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1981)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1981\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1981\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1640)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1640\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1640\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1408)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1408\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1408\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1775)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1775\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1775\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1399)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1399\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1399\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1604)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1604\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1604\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(55)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 55\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 55\n",
      "25/04/11 09:37:24 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 55\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 55\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_55_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_55_piece0 of size 24907 dropped from memory (free 381309673)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_55_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_55_piece0 on macbookpro.lan:57375 in memory (size: 24.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_55_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_55_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_55\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_55 of size 53856 dropped from memory (free 381363529)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 55, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 55\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1755)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1755\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1755\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1630)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1630\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1630\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1821)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1821\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1821\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1582)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1582\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1582\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1823)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1823\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1823\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1609)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1609\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1609\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1646)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1646\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1646\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1906)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1906\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1906\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1787)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1787\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1787\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1781)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1781\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1781\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1848)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1848\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1848\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1612)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1612\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1612\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1546)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1546\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1546\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1922)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1922\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1922\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1624)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1624\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1624\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1405)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1405\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1405\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1940)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1940\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1940\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1751)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1751\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1751\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1403)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1403\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1403\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1615)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1615\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1615\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1528)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1528\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1528\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1523)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1523\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1523\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1631)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1631\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1631\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1599)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1599\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1599\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1830)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1830\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1830\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1634)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1634\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1634\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1770)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1770\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1770\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1910)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1910\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1910\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1862)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1862\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1862\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1545)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1545\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1545\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1507)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1507\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1507\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1614)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1614\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1614\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1745)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1745\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1745\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1773)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1773\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1773\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1937)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1937\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1937\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1542)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1542\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1542\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1536)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1536\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1536\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1470)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1470\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1470\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1829)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1829\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1829\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1839)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1839\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1839\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1347)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1347\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1347\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1526)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1526\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1526\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1364)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1364\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1364\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1531)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1531\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1531\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1771)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1771\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1771\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1837)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1837\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1837\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1984)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1984\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1984\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2011)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2011\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2011\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1576)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1576\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1576\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1532)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1532\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1532\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1407)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1407\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1407\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1919)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1919\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1919\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1976)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1976\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1976\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1628)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1628\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1628\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1843)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1843\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1843\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1797)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1797\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1797\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1928)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1928\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1928\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1810)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1810\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1810\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1926)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1926\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1926\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2006)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2006\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2006\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1591)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1591\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1591\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1759)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1759\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1759\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2033)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2033\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2033\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1584)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1584\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1584\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1583)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1583\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1583\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1627)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1627\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1627\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1747)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1747\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1747\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1782)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1782\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1782\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1752)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1752\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1752\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1786)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1786\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1786\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1625)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1625\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1625\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1394)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1394\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1394\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1987)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1987\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1987\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1372)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2032)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2032\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2032\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1366)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1366\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1366\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1783)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1783\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1783\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1807)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1807\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1807\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2002)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2002\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2002\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1777)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1777\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1777\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1579)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1579\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1579\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1616)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1616\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1616\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2020)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2020\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2020\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2030)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2030\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2030\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1529)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1529\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1529\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1990)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1990\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1990\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1352)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1352\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1352\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1795)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1795\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1795\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1778)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1778\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1778\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1915)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1915\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1915\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1351)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1351\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1351\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1383)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1383\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1383\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(50)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 50\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 50\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 50\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 50\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_50\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_50 of size 15896 dropped from memory (free 381379425)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_50_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_50_piece0 of size 7814 dropped from memory (free 381387239)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_50_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_50_piece0 on macbookpro.lan:57375 in memory (size: 7.6 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_50_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_50_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 50, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 50\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2016)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2016\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2016\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1402)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1402\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1402\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1649)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1649\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1649\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1815)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1815\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1815\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1393)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1393\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1393\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1621)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1621\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1621\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1563)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1563\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1563\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1793)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1793\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1793\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1863)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1863\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1863\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1355)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1355\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1355\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1785)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1785\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1785\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1386)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1386\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1386\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1342)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1342\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1342\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1844)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1844\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1844\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1392)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1392\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1392\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1929)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1929\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1929\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1865)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1865\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1865\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1842)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1842\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1842\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(43)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 43\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 43\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 43\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 43\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_43\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_43 of size 360200 dropped from memory (free 381747439)\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_43_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_43_piece0 of size 35539 dropped from memory (free 381782978)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_43_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_43_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_43_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_43_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 43, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 43\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1554)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1554\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1554\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1561)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1561\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1561\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1791)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1791\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1791\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1776)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1776\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1776\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1633)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1633\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1633\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1521)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1521\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1521\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1382)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1382\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1382\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2027)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2027\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2027\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1379)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1379\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1379\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1395)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1395\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1395\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1396)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1396\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1396\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1558)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1558\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1558\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1935)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1935\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1935\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1367)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1367\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1367\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1749)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1749\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1749\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1357)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1357\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1357\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1509)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1509\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1509\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1517)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1517\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1517\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1825)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1825\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1825\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1914)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1914\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1914\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1650)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1650\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1650\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1543)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1543\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1543\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1593)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1593\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1593\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1391)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1391\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1391\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1548)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1548\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1548\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1397)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1397\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1397\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1544)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1544\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1544\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1568)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1568\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1568\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1620)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1620\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1620\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1373)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1373\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1373\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1769)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1769\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1769\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1508)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1508\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1508\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1556)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1556\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1556\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1790)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1790\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1790\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1585)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1585\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1585\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1818)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1818\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1818\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1911)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1911\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1911\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1389)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1389\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1389\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2013)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2013\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2013\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1909)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1909\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1909\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1566)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1566\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1566\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1340)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1340\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1340\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1590)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1590\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1590\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2017)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2017\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2017\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1623)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1623\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1623\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1600)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1600\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1600\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1368)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1368\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1368\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1847)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1847\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1847\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1348)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1348\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1348\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1611)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1611\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1611\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1657)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1657\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1657\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1936)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1936\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1936\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1375)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1375\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1375\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2014)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2014\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2014\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1602)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1602\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1602\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1924)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1924\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1924\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1927)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1927\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1927\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1765)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1765\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1765\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1535)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1535\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1535\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1750)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1750\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1750\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1400)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1400\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1400\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1639)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1639\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1639\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1635)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1635\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1635\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1570)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1570\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1570\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1808)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1808\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1808\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1932)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1932\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1932\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1512)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1512\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1512\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1574)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1574\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1574\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1409)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1409\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1409\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1513)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1513\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1513\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1980)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1980\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1980\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(46)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning broadcast 46\n",
      "25/04/11 09:37:24 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 46\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: removing broadcast 46\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing broadcast 46\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_46_piece0\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_46_piece0 of size 35539 dropped from memory (free 381818517)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_46_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:37:24 INFO BlockManagerInfo: Removed broadcast_46_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:37:24 DEBUG BlockManagerMaster: Updated info of block broadcast_46_piece0\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Told master about block broadcast_46_piece0\n",
      "25/04/11 09:37:24 DEBUG GenerateUnsafeProjection: code for sortprefix(input[1, double, true] DESC NULLS LAST):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 032 */     double value_1 = isNull_1 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     long value_0 = 0L;\n",
      "/* 035 */     boolean isNull_0 = isNull_1;\n",
      "/* 036 */     if (!isNull_1) {\n",
      "/* 037 */       value_0 = org.apache.spark.util.collection.unsafe.sort.PrefixComparators$DoublePrefixComparator.computePrefix((double)value_1);\n",
      "/* 038 */     }\n",
      "/* 039 */     if (isNull_0) {\n",
      "/* 040 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 041 */     } else {\n",
      "/* 042 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 045 */   }\n",
      "/* 046 */\n",
      "/* 047 */\n",
      "/* 048 */ }\n",
      "\n",
      "25/04/11 09:37:24 DEBUG BlockManager: Removing block broadcast_46\n",
      "25/04/11 09:37:24 DEBUG MemoryStore: Block broadcast_46 of size 360200 dropped from memory (free 382178717)\n",
      "25/04/11 09:37:24 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 032 */     double value_1 = isNull_1 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     long value_0 = 0L;\n",
      "/* 035 */     boolean isNull_0 = isNull_1;\n",
      "/* 036 */     if (!isNull_1) {\n",
      "/* 037 */       value_0 = org.apache.spark.util.collection.unsafe.sort.PrefixComparators$DoublePrefixComparator.computePrefix((double)value_1);\n",
      "/* 038 */     }\n",
      "/* 039 */     if (isNull_0) {\n",
      "/* 040 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 041 */     } else {\n",
      "/* 042 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 045 */   }\n",
      "/* 046 */\n",
      "/* 047 */\n",
      "/* 048 */ }\n",
      "\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 46, response is 0\n",
      "25/04/11 09:37:24 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned broadcast 46\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2009)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2009\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2009\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1469)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1469\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1469\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1828)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1828\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1828\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1985)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1985\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1985\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2039)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2039\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2039\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1349)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1349\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1349\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1551)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1551\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1551\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1388)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1388\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1388\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1341)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1341\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1341\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1559)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1559\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1559\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1763)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1763\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1763\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1772)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1772\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1772\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1840)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1840\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1840\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1643)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1643\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1643\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1809)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1809\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1809\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1766)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1766\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1766\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1644)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1644\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1644\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2029)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2029\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2029\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1346)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1346\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1346\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1996)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1996\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1996\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1767)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1767\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1767\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1533)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1533\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1533\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1597)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1597\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1597\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1764)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1764\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1764\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1859)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1859\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1859\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1607)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1607\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1607\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2023)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2023\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2023\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1520)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1520\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1520\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1538)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1538\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1538\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1826)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1826\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1826\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1645)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1645\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1645\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1660)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1660\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1660\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1740)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1740\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1740\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1939)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1939\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1939\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1540)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1540\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1540\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1992)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1992\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1992\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1369)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1369\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1369\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1376)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1376\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1376\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1636)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1636\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1636\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1743)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1743\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1743\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1549)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1549\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1549\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1858)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1858\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1858\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1988)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1988\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1988\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1471)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1471\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1471\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2028)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2028\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2028\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1921)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1921\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1921\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1798)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1798\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1798\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1555)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1555\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1555\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1580)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1580\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1580\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1603)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1603\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1603\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1757)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1757\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1757\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1569)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1569\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1569\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1594)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1594\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1594\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1998)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1998\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1998\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1465)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1465\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1465\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1742)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1742\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1742\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2035)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2035\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2035\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1552)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1552\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1552\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2022)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2022\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2022\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1360)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1360\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1360\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1541)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1541\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1541\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1852)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1852\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1852\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1989)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1989\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1989\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2010)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2010\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2010\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2040)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2040\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2040\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1817)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1817\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1817\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1748)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1748\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1748\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1753)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1753\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1753\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(2026)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 2026\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 2026\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1522)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1522\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1522\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1982)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1982\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1982\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1606)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1606\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1606\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1647)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1647\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1647\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Got cleaning task CleanAccum(1991)\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaning accumulator 1991\n",
      "25/04/11 09:37:24 DEBUG ContextCleaner: Cleaned accumulator 1991\n",
      "25/04/11 09:37:24 INFO CodeGenerator: Code generated in 7.429335 ms\n",
      "25/04/11 09:37:24 DEBUG TaskMemoryManager: Task 32 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3ed05121\n",
      "25/04/11 09:37:24 DEBUG TaskMemoryManager: Task 32 acquired 1024.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3ed05121\n",
      "25/04/11 09:37:24 DEBUG TaskMemoryManager: Task 32 release 1024.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3ed05121\n",
      "25/04/11 09:37:24 DEBUG TaskMemoryManager: Task 32 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3ed05121\n",
      "25/04/11 09:37:24 INFO Executor: Finished task 0.0 in stage 37.0 (TID 32). 7680 bytes result sent to driver\n",
      "25/04/11 09:37:24 DEBUG ExecutorMetricsPoller: stageTCMP: (37, 0) -> 0\n",
      "25/04/11 09:37:24 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 32) in 57 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:37:24 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:37:24 INFO DAGScheduler: ResultStage 37 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.076 s\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: After removal of stage 35, remaining stages = 2\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: After removal of stage 37, remaining stages = 1\n",
      "25/04/11 09:37:24 DEBUG DAGScheduler: After removal of stage 36, remaining stages = 0\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:37:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "25/04/11 09:37:24 INFO DAGScheduler: Job 32 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.078323 s\n",
      "25/04/11 09:37:24 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(4) Sort [total_spent#804 DESC NULLS LAST], true, 0\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 2\n",
      "      +- Exchange rangepartitioning(total_spent#804 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=658]\n",
      "         +- *(3) HashAggregate(keys=[Name#789], functions=[sum(saleamount#682)], output=[Name#789, total_spent#804])\n",
      "            +- AQEShuffleRead coalesced\n",
      "               +- ShuffleQueryStage 1\n",
      "                  +- Exchange hashpartitioning(Name#789, 200), ENSURE_REQUIREMENTS, [plan_id=624]\n",
      "                     +- *(2) HashAggregate(keys=[Name#789], functions=[partial_sum(saleamount#682)], output=[Name#789, sum#815])\n",
      "                        +- *(2) Project [saleamount#682, Name#789]\n",
      "                           +- *(2) BroadcastHashJoin [customerid#678], [CustomerID#788], Inner, BuildRight, false\n",
      "                              :- *(2) Filter isnotnull(customerid#678)\n",
      "                              :  +- FileScan csv [customerid#678,saleamount#682] Batched: false, DataFilters: [isnotnull(customerid#678)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                              +- BroadcastQueryStage 0\n",
      "                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=561]\n",
      "                                    +- *(1) Filter isnotnull(CustomerID#788)\n",
      "                                       +- FileScan csv [CustomerID#788,Name#789] Batched: false, DataFilters: [isnotnull(CustomerID#788)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (34, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (26, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (25, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (28, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (30, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (29, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (37, 0) from stageTCMP\n",
      "25/04/11 09:37:25 DEBUG ExecutorMetricsPoller: removing (32, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"SmartSales\").getOrCreate()\n",
    "\n",
    "# Register DataFrames as temporary views (if not already done)\n",
    "df_sales.createOrReplaceTempView(\"sales\")\n",
    "df_customer.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "# Write query using Spark SQL\n",
    "df_top_customers = spark.sql(\"\"\"\n",
    "    SELECT c.Name, SUM(s.saleamount) AS total_spent\n",
    "    FROM sales s\n",
    "    JOIN customer c ON s.customerid = c.CustomerID\n",
    "    GROUP BY c.Name\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show Spark results\n",
    "df_top_customers.show()\n",
    "\n",
    "# Convert to Pandas for use with charts\n",
    "import pandas as pd\n",
    "df_top_customers_pd = df_top_customers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0819cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:43:45 DEBUG Analyzer$ResolveReferences: Resolving 'saledate to saledate#878\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'billtype to billtype#684\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'storeid to storeid#680\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'billtype to billtype#684\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'storeid to storeid#680\n",
      "25/04/11 09:43:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:43:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:43:45 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: CollectLimit 21\n",
      "+- HashAggregate(keys=[billtype#684, storeid#680], functions=[sum(saleamount#682)], output=[toprettystring(billtype)#926, toprettystring(storeid)#927, toprettystring(sum(saleamount))#928])\n",
      "   +- HashAggregate(keys=[billtype#684, storeid#680], functions=[partial_sum(saleamount#682)], output=[billtype#684, storeid#680, sum#933])\n",
      "      +- FileScan csv [storeid#680,saleamount#682,billtype#684] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:43:45 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\n",
      "25/04/11 09:43:45 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 032 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 033 */\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 038 */     private int[] buckets;\n",
      "/* 039 */     private int capacity = 1 << 16;\n",
      "/* 040 */     private double loadFactor = 0.5;\n",
      "/* 041 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 042 */     private int maxSteps = 2;\n",
      "/* 043 */     private int numRows = 0;\n",
      "/* 044 */     private Object emptyVBase;\n",
      "/* 045 */     private long emptyVOff;\n",
      "/* 046 */     private int emptyVLen;\n",
      "/* 047 */     private boolean isBatchFull = false;\n",
      "/* 048 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 049 */\n",
      "/* 050 */     public hashAgg_FastHashMap_0(\n",
      "/* 051 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 052 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 053 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 054 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 055 */\n",
      "/* 056 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 057 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 058 */\n",
      "/* 059 */       emptyVBase = emptyBuffer;\n",
      "/* 060 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 061 */       emptyVLen = emptyBuffer.length;\n",
      "/* 062 */\n",
      "/* 063 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 064 */         2, 32);\n",
      "/* 065 */\n",
      "/* 066 */       buckets = new int[numBuckets];\n",
      "/* 067 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 071 */       long h = hash(hashAgg_key_0, hashAgg_key_1);\n",
      "/* 072 */       int step = 0;\n",
      "/* 073 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 074 */       while (step < maxSteps) {\n",
      "/* 075 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 076 */         if (buckets[idx] == -1) {\n",
      "/* 077 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 078 */             agg_rowWriter.reset();\n",
      "/* 079 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 080 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 081 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(hashAgg_key_0)) && (row.getInt(1) == hashAgg_key_1);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 116 */       long hashAgg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int hashAgg_result_0 = 0;\n",
      "/* 119 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 121 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 122 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 128 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 129 */\n",
      "/* 130 */       return hashAgg_hash_0;\n",
      "/* 131 */     }\n",
      "/* 132 */\n",
      "/* 133 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 134 */       return batch.rowIterator();\n",
      "/* 135 */     }\n",
      "/* 136 */\n",
      "/* 137 */     public void close() {\n",
      "/* 138 */       batch.close();\n",
      "/* 139 */     }\n",
      "/* 140 */\n",
      "/* 141 */   }\n",
      "/* 142 */\n",
      "/* 143 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 144 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 145 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 146 */\n",
      "/* 147 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 148 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 149 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 150 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 151 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 152 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 153 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 154 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 155 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 156 */\n",
      "/* 157 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 158 */       // shouldStop check is eliminated\n",
      "/* 159 */     }\n",
      "/* 160 */\n",
      "/* 161 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 162 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 163 */\n",
      "/* 164 */   }\n",
      "/* 165 */\n",
      "/* 166 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, UTF8String hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 167 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 168 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 169 */\n",
      "/* 170 */     if (!hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_0_0) {\n",
      "/* 171 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 172 */         hashAgg_expr_2_0, hashAgg_expr_0_0);\n",
      "/* 173 */     }\n",
      "/* 174 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 175 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 176 */       // generate grouping key\n",
      "/* 177 */       hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 178 */\n",
      "/* 179 */       hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 180 */\n",
      "/* 181 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 182 */         hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 183 */       } else {\n",
      "/* 184 */         hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_2_0);\n",
      "/* 185 */       }\n",
      "/* 186 */\n",
      "/* 187 */       if (hashAgg_exprIsNull_0_0) {\n",
      "/* 188 */         hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 189 */       } else {\n",
      "/* 190 */         hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_0_0);\n",
      "/* 191 */       }\n",
      "/* 192 */       int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 193 */       if (true) {\n",
      "/* 194 */         // try to get the buffer from hash map\n",
      "/* 195 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 196 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 197 */       }\n",
      "/* 198 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 199 */       // aggregation after processing all input rows.\n",
      "/* 200 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 201 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 202 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 203 */         } else {\n",
      "/* 204 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 205 */         }\n",
      "/* 206 */\n",
      "/* 207 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 208 */         // try to allocate buffer again.\n",
      "/* 209 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 210 */           (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 211 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 212 */           // failed to allocate the first page\n",
      "/* 213 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 214 */         }\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */     }\n",
      "/* 218 */\n",
      "/* 219 */     // Updates the proper row buffer\n",
      "/* 220 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 221 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 222 */     }\n",
      "/* 223 */\n",
      "/* 224 */     // common sub-expressions\n",
      "/* 225 */\n",
      "/* 226 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 227 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 232 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 233 */     double hashAgg_value_6 = -1.0;\n",
      "/* 234 */     do {\n",
      "/* 235 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 236 */       double hashAgg_value_7 = -1.0;\n",
      "/* 237 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 238 */       double hashAgg_value_8 = -1.0;\n",
      "/* 239 */       do {\n",
      "/* 240 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 241 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 242 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 243 */         if (!hashAgg_isNull_8) {\n",
      "/* 244 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 245 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 246 */           continue;\n",
      "/* 247 */         }\n",
      "/* 248 */\n",
      "/* 249 */         if (!false) {\n",
      "/* 250 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 251 */           hashAgg_value_8 = 0.0D;\n",
      "/* 252 */           continue;\n",
      "/* 253 */         }\n",
      "/* 254 */\n",
      "/* 255 */       } while (false);\n",
      "/* 256 */\n",
      "/* 257 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 258 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 259 */\n",
      "/* 260 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_1_0;\n",
      "/* 261 */\n",
      "/* 262 */       }\n",
      "/* 263 */       if (!hashAgg_isNull_6) {\n",
      "/* 264 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 265 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 266 */         continue;\n",
      "/* 267 */       }\n",
      "/* 268 */\n",
      "/* 269 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 270 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 271 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 272 */       if (!hashAgg_isNull_11) {\n",
      "/* 273 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 274 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 275 */         continue;\n",
      "/* 276 */       }\n",
      "/* 277 */\n",
      "/* 278 */     } while (false);\n",
      "/* 279 */\n",
      "/* 280 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 281 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 282 */     } else {\n",
      "/* 283 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 288 */   throws java.io.IOException {\n",
      "/* 289 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 290 */\n",
      "/* 291 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 292 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 293 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 294 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 295 */     int hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 296 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 297 */     boolean hashAgg_isNull_14 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 298 */     double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 299 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 300 */\n",
      "/* 301 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 302 */\n",
      "/* 303 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 304 */\n",
      "/* 305 */     if (hashAgg_isNull_12) {\n",
      "/* 306 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_13);\n",
      "/* 309 */     }\n",
      "/* 310 */\n",
      "/* 311 */     if (hashAgg_isNull_13) {\n",
      "/* 312 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 313 */     } else {\n",
      "/* 314 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_14);\n",
      "/* 315 */     }\n",
      "/* 316 */\n",
      "/* 317 */     if (hashAgg_isNull_14) {\n",
      "/* 318 */       hashAgg_mutableStateArray_0[1].setNullAt(2);\n",
      "/* 319 */     } else {\n",
      "/* 320 */       hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_15);\n",
      "/* 321 */     }\n",
      "/* 322 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 323 */\n",
      "/* 324 */   }\n",
      "/* 325 */\n",
      "/* 326 */   protected void processNext() throws java.io.IOException {\n",
      "/* 327 */     if (!hashAgg_initAgg_0) {\n",
      "/* 328 */       hashAgg_initAgg_0 = true;\n",
      "/* 329 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 330 */\n",
      "/* 331 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 332 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 333 */           @Override\n",
      "/* 334 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 335 */             hashAgg_fastHashMap_0.close();\n",
      "/* 336 */           }\n",
      "/* 337 */         });\n",
      "/* 338 */\n",
      "/* 339 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 340 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 341 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 342 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 343 */     }\n",
      "/* 344 */     // output the result\n",
      "/* 345 */\n",
      "/* 346 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 347 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 348 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 349 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 350 */\n",
      "/* 351 */       if (shouldStop()) return;\n",
      "/* 352 */     }\n",
      "/* 353 */     hashAgg_fastHashMap_0.close();\n",
      "/* 354 */\n",
      "/* 355 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 356 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 357 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 358 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 359 */       if (shouldStop()) return;\n",
      "/* 360 */     }\n",
      "/* 361 */     hashAgg_mapIter_0.close();\n",
      "/* 362 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 363 */       hashAgg_hashMap_0.free();\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 032 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 033 */\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 038 */     private int[] buckets;\n",
      "/* 039 */     private int capacity = 1 << 16;\n",
      "/* 040 */     private double loadFactor = 0.5;\n",
      "/* 041 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 042 */     private int maxSteps = 2;\n",
      "/* 043 */     private int numRows = 0;\n",
      "/* 044 */     private Object emptyVBase;\n",
      "/* 045 */     private long emptyVOff;\n",
      "/* 046 */     private int emptyVLen;\n",
      "/* 047 */     private boolean isBatchFull = false;\n",
      "/* 048 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 049 */\n",
      "/* 050 */     public hashAgg_FastHashMap_0(\n",
      "/* 051 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 052 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 053 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 054 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 055 */\n",
      "/* 056 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 057 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 058 */\n",
      "/* 059 */       emptyVBase = emptyBuffer;\n",
      "/* 060 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 061 */       emptyVLen = emptyBuffer.length;\n",
      "/* 062 */\n",
      "/* 063 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 064 */         2, 32);\n",
      "/* 065 */\n",
      "/* 066 */       buckets = new int[numBuckets];\n",
      "/* 067 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 071 */       long h = hash(hashAgg_key_0, hashAgg_key_1);\n",
      "/* 072 */       int step = 0;\n",
      "/* 073 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 074 */       while (step < maxSteps) {\n",
      "/* 075 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 076 */         if (buckets[idx] == -1) {\n",
      "/* 077 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 078 */             agg_rowWriter.reset();\n",
      "/* 079 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 080 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 081 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(hashAgg_key_0)) && (row.getInt(1) == hashAgg_key_1);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 116 */       long hashAgg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int hashAgg_result_0 = 0;\n",
      "/* 119 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 121 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 122 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 128 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 129 */\n",
      "/* 130 */       return hashAgg_hash_0;\n",
      "/* 131 */     }\n",
      "/* 132 */\n",
      "/* 133 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 134 */       return batch.rowIterator();\n",
      "/* 135 */     }\n",
      "/* 136 */\n",
      "/* 137 */     public void close() {\n",
      "/* 138 */       batch.close();\n",
      "/* 139 */     }\n",
      "/* 140 */\n",
      "/* 141 */   }\n",
      "/* 142 */\n",
      "/* 143 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 144 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 145 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 146 */\n",
      "/* 147 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 148 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 149 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 150 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 151 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 152 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 153 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 154 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 155 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 156 */\n",
      "/* 157 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 158 */       // shouldStop check is eliminated\n",
      "/* 159 */     }\n",
      "/* 160 */\n",
      "/* 161 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 162 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 163 */\n",
      "/* 164 */   }\n",
      "/* 165 */\n",
      "/* 166 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, UTF8String hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 167 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 168 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 169 */\n",
      "/* 170 */     if (!hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_0_0) {\n",
      "/* 171 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 172 */         hashAgg_expr_2_0, hashAgg_expr_0_0);\n",
      "/* 173 */     }\n",
      "/* 174 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 175 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 176 */       // generate grouping key\n",
      "/* 177 */       hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 178 */\n",
      "/* 179 */       hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 180 */\n",
      "/* 181 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 182 */         hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 183 */       } else {\n",
      "/* 184 */         hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_2_0);\n",
      "/* 185 */       }\n",
      "/* 186 */\n",
      "/* 187 */       if (hashAgg_exprIsNull_0_0) {\n",
      "/* 188 */         hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 189 */       } else {\n",
      "/* 190 */         hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_0_0);\n",
      "/* 191 */       }\n",
      "/* 192 */       int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 193 */       if (true) {\n",
      "/* 194 */         // try to get the buffer from hash map\n",
      "/* 195 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 196 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 197 */       }\n",
      "/* 198 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 199 */       // aggregation after processing all input rows.\n",
      "/* 200 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 201 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 202 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 203 */         } else {\n",
      "/* 204 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 205 */         }\n",
      "/* 206 */\n",
      "/* 207 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 208 */         // try to allocate buffer again.\n",
      "/* 209 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 210 */           (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 211 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 212 */           // failed to allocate the first page\n",
      "/* 213 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 214 */         }\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */     }\n",
      "/* 218 */\n",
      "/* 219 */     // Updates the proper row buffer\n",
      "/* 220 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 221 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 222 */     }\n",
      "/* 223 */\n",
      "/* 224 */     // common sub-expressions\n",
      "/* 225 */\n",
      "/* 226 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 227 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 232 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 233 */     double hashAgg_value_6 = -1.0;\n",
      "/* 234 */     do {\n",
      "/* 235 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 236 */       double hashAgg_value_7 = -1.0;\n",
      "/* 237 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 238 */       double hashAgg_value_8 = -1.0;\n",
      "/* 239 */       do {\n",
      "/* 240 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 241 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 242 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 243 */         if (!hashAgg_isNull_8) {\n",
      "/* 244 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 245 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 246 */           continue;\n",
      "/* 247 */         }\n",
      "/* 248 */\n",
      "/* 249 */         if (!false) {\n",
      "/* 250 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 251 */           hashAgg_value_8 = 0.0D;\n",
      "/* 252 */           continue;\n",
      "/* 253 */         }\n",
      "/* 254 */\n",
      "/* 255 */       } while (false);\n",
      "/* 256 */\n",
      "/* 257 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 258 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 259 */\n",
      "/* 260 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_1_0;\n",
      "/* 261 */\n",
      "/* 262 */       }\n",
      "/* 263 */       if (!hashAgg_isNull_6) {\n",
      "/* 264 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 265 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 266 */         continue;\n",
      "/* 267 */       }\n",
      "/* 268 */\n",
      "/* 269 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 270 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 271 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 272 */       if (!hashAgg_isNull_11) {\n",
      "/* 273 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 274 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 275 */         continue;\n",
      "/* 276 */       }\n",
      "/* 277 */\n",
      "/* 278 */     } while (false);\n",
      "/* 279 */\n",
      "/* 280 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 281 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 282 */     } else {\n",
      "/* 283 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 288 */   throws java.io.IOException {\n",
      "/* 289 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 290 */\n",
      "/* 291 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 292 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 293 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 294 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 295 */     int hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 296 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 297 */     boolean hashAgg_isNull_14 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 298 */     double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 299 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 300 */\n",
      "/* 301 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 302 */\n",
      "/* 303 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 304 */\n",
      "/* 305 */     if (hashAgg_isNull_12) {\n",
      "/* 306 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_13);\n",
      "/* 309 */     }\n",
      "/* 310 */\n",
      "/* 311 */     if (hashAgg_isNull_13) {\n",
      "/* 312 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 313 */     } else {\n",
      "/* 314 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_14);\n",
      "/* 315 */     }\n",
      "/* 316 */\n",
      "/* 317 */     if (hashAgg_isNull_14) {\n",
      "/* 318 */       hashAgg_mutableStateArray_0[1].setNullAt(2);\n",
      "/* 319 */     } else {\n",
      "/* 320 */       hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_15);\n",
      "/* 321 */     }\n",
      "/* 322 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 323 */\n",
      "/* 324 */   }\n",
      "/* 325 */\n",
      "/* 326 */   protected void processNext() throws java.io.IOException {\n",
      "/* 327 */     if (!hashAgg_initAgg_0) {\n",
      "/* 328 */       hashAgg_initAgg_0 = true;\n",
      "/* 329 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 330 */\n",
      "/* 331 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 332 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 333 */           @Override\n",
      "/* 334 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 335 */             hashAgg_fastHashMap_0.close();\n",
      "/* 336 */           }\n",
      "/* 337 */         });\n",
      "/* 338 */\n",
      "/* 339 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 340 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 341 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 342 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 343 */     }\n",
      "/* 344 */     // output the result\n",
      "/* 345 */\n",
      "/* 346 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 347 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 348 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 349 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 350 */\n",
      "/* 351 */       if (shouldStop()) return;\n",
      "/* 352 */     }\n",
      "/* 353 */     hashAgg_fastHashMap_0.close();\n",
      "/* 354 */\n",
      "/* 355 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 356 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 357 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 358 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 359 */       if (shouldStop()) return;\n",
      "/* 360 */     }\n",
      "/* 361 */     hashAgg_mapIter_0.close();\n",
      "/* 362 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 363 */       hashAgg_hashMap_0.free();\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 38.22278 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 351.8 KiB, free 364.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_57 locally took 1 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_57 without replication took 1 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 364.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_57_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_57_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_57_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_57_piece0 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_57_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO SparkContext: Created broadcast 57 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:43:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 139 took 0.000034 seconds\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Registering RDD 139 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Got map stage job 33 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Final stage: ShuffleMapStage 38 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitStage(ShuffleMapStage 38 (name=showString at NativeMethodAccessorImpl.java:0;jobs=33))\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[139] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 38)\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 42.7 KiB, free 364.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_58 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_58 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 364.0 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_58_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on macbookpro.lan:57375 (size: 19.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_58_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_58_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_58_piece0 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_58_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[139] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Epoch for TaskSet 38.0: 3\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Valid locality levels for TaskSet 38.0: NO_PREF, ANY\n",
      "25/04/11 09:43:45 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_38.0, runningTasks: 0\n",
      "25/04/11 09:43:45 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 33) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:43:45 INFO Executor: Running task 0.0 in stage 38.0 (TID 33)\n",
      "25/04/11 09:43:45 DEBUG ExecutorMetricsPoller: stageTCMP: (38, 0) -> 1\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Getting local block broadcast_58\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Level for block broadcast_58 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 032 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 033 */\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 038 */     private int[] buckets;\n",
      "/* 039 */     private int capacity = 1 << 16;\n",
      "/* 040 */     private double loadFactor = 0.5;\n",
      "/* 041 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 042 */     private int maxSteps = 2;\n",
      "/* 043 */     private int numRows = 0;\n",
      "/* 044 */     private Object emptyVBase;\n",
      "/* 045 */     private long emptyVOff;\n",
      "/* 046 */     private int emptyVLen;\n",
      "/* 047 */     private boolean isBatchFull = false;\n",
      "/* 048 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 049 */\n",
      "/* 050 */     public hashAgg_FastHashMap_0(\n",
      "/* 051 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 052 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 053 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 054 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 055 */\n",
      "/* 056 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 057 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 058 */\n",
      "/* 059 */       emptyVBase = emptyBuffer;\n",
      "/* 060 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 061 */       emptyVLen = emptyBuffer.length;\n",
      "/* 062 */\n",
      "/* 063 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 064 */         2, 32);\n",
      "/* 065 */\n",
      "/* 066 */       buckets = new int[numBuckets];\n",
      "/* 067 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 071 */       long h = hash(hashAgg_key_0, hashAgg_key_1);\n",
      "/* 072 */       int step = 0;\n",
      "/* 073 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 074 */       while (step < maxSteps) {\n",
      "/* 075 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 076 */         if (buckets[idx] == -1) {\n",
      "/* 077 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 078 */             agg_rowWriter.reset();\n",
      "/* 079 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 080 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 081 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(hashAgg_key_0)) && (row.getInt(1) == hashAgg_key_1);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 116 */       long hashAgg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int hashAgg_result_0 = 0;\n",
      "/* 119 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 121 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 122 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 128 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 129 */\n",
      "/* 130 */       return hashAgg_hash_0;\n",
      "/* 131 */     }\n",
      "/* 132 */\n",
      "/* 133 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 134 */       return batch.rowIterator();\n",
      "/* 135 */     }\n",
      "/* 136 */\n",
      "/* 137 */     public void close() {\n",
      "/* 138 */       batch.close();\n",
      "/* 139 */     }\n",
      "/* 140 */\n",
      "/* 141 */   }\n",
      "/* 142 */\n",
      "/* 143 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 144 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 145 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 146 */\n",
      "/* 147 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 148 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 149 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 150 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 151 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 152 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 153 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 154 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 155 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 156 */\n",
      "/* 157 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 158 */       // shouldStop check is eliminated\n",
      "/* 159 */     }\n",
      "/* 160 */\n",
      "/* 161 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 162 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 163 */\n",
      "/* 164 */   }\n",
      "/* 165 */\n",
      "/* 166 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, UTF8String hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 167 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 168 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 169 */\n",
      "/* 170 */     if (!hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_0_0) {\n",
      "/* 171 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 172 */         hashAgg_expr_2_0, hashAgg_expr_0_0);\n",
      "/* 173 */     }\n",
      "/* 174 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 175 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 176 */       // generate grouping key\n",
      "/* 177 */       hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 178 */\n",
      "/* 179 */       hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 180 */\n",
      "/* 181 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 182 */         hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 183 */       } else {\n",
      "/* 184 */         hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_2_0);\n",
      "/* 185 */       }\n",
      "/* 186 */\n",
      "/* 187 */       if (hashAgg_exprIsNull_0_0) {\n",
      "/* 188 */         hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 189 */       } else {\n",
      "/* 190 */         hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_0_0);\n",
      "/* 191 */       }\n",
      "/* 192 */       int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 193 */       if (true) {\n",
      "/* 194 */         // try to get the buffer from hash map\n",
      "/* 195 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 196 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 197 */       }\n",
      "/* 198 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 199 */       // aggregation after processing all input rows.\n",
      "/* 200 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 201 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 202 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 203 */         } else {\n",
      "/* 204 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 205 */         }\n",
      "/* 206 */\n",
      "/* 207 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 208 */         // try to allocate buffer again.\n",
      "/* 209 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 210 */           (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 211 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 212 */           // failed to allocate the first page\n",
      "/* 213 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 214 */         }\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */     }\n",
      "/* 218 */\n",
      "/* 219 */     // Updates the proper row buffer\n",
      "/* 220 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 221 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 222 */     }\n",
      "/* 223 */\n",
      "/* 224 */     // common sub-expressions\n",
      "/* 225 */\n",
      "/* 226 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 227 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 232 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 233 */     double hashAgg_value_6 = -1.0;\n",
      "/* 234 */     do {\n",
      "/* 235 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 236 */       double hashAgg_value_7 = -1.0;\n",
      "/* 237 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 238 */       double hashAgg_value_8 = -1.0;\n",
      "/* 239 */       do {\n",
      "/* 240 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 241 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 242 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 243 */         if (!hashAgg_isNull_8) {\n",
      "/* 244 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 245 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 246 */           continue;\n",
      "/* 247 */         }\n",
      "/* 248 */\n",
      "/* 249 */         if (!false) {\n",
      "/* 250 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 251 */           hashAgg_value_8 = 0.0D;\n",
      "/* 252 */           continue;\n",
      "/* 253 */         }\n",
      "/* 254 */\n",
      "/* 255 */       } while (false);\n",
      "/* 256 */\n",
      "/* 257 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 258 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 259 */\n",
      "/* 260 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_1_0;\n",
      "/* 261 */\n",
      "/* 262 */       }\n",
      "/* 263 */       if (!hashAgg_isNull_6) {\n",
      "/* 264 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 265 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 266 */         continue;\n",
      "/* 267 */       }\n",
      "/* 268 */\n",
      "/* 269 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 270 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 271 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 272 */       if (!hashAgg_isNull_11) {\n",
      "/* 273 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 274 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 275 */         continue;\n",
      "/* 276 */       }\n",
      "/* 277 */\n",
      "/* 278 */     } while (false);\n",
      "/* 279 */\n",
      "/* 280 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 281 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 282 */     } else {\n",
      "/* 283 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 288 */   throws java.io.IOException {\n",
      "/* 289 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 290 */\n",
      "/* 291 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 292 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 293 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 294 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 295 */     int hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 296 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 297 */     boolean hashAgg_isNull_14 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 298 */     double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 299 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 300 */\n",
      "/* 301 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 302 */\n",
      "/* 303 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 304 */\n",
      "/* 305 */     if (hashAgg_isNull_12) {\n",
      "/* 306 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_13);\n",
      "/* 309 */     }\n",
      "/* 310 */\n",
      "/* 311 */     if (hashAgg_isNull_13) {\n",
      "/* 312 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 313 */     } else {\n",
      "/* 314 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_14);\n",
      "/* 315 */     }\n",
      "/* 316 */\n",
      "/* 317 */     if (hashAgg_isNull_14) {\n",
      "/* 318 */       hashAgg_mutableStateArray_0[1].setNullAt(2);\n",
      "/* 319 */     } else {\n",
      "/* 320 */       hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_15);\n",
      "/* 321 */     }\n",
      "/* 322 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 323 */\n",
      "/* 324 */   }\n",
      "/* 325 */\n",
      "/* 326 */   protected void processNext() throws java.io.IOException {\n",
      "/* 327 */     if (!hashAgg_initAgg_0) {\n",
      "/* 328 */       hashAgg_initAgg_0 = true;\n",
      "/* 329 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 330 */\n",
      "/* 331 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 332 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 333 */           @Override\n",
      "/* 334 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 335 */             hashAgg_fastHashMap_0.close();\n",
      "/* 336 */           }\n",
      "/* 337 */         });\n",
      "/* 338 */\n",
      "/* 339 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 340 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 341 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 342 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 343 */     }\n",
      "/* 344 */     // output the result\n",
      "/* 345 */\n",
      "/* 346 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 347 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 348 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 349 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 350 */\n",
      "/* 351 */       if (shouldStop()) return;\n",
      "/* 352 */     }\n",
      "/* 353 */     hashAgg_fastHashMap_0.close();\n",
      "/* 354 */\n",
      "/* 355 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 356 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 357 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 358 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 359 */       if (shouldStop()) return;\n",
      "/* 360 */     }\n",
      "/* 361 */     hashAgg_mapIter_0.close();\n",
      "/* 362 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 363 */       hashAgg_hashMap_0.free();\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 22.656916 ms\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], input[1, int, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */\n",
      "/* 050 */       int remainder_0 = value_1 % 200;\n",
      "/* 051 */       if (remainder_0 < 0) {\n",
      "/* 052 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 053 */       } else {\n",
      "/* 054 */         value_0=remainder_0;\n",
      "/* 055 */       }\n",
      "/* 056 */\n",
      "/* 057 */     }\n",
      "/* 058 */     if (isNull_0) {\n",
      "/* 059 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 064 */   }\n",
      "/* 065 */\n",
      "/* 066 */\n",
      "/* 067 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */\n",
      "/* 050 */       int remainder_0 = value_1 % 200;\n",
      "/* 051 */       if (remainder_0 < 0) {\n",
      "/* 052 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 053 */       } else {\n",
      "/* 054 */         value_0=remainder_0;\n",
      "/* 055 */       }\n",
      "/* 056 */\n",
      "/* 057 */     }\n",
      "/* 058 */     if (isNull_0) {\n",
      "/* 059 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 064 */   }\n",
      "/* 065 */\n",
      "/* 066 */\n",
      "/* 067 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 7.159506 ms\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 33 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@3a9ff9e0\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 4.477176 ms\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 33 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@3ce9ec89\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true],input[2, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 051 */     null : (i.getUTF8String(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 051 */     null : (i.getUTF8String(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 4.392094 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Getting local block broadcast_57\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Level for block broadcast_57 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 33 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@3ce9ec89\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 33 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@3a9ff9e0\n",
      "25/04/11 09:43:45 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 33 with length 200\n",
      "25/04/11 09:43:45 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 33: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,81,0,0,0,0,0,0,81,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,81,0,0,0,0,0,0,0,0,81,81,0,0,81,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0]\n",
      "25/04/11 09:43:45 INFO Executor: Finished task 0.0 in stage 38.0 (TID 33). 2724 bytes result sent to driver\n",
      "25/04/11 09:43:45 DEBUG ExecutorMetricsPoller: stageTCMP: (38, 0) -> 0\n",
      "25/04/11 09:43:45 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 33) in 90 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:43:45 INFO DAGScheduler: ShuffleMapStage 38 (showString at NativeMethodAccessorImpl.java:0) finished in 0.094 s\n",
      "25/04/11 09:43:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:43:45 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:43:45 DEBUG MapOutputTrackerMaster: Increasing epoch to 4\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: After removal of stage 38, remaining stages = 0\n",
      "25/04/11 09:43:45 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=720.0 B, rowCount=18) for plan: HashAggregate(keys=[billtype#684, storeid#680], functions=[sum(saleamount#682)], output=[toprettystring(billtype)#926, toprettystring(storeid)#927, toprettystring(sum(saleamount))#928])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange hashpartitioning(billtype#684, storeid#680, 200), ENSURE_REQUIREMENTS, [plan_id=702]\n",
      "      +- *(1) HashAggregate(keys=[billtype#684, storeid#680], functions=[partial_sum(saleamount#682)], output=[billtype#684, storeid#680, sum#933])\n",
      "         +- FileScan csv [storeid#680,saleamount#682,billtype#684] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:43:45 DEBUG AdaptiveSparkPlanExec: Plan changed:\n",
      "!CollectLimit 21                                                                                                                                                                                                                                                                                                                                  HashAggregate(keys=[billtype#684, storeid#680], functions=[sum(saleamount#682)], output=[toprettystring(billtype)#926, toprettystring(storeid)#927, toprettystring(sum(saleamount))#928])\n",
      "!+- HashAggregate(keys=[billtype#684, storeid#680], functions=[sum(saleamount#682)], output=[toprettystring(billtype)#926, toprettystring(storeid)#927, toprettystring(sum(saleamount))#928])                                                                                                                                                     +- ShuffleQueryStage 0\n",
      "!   +- ShuffleQueryStage 0                                                                                                                                                                                                                                                                                                                           +- Exchange hashpartitioning(billtype#684, storeid#680, 200), ENSURE_REQUIREMENTS, [plan_id=702]\n",
      "!      +- Exchange hashpartitioning(billtype#684, storeid#680, 200), ENSURE_REQUIREMENTS, [plan_id=702]                                                                                                                                                                                                                                                 +- *(1) HashAggregate(keys=[billtype#684, storeid#680], functions=[partial_sum(saleamount#682)], output=[billtype#684, storeid#680, sum#933])\n",
      "!         +- *(1) HashAggregate(keys=[billtype#684, storeid#680], functions=[partial_sum(saleamount#682)], output=[billtype#684, storeid#680, sum#933])                                                                                                                                                                                                    +- FileScan csv [storeid#680,saleamount#682,billtype#684] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "!            +- FileScan csv [storeid#680,saleamount#682,billtype#684] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>   \n",
      "25/04/11 09:43:45 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:43:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:43:45 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 068 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 069 */     } else {\n",
      "/* 070 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 071 */     }\n",
      "/* 072 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 073 */     if (true) {\n",
      "/* 074 */       // try to get the buffer from hash map\n",
      "/* 075 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 076 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 077 */     }\n",
      "/* 078 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 079 */     // aggregation after processing all input rows.\n",
      "/* 080 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 081 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 082 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 083 */       } else {\n",
      "/* 084 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 088 */       // try to allocate buffer again.\n",
      "/* 089 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 090 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 091 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 092 */         // failed to allocate the first page\n",
      "/* 093 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 094 */       }\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     // common sub-expressions\n",
      "/* 098 */\n",
      "/* 099 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 100 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 101 */\n",
      "/* 102 */   }\n",
      "/* 103 */\n",
      "/* 104 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 105 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 106 */     double hashAgg_value_4 = -1.0;\n",
      "/* 107 */     do {\n",
      "/* 108 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 109 */       double hashAgg_value_5 = -1.0;\n",
      "/* 110 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 111 */       double hashAgg_value_6 = -1.0;\n",
      "/* 112 */       do {\n",
      "/* 113 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 114 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 115 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 116 */         if (!hashAgg_isNull_7) {\n",
      "/* 117 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 118 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 119 */           continue;\n",
      "/* 120 */         }\n",
      "/* 121 */\n",
      "/* 122 */         if (!false) {\n",
      "/* 123 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 124 */           hashAgg_value_6 = 0.0D;\n",
      "/* 125 */           continue;\n",
      "/* 126 */         }\n",
      "/* 127 */\n",
      "/* 128 */       } while (false);\n",
      "/* 129 */\n",
      "/* 130 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 131 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 132 */\n",
      "/* 133 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 134 */\n",
      "/* 135 */       }\n",
      "/* 136 */       if (!hashAgg_isNull_5) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 138 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 143 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 144 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 145 */       if (!hashAgg_isNull_10) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 147 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */     } while (false);\n",
      "/* 152 */\n",
      "/* 153 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 154 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 157 */     }\n",
      "/* 158 */   }\n",
      "/* 159 */\n",
      "/* 160 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 161 */   throws java.io.IOException {\n",
      "/* 162 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 163 */\n",
      "/* 164 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 165 */     UTF8String hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 166 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 167 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 168 */     int hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 169 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 170 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 171 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 172 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 173 */\n",
      "/* 174 */     UTF8String hashAgg_value_15;\n",
      "/* 175 */     if (hashAgg_isNull_11) {\n",
      "/* 176 */       hashAgg_value_15 = UTF8String.fromString(\"NULL\");\n",
      "/* 177 */     } else {\n",
      "/* 178 */       hashAgg_value_15 = hashAgg_value_11;\n",
      "/* 179 */     }\n",
      "/* 180 */     UTF8String hashAgg_value_17;\n",
      "/* 181 */     if (hashAgg_isNull_12) {\n",
      "/* 182 */       hashAgg_value_17 = UTF8String.fromString(\"NULL\");\n",
      "/* 183 */     } else {\n",
      "/* 184 */       hashAgg_value_17 = UTF8String.fromString(String.valueOf(hashAgg_value_12));\n",
      "/* 185 */     }\n",
      "/* 186 */     UTF8String hashAgg_value_19;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_19 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_19 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 193 */\n",
      "/* 194 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_15);\n",
      "/* 195 */\n",
      "/* 196 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_17);\n",
      "/* 197 */\n",
      "/* 198 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_19);\n",
      "/* 199 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 200 */\n",
      "/* 201 */   }\n",
      "/* 202 */\n",
      "/* 203 */   protected void processNext() throws java.io.IOException {\n",
      "/* 204 */     if (!hashAgg_initAgg_0) {\n",
      "/* 205 */       hashAgg_initAgg_0 = true;\n",
      "/* 206 */\n",
      "/* 207 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 208 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 209 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 210 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 211 */     }\n",
      "/* 212 */     // output the result\n",
      "/* 213 */\n",
      "/* 214 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 215 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 216 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 217 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 218 */       if (shouldStop()) return;\n",
      "/* 219 */     }\n",
      "/* 220 */     hashAgg_mapIter_0.close();\n",
      "/* 221 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 222 */       hashAgg_hashMap_0.free();\n",
      "/* 223 */     }\n",
      "/* 224 */   }\n",
      "/* 225 */\n",
      "/* 226 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 068 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 069 */     } else {\n",
      "/* 070 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 071 */     }\n",
      "/* 072 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 073 */     if (true) {\n",
      "/* 074 */       // try to get the buffer from hash map\n",
      "/* 075 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 076 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 077 */     }\n",
      "/* 078 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 079 */     // aggregation after processing all input rows.\n",
      "/* 080 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 081 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 082 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 083 */       } else {\n",
      "/* 084 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 088 */       // try to allocate buffer again.\n",
      "/* 089 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 090 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 091 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 092 */         // failed to allocate the first page\n",
      "/* 093 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 094 */       }\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     // common sub-expressions\n",
      "/* 098 */\n",
      "/* 099 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 100 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 101 */\n",
      "/* 102 */   }\n",
      "/* 103 */\n",
      "/* 104 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 105 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 106 */     double hashAgg_value_4 = -1.0;\n",
      "/* 107 */     do {\n",
      "/* 108 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 109 */       double hashAgg_value_5 = -1.0;\n",
      "/* 110 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 111 */       double hashAgg_value_6 = -1.0;\n",
      "/* 112 */       do {\n",
      "/* 113 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 114 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 115 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 116 */         if (!hashAgg_isNull_7) {\n",
      "/* 117 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 118 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 119 */           continue;\n",
      "/* 120 */         }\n",
      "/* 121 */\n",
      "/* 122 */         if (!false) {\n",
      "/* 123 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 124 */           hashAgg_value_6 = 0.0D;\n",
      "/* 125 */           continue;\n",
      "/* 126 */         }\n",
      "/* 127 */\n",
      "/* 128 */       } while (false);\n",
      "/* 129 */\n",
      "/* 130 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 131 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 132 */\n",
      "/* 133 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 134 */\n",
      "/* 135 */       }\n",
      "/* 136 */       if (!hashAgg_isNull_5) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 138 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 143 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 144 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 145 */       if (!hashAgg_isNull_10) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 147 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */     } while (false);\n",
      "/* 152 */\n",
      "/* 153 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 154 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 157 */     }\n",
      "/* 158 */   }\n",
      "/* 159 */\n",
      "/* 160 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 161 */   throws java.io.IOException {\n",
      "/* 162 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 163 */\n",
      "/* 164 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 165 */     UTF8String hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 166 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 167 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 168 */     int hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 169 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 170 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 171 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 172 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 173 */\n",
      "/* 174 */     UTF8String hashAgg_value_15;\n",
      "/* 175 */     if (hashAgg_isNull_11) {\n",
      "/* 176 */       hashAgg_value_15 = UTF8String.fromString(\"NULL\");\n",
      "/* 177 */     } else {\n",
      "/* 178 */       hashAgg_value_15 = hashAgg_value_11;\n",
      "/* 179 */     }\n",
      "/* 180 */     UTF8String hashAgg_value_17;\n",
      "/* 181 */     if (hashAgg_isNull_12) {\n",
      "/* 182 */       hashAgg_value_17 = UTF8String.fromString(\"NULL\");\n",
      "/* 183 */     } else {\n",
      "/* 184 */       hashAgg_value_17 = UTF8String.fromString(String.valueOf(hashAgg_value_12));\n",
      "/* 185 */     }\n",
      "/* 186 */     UTF8String hashAgg_value_19;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_19 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_19 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 193 */\n",
      "/* 194 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_15);\n",
      "/* 195 */\n",
      "/* 196 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_17);\n",
      "/* 197 */\n",
      "/* 198 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_19);\n",
      "/* 199 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 200 */\n",
      "/* 201 */   }\n",
      "/* 202 */\n",
      "/* 203 */   protected void processNext() throws java.io.IOException {\n",
      "/* 204 */     if (!hashAgg_initAgg_0) {\n",
      "/* 205 */       hashAgg_initAgg_0 = true;\n",
      "/* 206 */\n",
      "/* 207 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 208 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 209 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 210 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 211 */     }\n",
      "/* 212 */     // output the result\n",
      "/* 213 */\n",
      "/* 214 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 215 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 216 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 217 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 218 */       if (shouldStop()) return;\n",
      "/* 219 */     }\n",
      "/* 220 */     hashAgg_mapIter_0.close();\n",
      "/* 221 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 222 */       hashAgg_hashMap_0.free();\n",
      "/* 223 */     }\n",
      "/* 224 */   }\n",
      "/* 225 */\n",
      "/* 226 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 11.613873 ms\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:43:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 142 took 0.000068 seconds\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Got job 34 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Final stage: ResultStage 40 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitStage(ResultStage 40 (name=showString at NativeMethodAccessorImpl.java:0;jobs=34))\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[142] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitMissingTasks(ResultStage 40)\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 45.4 KiB, free 364.0 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_59 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_59 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 364.0 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_59_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on macbookpro.lan:57375 (size: 21.1 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_59_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_59_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_59_piece0 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_59_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[142] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Epoch for TaskSet 40.0: 4\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Valid locality levels for TaskSet 40.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:43:45 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_40.0, runningTasks: 0\n",
      "25/04/11 09:43:45 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 34) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:43:45 INFO Executor: Running task 0.0 in stage 40.0 (TID 34)\n",
      "25/04/11 09:43:45 DEBUG ExecutorMetricsPoller: stageTCMP: (40, 0) -> 1\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Getting local block broadcast_59\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Level for block broadcast_59 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:45 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 3\n",
      "25/04/11 09:43:45 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 3, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:43:45 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:43:45 INFO ShuffleBlockFetcherIterator: Getting 1 (1576.0 B) non-empty blocks including 1 (1576.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:43:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:43:45 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_3_33_14_196,0)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Getting local shuffle block shuffle_3_33_14_196\n",
      "25/04/11 09:43:45 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 068 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 069 */     } else {\n",
      "/* 070 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 071 */     }\n",
      "/* 072 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 073 */     if (true) {\n",
      "/* 074 */       // try to get the buffer from hash map\n",
      "/* 075 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 076 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 077 */     }\n",
      "/* 078 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 079 */     // aggregation after processing all input rows.\n",
      "/* 080 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 081 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 082 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 083 */       } else {\n",
      "/* 084 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 088 */       // try to allocate buffer again.\n",
      "/* 089 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 090 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 091 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 092 */         // failed to allocate the first page\n",
      "/* 093 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 094 */       }\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     // common sub-expressions\n",
      "/* 098 */\n",
      "/* 099 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 100 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 101 */\n",
      "/* 102 */   }\n",
      "/* 103 */\n",
      "/* 104 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 105 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 106 */     double hashAgg_value_4 = -1.0;\n",
      "/* 107 */     do {\n",
      "/* 108 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 109 */       double hashAgg_value_5 = -1.0;\n",
      "/* 110 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 111 */       double hashAgg_value_6 = -1.0;\n",
      "/* 112 */       do {\n",
      "/* 113 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 114 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 115 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 116 */         if (!hashAgg_isNull_7) {\n",
      "/* 117 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 118 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 119 */           continue;\n",
      "/* 120 */         }\n",
      "/* 121 */\n",
      "/* 122 */         if (!false) {\n",
      "/* 123 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 124 */           hashAgg_value_6 = 0.0D;\n",
      "/* 125 */           continue;\n",
      "/* 126 */         }\n",
      "/* 127 */\n",
      "/* 128 */       } while (false);\n",
      "/* 129 */\n",
      "/* 130 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 131 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 132 */\n",
      "/* 133 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 134 */\n",
      "/* 135 */       }\n",
      "/* 136 */       if (!hashAgg_isNull_5) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 138 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 143 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 144 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 145 */       if (!hashAgg_isNull_10) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 147 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */     } while (false);\n",
      "/* 152 */\n",
      "/* 153 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 154 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 157 */     }\n",
      "/* 158 */   }\n",
      "/* 159 */\n",
      "/* 160 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 161 */   throws java.io.IOException {\n",
      "/* 162 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 163 */\n",
      "/* 164 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 165 */     UTF8String hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 166 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 167 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 168 */     int hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 169 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 170 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 171 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 172 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 173 */\n",
      "/* 174 */     UTF8String hashAgg_value_15;\n",
      "/* 175 */     if (hashAgg_isNull_11) {\n",
      "/* 176 */       hashAgg_value_15 = UTF8String.fromString(\"NULL\");\n",
      "/* 177 */     } else {\n",
      "/* 178 */       hashAgg_value_15 = hashAgg_value_11;\n",
      "/* 179 */     }\n",
      "/* 180 */     UTF8String hashAgg_value_17;\n",
      "/* 181 */     if (hashAgg_isNull_12) {\n",
      "/* 182 */       hashAgg_value_17 = UTF8String.fromString(\"NULL\");\n",
      "/* 183 */     } else {\n",
      "/* 184 */       hashAgg_value_17 = UTF8String.fromString(String.valueOf(hashAgg_value_12));\n",
      "/* 185 */     }\n",
      "/* 186 */     UTF8String hashAgg_value_19;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_19 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_19 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 193 */\n",
      "/* 194 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_15);\n",
      "/* 195 */\n",
      "/* 196 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_17);\n",
      "/* 197 */\n",
      "/* 198 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_19);\n",
      "/* 199 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 200 */\n",
      "/* 201 */   }\n",
      "/* 202 */\n",
      "/* 203 */   protected void processNext() throws java.io.IOException {\n",
      "/* 204 */     if (!hashAgg_initAgg_0) {\n",
      "/* 205 */       hashAgg_initAgg_0 = true;\n",
      "/* 206 */\n",
      "/* 207 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 208 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 209 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 210 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 211 */     }\n",
      "/* 212 */     // output the result\n",
      "/* 213 */\n",
      "/* 214 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 215 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 216 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 217 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 218 */       if (shouldStop()) return;\n",
      "/* 219 */     }\n",
      "/* 220 */     hashAgg_mapIter_0.close();\n",
      "/* 221 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 222 */       hashAgg_hashMap_0.free();\n",
      "/* 223 */     }\n",
      "/* 224 */   }\n",
      "/* 225 */\n",
      "/* 226 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 15.309463 ms\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 34 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@4d7f80d3\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 34 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@4d7f80d3\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 34 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@4d7f80d3\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 34 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@4d7f80d3\n",
      "25/04/11 09:43:45 INFO Executor: Finished task 0.0 in stage 40.0 (TID 34). 5613 bytes result sent to driver\n",
      "25/04/11 09:43:45 DEBUG ExecutorMetricsPoller: stageTCMP: (40, 0) -> 0\n",
      "25/04/11 09:43:45 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 34) in 27 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:43:45 INFO DAGScheduler: ResultStage 40 (showString at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: After removal of stage 40, remaining stages = 1\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: After removal of stage 39, remaining stages = 0\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Job 34 finished: showString at NativeMethodAccessorImpl.java:0, took 0.035512 s\n",
      "25/04/11 09:43:45 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(2) HashAggregate(keys=[billtype#684, storeid#680], functions=[sum(saleamount#682)], output=[toprettystring(billtype)#926, toprettystring(storeid)#927, toprettystring(sum(saleamount))#928])\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange hashpartitioning(billtype#684, storeid#680, 200), ENSURE_REQUIREMENTS, [plan_id=702]\n",
      "         +- *(1) HashAggregate(keys=[billtype#684, storeid#680], functions=[partial_sum(saleamount#682)], output=[billtype#684, storeid#680, sum#933])\n",
      "            +- FileScan csv [storeid#680,saleamount#682,billtype#684] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, StructField(toprettystring(billtype),StringType,false), StructField(toprettystring(storeid),StringType,false), StructField(toprettystring(sum(saleamount)),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 5.977235 ms\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'year to year#943\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'quarter to quarter#954\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'month to month#966\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'year to year#943\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'quarter to quarter#954\n",
      "25/04/11 09:43:45 DEBUG ResolveReferencesInAggregate: Resolving 'month to month#966\n",
      "25/04/11 09:43:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:43:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:43:45 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: CollectLimit 21\n",
      "+- HashAggregate(keys=[year#943, quarter#954, month#966], functions=[sum(saleamount#682)], output=[toprettystring(year)#1001, toprettystring(quarter)#1002, toprettystring(month)#1003, toprettystring(sum(saleamount))#1004])\n",
      "   +- HashAggregate(keys=[year#943, quarter#954, month#966], functions=[partial_sum(saleamount#682)], output=[year#943, quarter#954, month#966, sum#1010])\n",
      "      +- Project [saleamount#682, year(saledate#899) AS year#943, quarter(saledate#899) AS quarter#954, month(saledate#899) AS month#966]\n",
      "         +- Project [cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(saledate#677, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#899, saleamount#682]\n",
      "            +- FileScan csv [saledate#677,saleamount#682] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:43:45 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\n",
      "25/04/11 09:43:45 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_7 = true;\n",
      "/* 151 */       long project_value_7 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_7 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[10] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_7 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_7 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_6 = project_isNull_7;\n",
      "/* 168 */       int project_value_6 = -1;\n",
      "/* 169 */       if (!project_isNull_7) {\n",
      "/* 170 */         project_value_6 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_7, ((java.time.ZoneId) references[12] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */       boolean project_isNull_5 = project_isNull_6;\n",
      "/* 174 */       long project_value_5 = -1L;\n",
      "/* 175 */       if (!project_isNull_5) {\n",
      "/* 176 */         project_value_5 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_6, ((java.time.ZoneId) references[9] /* zoneId */)) / 1;\n",
      "/* 177 */       }\n",
      "/* 178 */       boolean project_isNull_4 = project_isNull_5;\n",
      "/* 179 */       int project_value_4 = -1;\n",
      "/* 180 */       if (!project_isNull_5) {\n",
      "/* 181 */         project_value_4 =\n",
      "/* 182 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_5, ((java.time.ZoneId) references[13] /* zoneId */));\n",
      "/* 183 */       }\n",
      "/* 184 */       boolean project_isNull_3 = project_isNull_4;\n",
      "/* 185 */       long project_value_3 = -1L;\n",
      "/* 186 */       if (!project_isNull_3) {\n",
      "/* 187 */         project_value_3 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_4, ((java.time.ZoneId) references[8] /* zoneId */)) / 1;\n",
      "/* 188 */       }\n",
      "/* 189 */       boolean project_isNull_2 = project_isNull_3;\n",
      "/* 190 */       int project_value_2 = -1;\n",
      "/* 191 */       if (!project_isNull_3) {\n",
      "/* 192 */         project_value_2 =\n",
      "/* 193 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_3, ((java.time.ZoneId) references[14] /* zoneId */));\n",
      "/* 194 */       }\n",
      "/* 195 */       boolean project_isNull_1 = project_isNull_2;\n",
      "/* 196 */       long project_value_1 = -1L;\n",
      "/* 197 */       if (!project_isNull_1) {\n",
      "/* 198 */         project_value_1 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_2, ((java.time.ZoneId) references[7] /* zoneId */)) / 1;\n",
      "/* 199 */       }\n",
      "/* 200 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 201 */       int project_value_0 = -1;\n",
      "/* 202 */       if (!project_isNull_1) {\n",
      "/* 203 */         project_value_0 =\n",
      "/* 204 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[15] /* zoneId */));\n",
      "/* 205 */       }\n",
      "/* 206 */\n",
      "/* 207 */       // common sub-expressions\n",
      "/* 208 */\n",
      "/* 209 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 210 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 211 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 212 */       boolean project_isNull_14 = project_isNull_0;\n",
      "/* 213 */       int project_value_14 = -1;\n",
      "/* 214 */\n",
      "/* 215 */       if (!project_isNull_0) {\n",
      "/* 216 */         project_value_14 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 217 */       }\n",
      "/* 218 */       boolean project_isNull_16 = project_isNull_0;\n",
      "/* 219 */       int project_value_16 = -1;\n",
      "/* 220 */\n",
      "/* 221 */       if (!project_isNull_0) {\n",
      "/* 222 */         project_value_16 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 223 */       }\n",
      "/* 224 */       boolean project_isNull_18 = project_isNull_0;\n",
      "/* 225 */       int project_value_18 = -1;\n",
      "/* 226 */\n",
      "/* 227 */       if (!project_isNull_0) {\n",
      "/* 228 */         project_value_18 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 229 */       }\n",
      "/* 230 */\n",
      "/* 231 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_14, project_isNull_14, project_value_16, project_isNull_16, project_value_18, project_isNull_18);\n",
      "/* 232 */       // shouldStop check is eliminated\n",
      "/* 233 */     }\n",
      "/* 234 */\n",
      "/* 235 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 236 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 237 */\n",
      "/* 238 */   }\n",
      "/* 239 */\n",
      "/* 240 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 241 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 242 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 243 */\n",
      "/* 244 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 245 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 246 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 247 */     }\n",
      "/* 248 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 249 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 250 */       // generate grouping key\n",
      "/* 251 */       project_mutableStateArray_0[3].reset();\n",
      "/* 252 */\n",
      "/* 253 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 254 */\n",
      "/* 255 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 256 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 257 */       } else {\n",
      "/* 258 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 259 */       }\n",
      "/* 260 */\n",
      "/* 261 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 262 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 263 */       } else {\n",
      "/* 264 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 265 */       }\n",
      "/* 266 */\n",
      "/* 267 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 268 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 269 */       } else {\n",
      "/* 270 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 271 */       }\n",
      "/* 272 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 273 */       if (true) {\n",
      "/* 274 */         // try to get the buffer from hash map\n",
      "/* 275 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 276 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 277 */       }\n",
      "/* 278 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 279 */       // aggregation after processing all input rows.\n",
      "/* 280 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 281 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 282 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 283 */         } else {\n",
      "/* 284 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 285 */         }\n",
      "/* 286 */\n",
      "/* 287 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 288 */         // try to allocate buffer again.\n",
      "/* 289 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 290 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 291 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 292 */           // failed to allocate the first page\n",
      "/* 293 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 294 */         }\n",
      "/* 295 */       }\n",
      "/* 296 */\n",
      "/* 297 */     }\n",
      "/* 298 */\n",
      "/* 299 */     // Updates the proper row buffer\n",
      "/* 300 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 301 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 302 */     }\n",
      "/* 303 */\n",
      "/* 304 */     // common sub-expressions\n",
      "/* 305 */\n",
      "/* 306 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 307 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 308 */\n",
      "/* 309 */   }\n",
      "/* 310 */\n",
      "/* 311 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 312 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 313 */     double hashAgg_value_12 = -1.0;\n",
      "/* 314 */     do {\n",
      "/* 315 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 316 */       double hashAgg_value_13 = -1.0;\n",
      "/* 317 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 318 */       double hashAgg_value_14 = -1.0;\n",
      "/* 319 */       do {\n",
      "/* 320 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 321 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 322 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 323 */         if (!hashAgg_isNull_14) {\n",
      "/* 324 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 325 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 326 */           continue;\n",
      "/* 327 */         }\n",
      "/* 328 */\n",
      "/* 329 */         if (!false) {\n",
      "/* 330 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 331 */           hashAgg_value_14 = 0.0D;\n",
      "/* 332 */           continue;\n",
      "/* 333 */         }\n",
      "/* 334 */\n",
      "/* 335 */       } while (false);\n",
      "/* 336 */\n",
      "/* 337 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 338 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 339 */\n",
      "/* 340 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 341 */\n",
      "/* 342 */       }\n",
      "/* 343 */       if (!hashAgg_isNull_12) {\n",
      "/* 344 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 345 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 346 */         continue;\n",
      "/* 347 */       }\n",
      "/* 348 */\n",
      "/* 349 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 350 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 351 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 352 */       if (!hashAgg_isNull_17) {\n",
      "/* 353 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 354 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 355 */         continue;\n",
      "/* 356 */       }\n",
      "/* 357 */\n",
      "/* 358 */     } while (false);\n",
      "/* 359 */\n",
      "/* 360 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 361 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 362 */     } else {\n",
      "/* 363 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 368 */   throws java.io.IOException {\n",
      "/* 369 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[16] /* numOutputRows */).add(1);\n",
      "/* 370 */\n",
      "/* 371 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 372 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 373 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 374 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 375 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 376 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 377 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 378 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 379 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 380 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 381 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 382 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 383 */\n",
      "/* 384 */     project_mutableStateArray_0[4].reset();\n",
      "/* 385 */\n",
      "/* 386 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 387 */\n",
      "/* 388 */     if (hashAgg_isNull_18) {\n",
      "/* 389 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 390 */     } else {\n",
      "/* 391 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 392 */     }\n",
      "/* 393 */\n",
      "/* 394 */     if (hashAgg_isNull_19) {\n",
      "/* 395 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 396 */     } else {\n",
      "/* 397 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 398 */     }\n",
      "/* 399 */\n",
      "/* 400 */     if (hashAgg_isNull_20) {\n",
      "/* 401 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 402 */     } else {\n",
      "/* 403 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 404 */     }\n",
      "/* 405 */\n",
      "/* 406 */     if (hashAgg_isNull_21) {\n",
      "/* 407 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 408 */     } else {\n",
      "/* 409 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 410 */     }\n",
      "/* 411 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 412 */\n",
      "/* 413 */   }\n",
      "/* 414 */\n",
      "/* 415 */   protected void processNext() throws java.io.IOException {\n",
      "/* 416 */     if (!hashAgg_initAgg_0) {\n",
      "/* 417 */       hashAgg_initAgg_0 = true;\n",
      "/* 418 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 419 */\n",
      "/* 420 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 421 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 422 */           @Override\n",
      "/* 423 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 424 */             hashAgg_fastHashMap_0.close();\n",
      "/* 425 */           }\n",
      "/* 426 */         });\n",
      "/* 427 */\n",
      "/* 428 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 429 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 430 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 431 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[17] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 432 */     }\n",
      "/* 433 */     // output the result\n",
      "/* 434 */\n",
      "/* 435 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 436 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 437 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 438 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 439 */\n",
      "/* 440 */       if (shouldStop()) return;\n",
      "/* 441 */     }\n",
      "/* 442 */     hashAgg_fastHashMap_0.close();\n",
      "/* 443 */\n",
      "/* 444 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 445 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 446 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 447 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 448 */       if (shouldStop()) return;\n",
      "/* 449 */     }\n",
      "/* 450 */     hashAgg_mapIter_0.close();\n",
      "/* 451 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 452 */       hashAgg_hashMap_0.free();\n",
      "/* 453 */     }\n",
      "/* 454 */   }\n",
      "/* 455 */\n",
      "/* 456 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_7 = true;\n",
      "/* 151 */       long project_value_7 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_7 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[10] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_7 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_7 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_6 = project_isNull_7;\n",
      "/* 168 */       int project_value_6 = -1;\n",
      "/* 169 */       if (!project_isNull_7) {\n",
      "/* 170 */         project_value_6 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_7, ((java.time.ZoneId) references[12] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */       boolean project_isNull_5 = project_isNull_6;\n",
      "/* 174 */       long project_value_5 = -1L;\n",
      "/* 175 */       if (!project_isNull_5) {\n",
      "/* 176 */         project_value_5 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_6, ((java.time.ZoneId) references[9] /* zoneId */)) / 1;\n",
      "/* 177 */       }\n",
      "/* 178 */       boolean project_isNull_4 = project_isNull_5;\n",
      "/* 179 */       int project_value_4 = -1;\n",
      "/* 180 */       if (!project_isNull_5) {\n",
      "/* 181 */         project_value_4 =\n",
      "/* 182 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_5, ((java.time.ZoneId) references[13] /* zoneId */));\n",
      "/* 183 */       }\n",
      "/* 184 */       boolean project_isNull_3 = project_isNull_4;\n",
      "/* 185 */       long project_value_3 = -1L;\n",
      "/* 186 */       if (!project_isNull_3) {\n",
      "/* 187 */         project_value_3 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_4, ((java.time.ZoneId) references[8] /* zoneId */)) / 1;\n",
      "/* 188 */       }\n",
      "/* 189 */       boolean project_isNull_2 = project_isNull_3;\n",
      "/* 190 */       int project_value_2 = -1;\n",
      "/* 191 */       if (!project_isNull_3) {\n",
      "/* 192 */         project_value_2 =\n",
      "/* 193 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_3, ((java.time.ZoneId) references[14] /* zoneId */));\n",
      "/* 194 */       }\n",
      "/* 195 */       boolean project_isNull_1 = project_isNull_2;\n",
      "/* 196 */       long project_value_1 = -1L;\n",
      "/* 197 */       if (!project_isNull_1) {\n",
      "/* 198 */         project_value_1 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_2, ((java.time.ZoneId) references[7] /* zoneId */)) / 1;\n",
      "/* 199 */       }\n",
      "/* 200 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 201 */       int project_value_0 = -1;\n",
      "/* 202 */       if (!project_isNull_1) {\n",
      "/* 203 */         project_value_0 =\n",
      "/* 204 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[15] /* zoneId */));\n",
      "/* 205 */       }\n",
      "/* 206 */\n",
      "/* 207 */       // common sub-expressions\n",
      "/* 208 */\n",
      "/* 209 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 210 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 211 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 212 */       boolean project_isNull_14 = project_isNull_0;\n",
      "/* 213 */       int project_value_14 = -1;\n",
      "/* 214 */\n",
      "/* 215 */       if (!project_isNull_0) {\n",
      "/* 216 */         project_value_14 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 217 */       }\n",
      "/* 218 */       boolean project_isNull_16 = project_isNull_0;\n",
      "/* 219 */       int project_value_16 = -1;\n",
      "/* 220 */\n",
      "/* 221 */       if (!project_isNull_0) {\n",
      "/* 222 */         project_value_16 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 223 */       }\n",
      "/* 224 */       boolean project_isNull_18 = project_isNull_0;\n",
      "/* 225 */       int project_value_18 = -1;\n",
      "/* 226 */\n",
      "/* 227 */       if (!project_isNull_0) {\n",
      "/* 228 */         project_value_18 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 229 */       }\n",
      "/* 230 */\n",
      "/* 231 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_14, project_isNull_14, project_value_16, project_isNull_16, project_value_18, project_isNull_18);\n",
      "/* 232 */       // shouldStop check is eliminated\n",
      "/* 233 */     }\n",
      "/* 234 */\n",
      "/* 235 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 236 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 237 */\n",
      "/* 238 */   }\n",
      "/* 239 */\n",
      "/* 240 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 241 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 242 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 243 */\n",
      "/* 244 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 245 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 246 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 247 */     }\n",
      "/* 248 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 249 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 250 */       // generate grouping key\n",
      "/* 251 */       project_mutableStateArray_0[3].reset();\n",
      "/* 252 */\n",
      "/* 253 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 254 */\n",
      "/* 255 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 256 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 257 */       } else {\n",
      "/* 258 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 259 */       }\n",
      "/* 260 */\n",
      "/* 261 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 262 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 263 */       } else {\n",
      "/* 264 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 265 */       }\n",
      "/* 266 */\n",
      "/* 267 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 268 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 269 */       } else {\n",
      "/* 270 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 271 */       }\n",
      "/* 272 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 273 */       if (true) {\n",
      "/* 274 */         // try to get the buffer from hash map\n",
      "/* 275 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 276 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 277 */       }\n",
      "/* 278 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 279 */       // aggregation after processing all input rows.\n",
      "/* 280 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 281 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 282 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 283 */         } else {\n",
      "/* 284 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 285 */         }\n",
      "/* 286 */\n",
      "/* 287 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 288 */         // try to allocate buffer again.\n",
      "/* 289 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 290 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 291 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 292 */           // failed to allocate the first page\n",
      "/* 293 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 294 */         }\n",
      "/* 295 */       }\n",
      "/* 296 */\n",
      "/* 297 */     }\n",
      "/* 298 */\n",
      "/* 299 */     // Updates the proper row buffer\n",
      "/* 300 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 301 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 302 */     }\n",
      "/* 303 */\n",
      "/* 304 */     // common sub-expressions\n",
      "/* 305 */\n",
      "/* 306 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 307 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 308 */\n",
      "/* 309 */   }\n",
      "/* 310 */\n",
      "/* 311 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 312 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 313 */     double hashAgg_value_12 = -1.0;\n",
      "/* 314 */     do {\n",
      "/* 315 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 316 */       double hashAgg_value_13 = -1.0;\n",
      "/* 317 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 318 */       double hashAgg_value_14 = -1.0;\n",
      "/* 319 */       do {\n",
      "/* 320 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 321 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 322 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 323 */         if (!hashAgg_isNull_14) {\n",
      "/* 324 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 325 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 326 */           continue;\n",
      "/* 327 */         }\n",
      "/* 328 */\n",
      "/* 329 */         if (!false) {\n",
      "/* 330 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 331 */           hashAgg_value_14 = 0.0D;\n",
      "/* 332 */           continue;\n",
      "/* 333 */         }\n",
      "/* 334 */\n",
      "/* 335 */       } while (false);\n",
      "/* 336 */\n",
      "/* 337 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 338 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 339 */\n",
      "/* 340 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 341 */\n",
      "/* 342 */       }\n",
      "/* 343 */       if (!hashAgg_isNull_12) {\n",
      "/* 344 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 345 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 346 */         continue;\n",
      "/* 347 */       }\n",
      "/* 348 */\n",
      "/* 349 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 350 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 351 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 352 */       if (!hashAgg_isNull_17) {\n",
      "/* 353 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 354 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 355 */         continue;\n",
      "/* 356 */       }\n",
      "/* 357 */\n",
      "/* 358 */     } while (false);\n",
      "/* 359 */\n",
      "/* 360 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 361 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 362 */     } else {\n",
      "/* 363 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 368 */   throws java.io.IOException {\n",
      "/* 369 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[16] /* numOutputRows */).add(1);\n",
      "/* 370 */\n",
      "/* 371 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 372 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 373 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 374 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 375 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 376 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 377 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 378 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 379 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 380 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 381 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 382 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 383 */\n",
      "/* 384 */     project_mutableStateArray_0[4].reset();\n",
      "/* 385 */\n",
      "/* 386 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 387 */\n",
      "/* 388 */     if (hashAgg_isNull_18) {\n",
      "/* 389 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 390 */     } else {\n",
      "/* 391 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 392 */     }\n",
      "/* 393 */\n",
      "/* 394 */     if (hashAgg_isNull_19) {\n",
      "/* 395 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 396 */     } else {\n",
      "/* 397 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 398 */     }\n",
      "/* 399 */\n",
      "/* 400 */     if (hashAgg_isNull_20) {\n",
      "/* 401 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 402 */     } else {\n",
      "/* 403 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 404 */     }\n",
      "/* 405 */\n",
      "/* 406 */     if (hashAgg_isNull_21) {\n",
      "/* 407 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 408 */     } else {\n",
      "/* 409 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 410 */     }\n",
      "/* 411 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 412 */\n",
      "/* 413 */   }\n",
      "/* 414 */\n",
      "/* 415 */   protected void processNext() throws java.io.IOException {\n",
      "/* 416 */     if (!hashAgg_initAgg_0) {\n",
      "/* 417 */       hashAgg_initAgg_0 = true;\n",
      "/* 418 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 419 */\n",
      "/* 420 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 421 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 422 */           @Override\n",
      "/* 423 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 424 */             hashAgg_fastHashMap_0.close();\n",
      "/* 425 */           }\n",
      "/* 426 */         });\n",
      "/* 427 */\n",
      "/* 428 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 429 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 430 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 431 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[17] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 432 */     }\n",
      "/* 433 */     // output the result\n",
      "/* 434 */\n",
      "/* 435 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 436 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 437 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 438 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 439 */\n",
      "/* 440 */       if (shouldStop()) return;\n",
      "/* 441 */     }\n",
      "/* 442 */     hashAgg_fastHashMap_0.close();\n",
      "/* 443 */\n",
      "/* 444 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 445 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 446 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 447 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 448 */       if (shouldStop()) return;\n",
      "/* 449 */     }\n",
      "/* 450 */     hashAgg_mapIter_0.close();\n",
      "/* 451 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 452 */       hashAgg_hashMap_0.free();\n",
      "/* 453 */     }\n",
      "/* 454 */   }\n",
      "/* 455 */\n",
      "/* 456 */ }\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------------+\n",
      "|billtype|storeid|   sum(saleamount)|\n",
      "+--------+-------+------------------+\n",
      "| Invoice|    406|            391.82|\n",
      "| Invoice|    402| 4940.400000000001|\n",
      "| Invoice|    404|             431.0|\n",
      "|  Credit|    406|3375.6800000000003|\n",
      "|  Credit|    401|            7889.6|\n",
      "|  Credit|    402|          23673.66|\n",
      "| Invoice|    403|            593.98|\n",
      "|  Credit|    405|1367.6399999999999|\n",
      "|  Credit|    404|          12474.66|\n",
      "| Invoice|    405|           6640.54|\n",
      "|    Paid|    403|           2846.54|\n",
      "|    Paid|    402|1642.9199999999998|\n",
      "|  Credit|    403|            793.12|\n",
      "|    Paid|    404|453.32000000000005|\n",
      "| Invoice|    401|16515.920000000002|\n",
      "|    Paid|    406|           7948.32|\n",
      "|    Paid|    401|           8097.94|\n",
      "|    Paid|    405| 8479.279999999999|\n",
      "+--------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 27.920926 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 351.8 KiB, free 363.6 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_60 locally took 1 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_60 without replication took 1 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 363.6 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_60_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_60_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_60_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_60_piece0 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_60_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO SparkContext: Created broadcast 60 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:43:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:43:45 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 146 took 0.000033 seconds\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Registering RDD 146 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Got map stage job 35 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitStage(ShuffleMapStage 41 (name=showString at NativeMethodAccessorImpl.java:0;jobs=35))\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[146] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:43:45 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 41)\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 50.8 KiB, free 363.5 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_61 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_61 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 363.5 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_61_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on macbookpro.lan:57375 (size: 22.2 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_61_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_61_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Put block broadcast_61_piece0 locally took 0 ms\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Putting block broadcast_61_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:45 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[146] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:43:45 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Epoch for TaskSet 41.0: 4\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: Valid locality levels for TaskSet 41.0: NO_PREF, ANY\n",
      "25/04/11 09:43:45 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_41.0, runningTasks: 0\n",
      "25/04/11 09:43:45 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 35) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:43:45 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:43:45 INFO Executor: Running task 0.0 in stage 41.0 (TID 35)\n",
      "25/04/11 09:43:45 DEBUG ExecutorMetricsPoller: stageTCMP: (41, 0) -> 1\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Getting local block broadcast_61\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Level for block broadcast_61 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_7 = true;\n",
      "/* 151 */       long project_value_7 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_7 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[10] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_7 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_7 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_6 = project_isNull_7;\n",
      "/* 168 */       int project_value_6 = -1;\n",
      "/* 169 */       if (!project_isNull_7) {\n",
      "/* 170 */         project_value_6 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_7, ((java.time.ZoneId) references[12] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */       boolean project_isNull_5 = project_isNull_6;\n",
      "/* 174 */       long project_value_5 = -1L;\n",
      "/* 175 */       if (!project_isNull_5) {\n",
      "/* 176 */         project_value_5 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_6, ((java.time.ZoneId) references[9] /* zoneId */)) / 1;\n",
      "/* 177 */       }\n",
      "/* 178 */       boolean project_isNull_4 = project_isNull_5;\n",
      "/* 179 */       int project_value_4 = -1;\n",
      "/* 180 */       if (!project_isNull_5) {\n",
      "/* 181 */         project_value_4 =\n",
      "/* 182 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_5, ((java.time.ZoneId) references[13] /* zoneId */));\n",
      "/* 183 */       }\n",
      "/* 184 */       boolean project_isNull_3 = project_isNull_4;\n",
      "/* 185 */       long project_value_3 = -1L;\n",
      "/* 186 */       if (!project_isNull_3) {\n",
      "/* 187 */         project_value_3 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_4, ((java.time.ZoneId) references[8] /* zoneId */)) / 1;\n",
      "/* 188 */       }\n",
      "/* 189 */       boolean project_isNull_2 = project_isNull_3;\n",
      "/* 190 */       int project_value_2 = -1;\n",
      "/* 191 */       if (!project_isNull_3) {\n",
      "/* 192 */         project_value_2 =\n",
      "/* 193 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_3, ((java.time.ZoneId) references[14] /* zoneId */));\n",
      "/* 194 */       }\n",
      "/* 195 */       boolean project_isNull_1 = project_isNull_2;\n",
      "/* 196 */       long project_value_1 = -1L;\n",
      "/* 197 */       if (!project_isNull_1) {\n",
      "/* 198 */         project_value_1 = org.apache.spark.sql.catalyst.util.DateTimeUtils.daysToMicros(project_value_2, ((java.time.ZoneId) references[7] /* zoneId */)) / 1;\n",
      "/* 199 */       }\n",
      "/* 200 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 201 */       int project_value_0 = -1;\n",
      "/* 202 */       if (!project_isNull_1) {\n",
      "/* 203 */         project_value_0 =\n",
      "/* 204 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[15] /* zoneId */));\n",
      "/* 205 */       }\n",
      "/* 206 */\n",
      "/* 207 */       // common sub-expressions\n",
      "/* 208 */\n",
      "/* 209 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 210 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 211 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 212 */       boolean project_isNull_14 = project_isNull_0;\n",
      "/* 213 */       int project_value_14 = -1;\n",
      "/* 214 */\n",
      "/* 215 */       if (!project_isNull_0) {\n",
      "/* 216 */         project_value_14 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 217 */       }\n",
      "/* 218 */       boolean project_isNull_16 = project_isNull_0;\n",
      "/* 219 */       int project_value_16 = -1;\n",
      "/* 220 */\n",
      "/* 221 */       if (!project_isNull_0) {\n",
      "/* 222 */         project_value_16 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 223 */       }\n",
      "/* 224 */       boolean project_isNull_18 = project_isNull_0;\n",
      "/* 225 */       int project_value_18 = -1;\n",
      "/* 226 */\n",
      "/* 227 */       if (!project_isNull_0) {\n",
      "/* 228 */         project_value_18 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 229 */       }\n",
      "/* 230 */\n",
      "/* 231 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_14, project_isNull_14, project_value_16, project_isNull_16, project_value_18, project_isNull_18);\n",
      "/* 232 */       // shouldStop check is eliminated\n",
      "/* 233 */     }\n",
      "/* 234 */\n",
      "/* 235 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 236 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 237 */\n",
      "/* 238 */   }\n",
      "/* 239 */\n",
      "/* 240 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 241 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 242 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 243 */\n",
      "/* 244 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 245 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 246 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 247 */     }\n",
      "/* 248 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 249 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 250 */       // generate grouping key\n",
      "/* 251 */       project_mutableStateArray_0[3].reset();\n",
      "/* 252 */\n",
      "/* 253 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 254 */\n",
      "/* 255 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 256 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 257 */       } else {\n",
      "/* 258 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 259 */       }\n",
      "/* 260 */\n",
      "/* 261 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 262 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 263 */       } else {\n",
      "/* 264 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 265 */       }\n",
      "/* 266 */\n",
      "/* 267 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 268 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 269 */       } else {\n",
      "/* 270 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 271 */       }\n",
      "/* 272 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 273 */       if (true) {\n",
      "/* 274 */         // try to get the buffer from hash map\n",
      "/* 275 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 276 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 277 */       }\n",
      "/* 278 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 279 */       // aggregation after processing all input rows.\n",
      "/* 280 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 281 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 282 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 283 */         } else {\n",
      "/* 284 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 285 */         }\n",
      "/* 286 */\n",
      "/* 287 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 288 */         // try to allocate buffer again.\n",
      "/* 289 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 290 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 291 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 292 */           // failed to allocate the first page\n",
      "/* 293 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 294 */         }\n",
      "/* 295 */       }\n",
      "/* 296 */\n",
      "/* 297 */     }\n",
      "/* 298 */\n",
      "/* 299 */     // Updates the proper row buffer\n",
      "/* 300 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 301 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 302 */     }\n",
      "/* 303 */\n",
      "/* 304 */     // common sub-expressions\n",
      "/* 305 */\n",
      "/* 306 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 307 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 308 */\n",
      "/* 309 */   }\n",
      "/* 310 */\n",
      "/* 311 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 312 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 313 */     double hashAgg_value_12 = -1.0;\n",
      "/* 314 */     do {\n",
      "/* 315 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 316 */       double hashAgg_value_13 = -1.0;\n",
      "/* 317 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 318 */       double hashAgg_value_14 = -1.0;\n",
      "/* 319 */       do {\n",
      "/* 320 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 321 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 322 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 323 */         if (!hashAgg_isNull_14) {\n",
      "/* 324 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 325 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 326 */           continue;\n",
      "/* 327 */         }\n",
      "/* 328 */\n",
      "/* 329 */         if (!false) {\n",
      "/* 330 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 331 */           hashAgg_value_14 = 0.0D;\n",
      "/* 332 */           continue;\n",
      "/* 333 */         }\n",
      "/* 334 */\n",
      "/* 335 */       } while (false);\n",
      "/* 336 */\n",
      "/* 337 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 338 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 339 */\n",
      "/* 340 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 341 */\n",
      "/* 342 */       }\n",
      "/* 343 */       if (!hashAgg_isNull_12) {\n",
      "/* 344 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 345 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 346 */         continue;\n",
      "/* 347 */       }\n",
      "/* 348 */\n",
      "/* 349 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 350 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 351 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 352 */       if (!hashAgg_isNull_17) {\n",
      "/* 353 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 354 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 355 */         continue;\n",
      "/* 356 */       }\n",
      "/* 357 */\n",
      "/* 358 */     } while (false);\n",
      "/* 359 */\n",
      "/* 360 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 361 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 362 */     } else {\n",
      "/* 363 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 368 */   throws java.io.IOException {\n",
      "/* 369 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[16] /* numOutputRows */).add(1);\n",
      "/* 370 */\n",
      "/* 371 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 372 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 373 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 374 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 375 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 376 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 377 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 378 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 379 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 380 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 381 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 382 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 383 */\n",
      "/* 384 */     project_mutableStateArray_0[4].reset();\n",
      "/* 385 */\n",
      "/* 386 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 387 */\n",
      "/* 388 */     if (hashAgg_isNull_18) {\n",
      "/* 389 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 390 */     } else {\n",
      "/* 391 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 392 */     }\n",
      "/* 393 */\n",
      "/* 394 */     if (hashAgg_isNull_19) {\n",
      "/* 395 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 396 */     } else {\n",
      "/* 397 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 398 */     }\n",
      "/* 399 */\n",
      "/* 400 */     if (hashAgg_isNull_20) {\n",
      "/* 401 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 402 */     } else {\n",
      "/* 403 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 404 */     }\n",
      "/* 405 */\n",
      "/* 406 */     if (hashAgg_isNull_21) {\n",
      "/* 407 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 408 */     } else {\n",
      "/* 409 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 410 */     }\n",
      "/* 411 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 412 */\n",
      "/* 413 */   }\n",
      "/* 414 */\n",
      "/* 415 */   protected void processNext() throws java.io.IOException {\n",
      "/* 416 */     if (!hashAgg_initAgg_0) {\n",
      "/* 417 */       hashAgg_initAgg_0 = true;\n",
      "/* 418 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 419 */\n",
      "/* 420 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 421 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 422 */           @Override\n",
      "/* 423 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 424 */             hashAgg_fastHashMap_0.close();\n",
      "/* 425 */           }\n",
      "/* 426 */         });\n",
      "/* 427 */\n",
      "/* 428 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 429 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 430 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 431 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[17] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 432 */     }\n",
      "/* 433 */     // output the result\n",
      "/* 434 */\n",
      "/* 435 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 436 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 437 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 438 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 439 */\n",
      "/* 440 */       if (shouldStop()) return;\n",
      "/* 441 */     }\n",
      "/* 442 */     hashAgg_fastHashMap_0.close();\n",
      "/* 443 */\n",
      "/* 444 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 445 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 446 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 447 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 448 */       if (shouldStop()) return;\n",
      "/* 449 */     }\n",
      "/* 450 */     hashAgg_mapIter_0.close();\n",
      "/* 451 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 452 */       hashAgg_hashMap_0.free();\n",
      "/* 453 */     }\n",
      "/* 454 */   }\n",
      "/* 455 */\n",
      "/* 456 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2246)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2246\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2246\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2262)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2262\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2262\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2170)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2170\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2170\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2116)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2116\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2116\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2184)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2184\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2184\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2150)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2150\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2150\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2256)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2256\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2256\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2199)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2199\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2199\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2123)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2123\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2123\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2100)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2100\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2100\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2263)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2263\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2263\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2275)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2275\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2275\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2186)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2186\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2186\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanShuffle(3)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning shuffle 3\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned shuffle 3\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2074)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2074\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2074\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2257)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2257\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2257\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2089)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2089\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2089\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2249)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2249\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2249\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2119)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2119\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2119\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2064)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: removing shuffle 3\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2064\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 39.51367 ms\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2064\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2268)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2268\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2268\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2083)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2083\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2083\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2068)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2068\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2068\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2183)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2183\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2183\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2124)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2124\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2124\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2227)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2227\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2227\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2221)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2221\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2221\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2060)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2060\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2060\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2220)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2220\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2220\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2078)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2078\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2078\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2185)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2185\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2185\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2207)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2207\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2207\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2127)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2127\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2127\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2192)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2192\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2192\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2223)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2223\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2223\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2051)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2051\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2051\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2167)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2167\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2167\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2112)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2112\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2112\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2142)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2142\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2142\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2158)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2158\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2158\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2165)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2165\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2165\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2242)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2242\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2242\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2117)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2117\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2117\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2053)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2053\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2053\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2255)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2255\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2255\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2248)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2248\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2248\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2125)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2125\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2125\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(59)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning broadcast 59\n",
      "25/04/11 09:43:45 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 59\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: removing broadcast 59\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing broadcast 59\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_59_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Done removing shuffle 3, response is true\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_59_piece0 of size 21582 dropped from memory (free 381202058)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_59_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Removed broadcast_59_piece0 on macbookpro.lan:57375 in memory (size: 21.1 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Sent response: true to macbookpro.lan:57372\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_59_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_59_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_59\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_59 of size 46528 dropped from memory (free 381248586)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 59, response is 0\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned broadcast 59\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2144)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2144\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2144\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2118)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2118\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2118\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2254)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2254\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2254\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2239)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2239\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2239\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2140)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2140\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2140\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2194)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2194\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2194\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2146)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2146\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2146\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2191)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2191\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2191\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2062)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2062\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2062\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2233)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2233\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2233\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2145)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2145\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2145\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2168)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2168\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2168\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2160)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2160\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2160\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2075)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2075\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2075\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2270)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2270\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2270\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2250)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2250\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2250\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2137)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2137\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2137\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(58)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning broadcast 58\n",
      "25/04/11 09:43:45 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 58\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: removing broadcast 58\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing broadcast 58\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_58_piece0\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_58_piece0 of size 20248 dropped from memory (free 381268834)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_58_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Removed broadcast_58_piece0 on macbookpro.lan:57375 in memory (size: 19.8 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_58_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_58_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_58\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int, true], input[1, int, true], input[2, int, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       int value_2 = isNull_2 ?\n",
      "/* 039 */       -1 : (i.getInt(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */       boolean isNull_4 = i.isNullAt(2);\n",
      "/* 050 */       int value_4 = isNull_4 ?\n",
      "/* 051 */       -1 : (i.getInt(2));\n",
      "/* 052 */       if (!isNull_4) {\n",
      "/* 053 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_4, value_1);\n",
      "/* 054 */       }\n",
      "/* 055 */\n",
      "/* 056 */       int remainder_0 = value_1 % 200;\n",
      "/* 057 */       if (remainder_0 < 0) {\n",
      "/* 058 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 059 */       } else {\n",
      "/* 060 */         value_0=remainder_0;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */     }\n",
      "/* 064 */     if (isNull_0) {\n",
      "/* 065 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 070 */   }\n",
      "/* 071 */\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_58 of size 43752 dropped from memory (free 381312586)\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       int value_2 = isNull_2 ?\n",
      "/* 039 */       -1 : (i.getInt(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */       boolean isNull_4 = i.isNullAt(2);\n",
      "/* 050 */       int value_4 = isNull_4 ?\n",
      "/* 051 */       -1 : (i.getInt(2));\n",
      "/* 052 */       if (!isNull_4) {\n",
      "/* 053 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_4, value_1);\n",
      "/* 054 */       }\n",
      "/* 055 */\n",
      "/* 056 */       int remainder_0 = value_1 % 200;\n",
      "/* 057 */       if (remainder_0 < 0) {\n",
      "/* 058 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 059 */       } else {\n",
      "/* 060 */         value_0=remainder_0;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */     }\n",
      "/* 064 */     if (isNull_0) {\n",
      "/* 065 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 070 */   }\n",
      "/* 071 */\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 58, response is 0\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned broadcast 58\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2155)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2155\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2155\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2201)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2201\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2201\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2090)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2090\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2090\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2148)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2148\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2148\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2151)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2151\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2151\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2130)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2130\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2130\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2237)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2237\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2237\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2099)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2099\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2099\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2070)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2070\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2070\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2077)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2077\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2077\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2229)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2229\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2229\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2073)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2073\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2073\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2059)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2059\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2059\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2061)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2061\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2061\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2103)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2103\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2103\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2109)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2109\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2109\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2240)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2240\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2240\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2104)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2104\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2104\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2274)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2274\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2274\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2217)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2217\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2217\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2266)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2266\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2266\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2106)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2106\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2106\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2136)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2136\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2136\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2131)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2131\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2131\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2216)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2216\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2216\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2222)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2222\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2222\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2259)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2259\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2259\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2219)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2219\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2219\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2147)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2147\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2147\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2243)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2243\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2243\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(57)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning broadcast 57\n",
      "25/04/11 09:43:45 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 57\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: removing broadcast 57\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing broadcast 57\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_57_piece0\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_57_piece0 of size 35539 dropped from memory (free 381348125)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_57_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Removed broadcast_57_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_57_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_57_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_57\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_57 of size 360200 dropped from memory (free 381708325)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 57, response is 0\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned broadcast 57\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2162)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2162\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2162\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2087)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2087\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2087\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2197)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2197\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2197\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2200)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2200\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2200\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2205)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2205\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2205\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2108)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2108\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2108\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2159)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2159\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2159\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2195)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2195\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2195\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2149)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2149\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2149\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2204)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2204\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2204\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2180)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2180\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2180\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2236)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2236\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2236\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2267)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2267\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2267\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2058)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2058\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2058\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2098)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2098\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2098\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2235)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2235\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2235\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2234)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2234\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2234\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2067)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2067\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2067\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2193)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2193\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2193\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2164)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2164\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2164\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2228)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2228\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2228\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2111)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2111\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2111\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2133)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2133\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2133\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2126)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2126\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2126\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2203)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2203\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2203\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2252)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2252\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2252\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2251)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2251\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2251\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2096)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2096\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2096\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2169)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2169\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2169\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2172)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2172\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2172\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2247)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2247\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2247\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2272)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2272\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2272\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2277)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2277\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2277\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2209)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2209\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2209\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2211)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2211\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2211\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2264)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2264\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2264\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2065)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2065\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2065\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2101)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2101\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2101\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2198)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2198\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2198\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2072)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2072\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2072\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2174)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2174\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2174\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2086)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2086\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2086\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2176)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2176\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2176\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2189)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2189\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2189\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2156)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2156\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2156\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2094)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2094\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2094\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2173)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2173\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2173\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2095)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2095\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2095\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2231)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2231\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2231\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2141)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2141\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2141\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2206)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2206\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2206\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2245)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2245\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2245\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2253)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2253\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2253\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(56)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning broadcast 56\n",
      "25/04/11 09:43:45 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 56\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: removing broadcast 56\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing broadcast 56\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_56_piece0\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_56_piece0 of size 23740 dropped from memory (free 381732065)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_56_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:45 INFO BlockManagerInfo: Removed broadcast_56_piece0 on macbookpro.lan:57375 in memory (size: 23.2 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerMaster: Updated info of block broadcast_56_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Told master about block broadcast_56_piece0\n",
      "25/04/11 09:43:45 DEBUG BlockManager: Removing block broadcast_56\n",
      "25/04/11 09:43:45 DEBUG MemoryStore: Block broadcast_56 of size 50256 dropped from memory (free 381782321)\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 56, response is 0\n",
      "25/04/11 09:43:45 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned broadcast 56\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2054)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2054\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2054\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2132)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2132\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2132\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2079)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2079\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2079\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2202)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2202\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2202\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2232)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2232\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2232\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2190)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2190\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2190\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2056)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2056\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2056\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2154)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2154\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2154\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2069)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2069\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2069\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2107)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2107\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2107\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2088)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2088\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2088\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2218)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2218\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2218\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2241)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2241\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2241\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2081)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2081\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2081\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2226)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2226\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2226\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2050)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2050\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2050\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2122)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2122\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2122\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2135)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2135\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2135\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2224)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2224\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2224\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2273)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2273\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2273\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2187)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2187\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2187\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2260)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2260\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2260\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2093)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2093\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2093\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2175)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2175\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2175\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2134)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2134\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2134\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2161)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2161\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2161\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2269)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2269\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2269\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2084)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2084\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2084\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2102)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2102\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2102\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2052)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2052\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2052\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2182)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2182\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2182\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2113)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2113\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2113\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2179)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2179\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2179\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2166)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2166\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2166\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2171)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2171\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2171\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2210)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2210\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2210\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2196)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2196\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2196\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2157)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2157\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2157\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2276)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2276\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2276\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2178)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2178\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2178\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2138)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2138\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2138\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2213)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2213\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2213\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2214)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2214\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2214\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2143)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2143\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2143\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2152)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2152\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2152\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2085)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2085\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2085\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2082)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2082\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2082\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2225)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2225\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2225\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2230)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2230\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2230\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2120)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2120\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2120\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2238)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2238\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2238\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2066)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2066\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2066\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2177)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2177\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2177\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2265)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2265\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2265\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2076)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2076\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2076\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2080)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2080\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2080\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2139)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2139\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2139\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2121)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2121\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2121\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2215)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2215\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2215\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2271)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2271\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2271\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2212)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2212\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2212\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2244)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2244\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2244\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2114)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2114\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2114\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2188)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2188\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2188\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2057)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2057\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2057\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2163)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2163\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2163\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2258)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2258\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2258\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2055)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2055\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2055\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2181)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2181\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2181\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2115)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2115\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2115\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2063)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2063\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2063\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2071)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2071\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2071\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2261)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2261\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2261\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2092)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2092\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2092\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2208)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2208\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2208\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2110)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2110\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2110\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2153)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2153\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2153\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2105)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2105\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2105\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2128)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2128\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2128\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2097)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2097\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2097\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2091)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2091\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2091\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Got cleaning task CleanAccum(2129)\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaning accumulator 2129\n",
      "25/04/11 09:43:45 DEBUG ContextCleaner: Cleaned accumulator 2129\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 8.894292 ms\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 35 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@6a578599\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, int, true],input[2, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     int value_2 = isNull_2 ?\n",
      "/* 051 */     -1 : (i.getInt(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     int value_2 = isNull_2 ?\n",
      "/* 051 */     -1 : (i.getInt(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO CodeGenerator: Code generated in 4.324449 ms\n",
      "25/04/11 09:43:45 DEBUG TaskMemoryManager: Task 35 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@612552e7\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:45 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:43:45 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:43:45 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:43:46 INFO CodeGenerator: Code generated in 6.445701 ms\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Getting local block broadcast_60\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Level for block broadcast_60 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 35 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@612552e7\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 35 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@612552e7\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 35 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@6a578599\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 35 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@612552e7\n",
      "25/04/11 09:43:46 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 35 with length 200\n",
      "25/04/11 09:43:46 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 35: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:43:46 INFO Executor: Finished task 0.0 in stage 41.0 (TID 35). 2767 bytes result sent to driver\n",
      "25/04/11 09:43:46 DEBUG ExecutorMetricsPoller: stageTCMP: (41, 0) -> 0\n",
      "25/04/11 09:43:46 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 35) in 110 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:43:46 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:43:46 INFO DAGScheduler: ShuffleMapStage 41 (showString at NativeMethodAccessorImpl.java:0) finished in 0.114 s\n",
      "25/04/11 09:43:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:43:46 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:43:46 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:43:46 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:43:46 DEBUG MapOutputTrackerMaster: Increasing epoch to 5\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: After removal of stage 41, remaining stages = 0\n",
      "25/04/11 09:43:46 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=40.0 B, rowCount=1) for plan: HashAggregate(keys=[year#943, quarter#954, month#966], functions=[sum(saleamount#682)], output=[toprettystring(year)#1001, toprettystring(quarter)#1002, toprettystring(month)#1003, toprettystring(sum(saleamount))#1004])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange hashpartitioning(year#943, quarter#954, month#966, 200), ENSURE_REQUIREMENTS, [plan_id=755]\n",
      "      +- *(1) HashAggregate(keys=[year#943, quarter#954, month#966], functions=[partial_sum(saleamount#682)], output=[year#943, quarter#954, month#966, sum#1010])\n",
      "         +- *(1) Project [saleamount#682, year(saledate#899) AS year#943, quarter(saledate#899) AS quarter#954, month(saledate#899) AS month#966]\n",
      "            +- *(1) Project [cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(saledate#677, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#899, saleamount#682]\n",
      "               +- FileScan csv [saledate#677,saleamount#682] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:43:46 DEBUG AdaptiveSparkPlanExec: Plan changed:\n",
      "!CollectLimit 21                                                                                                                                                                                                                                                                                                                                                                                                                     HashAggregate(keys=[year#943, quarter#954, month#966], functions=[sum(saleamount#682)], output=[toprettystring(year)#1001, toprettystring(quarter)#1002, toprettystring(month)#1003, toprettystring(sum(saleamount))#1004])\n",
      "!+- HashAggregate(keys=[year#943, quarter#954, month#966], functions=[sum(saleamount#682)], output=[toprettystring(year)#1001, toprettystring(quarter)#1002, toprettystring(month)#1003, toprettystring(sum(saleamount))#1004])                                                                                                                                                                                                      +- ShuffleQueryStage 0\n",
      "!   +- ShuffleQueryStage 0                                                                                                                                                                                                                                                                                                                                                                                                              +- Exchange hashpartitioning(year#943, quarter#954, month#966, 200), ENSURE_REQUIREMENTS, [plan_id=755]\n",
      "!      +- Exchange hashpartitioning(year#943, quarter#954, month#966, 200), ENSURE_REQUIREMENTS, [plan_id=755]                                                                                                                                                                                                                                                                                                                             +- *(1) HashAggregate(keys=[year#943, quarter#954, month#966], functions=[partial_sum(saleamount#682)], output=[year#943, quarter#954, month#966, sum#1010])\n",
      "!         +- *(1) HashAggregate(keys=[year#943, quarter#954, month#966], functions=[partial_sum(saleamount#682)], output=[year#943, quarter#954, month#966, sum#1010])                                                                                                                                                                                                                                                                        +- *(1) Project [saleamount#682, year(saledate#899) AS year#943, quarter(saledate#899) AS quarter#954, month(saledate#899) AS month#966]\n",
      "!            +- *(1) Project [saleamount#682, year(saledate#899) AS year#943, quarter(saledate#899) AS quarter#954, month(saledate#899) AS month#966]                                                                                                                                                                                                                                                                                            +- *(1) Project [cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(saledate#677, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#899, saleamount#682]\n",
      "!               +- *(1) Project [cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(saledate#677, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#899, saleamount#682]                  +- FileScan csv [saledate#677,saleamount#682] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "!                  +- FileScan csv [saledate#677,saleamount#682] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>                                                                                                        \n",
      "25/04/11 09:43:46 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:43:46 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:43:46 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_8_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 045 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 046 */       double inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 047 */       -1.0 : (inputadapter_row_0.getDouble(3));\n",
      "/* 048 */\n",
      "/* 049 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2, inputadapter_value_3, inputadapter_isNull_3);\n",
      "/* 050 */       // shouldStop check is eliminated\n",
      "/* 051 */     }\n",
      "/* 052 */\n",
      "/* 053 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 054 */   }\n",
      "/* 055 */\n",
      "/* 056 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 057 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 058 */\n",
      "/* 059 */     // generate grouping key\n",
      "/* 060 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 061 */\n",
      "/* 062 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 063 */\n",
      "/* 064 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 065 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 071 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 072 */     } else {\n",
      "/* 073 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 074 */     }\n",
      "/* 075 */\n",
      "/* 076 */     if (hashAgg_exprIsNull_2_0) {\n",
      "/* 077 */       hashAgg_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 078 */     } else {\n",
      "/* 079 */       hashAgg_mutableStateArray_0[0].write(2, hashAgg_expr_2_0);\n",
      "/* 080 */     }\n",
      "/* 081 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 082 */     if (true) {\n",
      "/* 083 */       // try to get the buffer from hash map\n",
      "/* 084 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 085 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 086 */     }\n",
      "/* 087 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 088 */     // aggregation after processing all input rows.\n",
      "/* 089 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 090 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 091 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 092 */       } else {\n",
      "/* 093 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 097 */       // try to allocate buffer again.\n",
      "/* 098 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 099 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 100 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 101 */         // failed to allocate the first page\n",
      "/* 102 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 103 */       }\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     // common sub-expressions\n",
      "/* 107 */\n",
      "/* 108 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 109 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_3_0, hashAgg_exprIsNull_3_0);\n",
      "/* 110 */\n",
      "/* 111 */   }\n",
      "/* 112 */\n",
      "/* 113 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 114 */     hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 115 */     double hashAgg_value_6 = -1.0;\n",
      "/* 116 */     do {\n",
      "/* 117 */       boolean hashAgg_isNull_7 = true;\n",
      "/* 118 */       double hashAgg_value_7 = -1.0;\n",
      "/* 119 */       hashAgg_hashAgg_isNull_8_0 = true;\n",
      "/* 120 */       double hashAgg_value_8 = -1.0;\n",
      "/* 121 */       do {\n",
      "/* 122 */         boolean hashAgg_isNull_9 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 123 */         double hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 124 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 125 */         if (!hashAgg_isNull_9) {\n",
      "/* 126 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 127 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 128 */           continue;\n",
      "/* 129 */         }\n",
      "/* 130 */\n",
      "/* 131 */         if (!false) {\n",
      "/* 132 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 133 */           hashAgg_value_8 = 0.0D;\n",
      "/* 134 */           continue;\n",
      "/* 135 */         }\n",
      "/* 136 */\n",
      "/* 137 */       } while (false);\n",
      "/* 138 */\n",
      "/* 139 */       if (!hashAgg_exprIsNull_3_0) {\n",
      "/* 140 */         hashAgg_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 141 */\n",
      "/* 142 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_3_0;\n",
      "/* 143 */\n",
      "/* 144 */       }\n",
      "/* 145 */       if (!hashAgg_isNull_7) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 147 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */       boolean hashAgg_isNull_12 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 152 */       double hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 153 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 154 */       if (!hashAgg_isNull_12) {\n",
      "/* 155 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 156 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 157 */         continue;\n",
      "/* 158 */       }\n",
      "/* 159 */\n",
      "/* 160 */     } while (false);\n",
      "/* 161 */\n",
      "/* 162 */     if (!hashAgg_hashAgg_isNull_6_0) {\n",
      "/* 163 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 164 */     } else {\n",
      "/* 165 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 166 */     }\n",
      "/* 167 */   }\n",
      "/* 168 */\n",
      "/* 169 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 170 */   throws java.io.IOException {\n",
      "/* 171 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 172 */\n",
      "/* 173 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 174 */     int hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 175 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 176 */     boolean hashAgg_isNull_14 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 177 */     int hashAgg_value_14 = hashAgg_isNull_14 ?\n",
      "/* 178 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 179 */     boolean hashAgg_isNull_15 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 180 */     int hashAgg_value_15 = hashAgg_isNull_15 ?\n",
      "/* 181 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 182 */     boolean hashAgg_isNull_16 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 183 */     double hashAgg_value_16 = hashAgg_isNull_16 ?\n",
      "/* 184 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 185 */\n",
      "/* 186 */     UTF8String hashAgg_value_18;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_18 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     UTF8String hashAgg_value_20;\n",
      "/* 193 */     if (hashAgg_isNull_14) {\n",
      "/* 194 */       hashAgg_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 195 */     } else {\n",
      "/* 196 */       hashAgg_value_20 = UTF8String.fromString(String.valueOf(hashAgg_value_14));\n",
      "/* 197 */     }\n",
      "/* 198 */     UTF8String hashAgg_value_22;\n",
      "/* 199 */     if (hashAgg_isNull_15) {\n",
      "/* 200 */       hashAgg_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 201 */     } else {\n",
      "/* 202 */       hashAgg_value_22 = UTF8String.fromString(String.valueOf(hashAgg_value_15));\n",
      "/* 203 */     }\n",
      "/* 204 */     UTF8String hashAgg_value_24;\n",
      "/* 205 */     if (hashAgg_isNull_16) {\n",
      "/* 206 */       hashAgg_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 207 */     } else {\n",
      "/* 208 */       hashAgg_value_24 = UTF8String.fromString(String.valueOf(hashAgg_value_16));\n",
      "/* 209 */     }\n",
      "/* 210 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 211 */\n",
      "/* 212 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_18);\n",
      "/* 213 */\n",
      "/* 214 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_20);\n",
      "/* 215 */\n",
      "/* 216 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_22);\n",
      "/* 217 */\n",
      "/* 218 */     hashAgg_mutableStateArray_0[1].write(3, hashAgg_value_24);\n",
      "/* 219 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 220 */\n",
      "/* 221 */   }\n",
      "/* 222 */\n",
      "/* 223 */   protected void processNext() throws java.io.IOException {\n",
      "/* 224 */     if (!hashAgg_initAgg_0) {\n",
      "/* 225 */       hashAgg_initAgg_0 = true;\n",
      "/* 226 */\n",
      "/* 227 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 228 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 229 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 230 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 231 */     }\n",
      "/* 232 */     // output the result\n",
      "/* 233 */\n",
      "/* 234 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 235 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 236 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 237 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 238 */       if (shouldStop()) return;\n",
      "/* 239 */     }\n",
      "/* 240 */     hashAgg_mapIter_0.close();\n",
      "/* 241 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 242 */       hashAgg_hashMap_0.free();\n",
      "/* 243 */     }\n",
      "/* 244 */   }\n",
      "/* 245 */\n",
      "/* 246 */ }\n",
      "\n",
      "25/04/11 09:43:46 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_8_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 045 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 046 */       double inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 047 */       -1.0 : (inputadapter_row_0.getDouble(3));\n",
      "/* 048 */\n",
      "/* 049 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2, inputadapter_value_3, inputadapter_isNull_3);\n",
      "/* 050 */       // shouldStop check is eliminated\n",
      "/* 051 */     }\n",
      "/* 052 */\n",
      "/* 053 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 054 */   }\n",
      "/* 055 */\n",
      "/* 056 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 057 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 058 */\n",
      "/* 059 */     // generate grouping key\n",
      "/* 060 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 061 */\n",
      "/* 062 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 063 */\n",
      "/* 064 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 065 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 071 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 072 */     } else {\n",
      "/* 073 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 074 */     }\n",
      "/* 075 */\n",
      "/* 076 */     if (hashAgg_exprIsNull_2_0) {\n",
      "/* 077 */       hashAgg_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 078 */     } else {\n",
      "/* 079 */       hashAgg_mutableStateArray_0[0].write(2, hashAgg_expr_2_0);\n",
      "/* 080 */     }\n",
      "/* 081 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 082 */     if (true) {\n",
      "/* 083 */       // try to get the buffer from hash map\n",
      "/* 084 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 085 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 086 */     }\n",
      "/* 087 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 088 */     // aggregation after processing all input rows.\n",
      "/* 089 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 090 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 091 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 092 */       } else {\n",
      "/* 093 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 097 */       // try to allocate buffer again.\n",
      "/* 098 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 099 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 100 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 101 */         // failed to allocate the first page\n",
      "/* 102 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 103 */       }\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     // common sub-expressions\n",
      "/* 107 */\n",
      "/* 108 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 109 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_3_0, hashAgg_exprIsNull_3_0);\n",
      "/* 110 */\n",
      "/* 111 */   }\n",
      "/* 112 */\n",
      "/* 113 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 114 */     hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 115 */     double hashAgg_value_6 = -1.0;\n",
      "/* 116 */     do {\n",
      "/* 117 */       boolean hashAgg_isNull_7 = true;\n",
      "/* 118 */       double hashAgg_value_7 = -1.0;\n",
      "/* 119 */       hashAgg_hashAgg_isNull_8_0 = true;\n",
      "/* 120 */       double hashAgg_value_8 = -1.0;\n",
      "/* 121 */       do {\n",
      "/* 122 */         boolean hashAgg_isNull_9 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 123 */         double hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 124 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 125 */         if (!hashAgg_isNull_9) {\n",
      "/* 126 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 127 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 128 */           continue;\n",
      "/* 129 */         }\n",
      "/* 130 */\n",
      "/* 131 */         if (!false) {\n",
      "/* 132 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 133 */           hashAgg_value_8 = 0.0D;\n",
      "/* 134 */           continue;\n",
      "/* 135 */         }\n",
      "/* 136 */\n",
      "/* 137 */       } while (false);\n",
      "/* 138 */\n",
      "/* 139 */       if (!hashAgg_exprIsNull_3_0) {\n",
      "/* 140 */         hashAgg_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 141 */\n",
      "/* 142 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_3_0;\n",
      "/* 143 */\n",
      "/* 144 */       }\n",
      "/* 145 */       if (!hashAgg_isNull_7) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 147 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */       boolean hashAgg_isNull_12 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 152 */       double hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 153 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 154 */       if (!hashAgg_isNull_12) {\n",
      "/* 155 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 156 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 157 */         continue;\n",
      "/* 158 */       }\n",
      "/* 159 */\n",
      "/* 160 */     } while (false);\n",
      "/* 161 */\n",
      "/* 162 */     if (!hashAgg_hashAgg_isNull_6_0) {\n",
      "/* 163 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 164 */     } else {\n",
      "/* 165 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 166 */     }\n",
      "/* 167 */   }\n",
      "/* 168 */\n",
      "/* 169 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 170 */   throws java.io.IOException {\n",
      "/* 171 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 172 */\n",
      "/* 173 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 174 */     int hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 175 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 176 */     boolean hashAgg_isNull_14 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 177 */     int hashAgg_value_14 = hashAgg_isNull_14 ?\n",
      "/* 178 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 179 */     boolean hashAgg_isNull_15 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 180 */     int hashAgg_value_15 = hashAgg_isNull_15 ?\n",
      "/* 181 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 182 */     boolean hashAgg_isNull_16 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 183 */     double hashAgg_value_16 = hashAgg_isNull_16 ?\n",
      "/* 184 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 185 */\n",
      "/* 186 */     UTF8String hashAgg_value_18;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_18 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     UTF8String hashAgg_value_20;\n",
      "/* 193 */     if (hashAgg_isNull_14) {\n",
      "/* 194 */       hashAgg_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 195 */     } else {\n",
      "/* 196 */       hashAgg_value_20 = UTF8String.fromString(String.valueOf(hashAgg_value_14));\n",
      "/* 197 */     }\n",
      "/* 198 */     UTF8String hashAgg_value_22;\n",
      "/* 199 */     if (hashAgg_isNull_15) {\n",
      "/* 200 */       hashAgg_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 201 */     } else {\n",
      "/* 202 */       hashAgg_value_22 = UTF8String.fromString(String.valueOf(hashAgg_value_15));\n",
      "/* 203 */     }\n",
      "/* 204 */     UTF8String hashAgg_value_24;\n",
      "/* 205 */     if (hashAgg_isNull_16) {\n",
      "/* 206 */       hashAgg_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 207 */     } else {\n",
      "/* 208 */       hashAgg_value_24 = UTF8String.fromString(String.valueOf(hashAgg_value_16));\n",
      "/* 209 */     }\n",
      "/* 210 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 211 */\n",
      "/* 212 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_18);\n",
      "/* 213 */\n",
      "/* 214 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_20);\n",
      "/* 215 */\n",
      "/* 216 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_22);\n",
      "/* 217 */\n",
      "/* 218 */     hashAgg_mutableStateArray_0[1].write(3, hashAgg_value_24);\n",
      "/* 219 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 220 */\n",
      "/* 221 */   }\n",
      "/* 222 */\n",
      "/* 223 */   protected void processNext() throws java.io.IOException {\n",
      "/* 224 */     if (!hashAgg_initAgg_0) {\n",
      "/* 225 */       hashAgg_initAgg_0 = true;\n",
      "/* 226 */\n",
      "/* 227 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 228 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 229 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 230 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 231 */     }\n",
      "/* 232 */     // output the result\n",
      "/* 233 */\n",
      "/* 234 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 235 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 236 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 237 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 238 */       if (shouldStop()) return;\n",
      "/* 239 */     }\n",
      "/* 240 */     hashAgg_mapIter_0.close();\n",
      "/* 241 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 242 */       hashAgg_hashMap_0.free();\n",
      "/* 243 */     }\n",
      "/* 244 */   }\n",
      "/* 245 */\n",
      "/* 246 */ }\n",
      "\n",
      "25/04/11 09:43:46 INFO CodeGenerator: Code generated in 14.209441 ms\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:43:46 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:43:46 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 149 took 0.000104 seconds\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Got job 36 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Final stage: ResultStage 43 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: submitStage(ResultStage 43 (name=showString at NativeMethodAccessorImpl.java:0;jobs=36))\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[149] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: submitMissingTasks(ResultStage 43)\n",
      "25/04/11 09:43:46 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 51.1 KiB, free 364.0 MiB)\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Put block broadcast_62 locally took 0 ms\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Putting block broadcast_62 without replication took 0 ms\n",
      "25/04/11 09:43:46 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 364.0 MiB)\n",
      "25/04/11 09:43:46 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_62_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:43:46 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on macbookpro.lan:57375 (size: 22.6 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:43:46 DEBUG BlockManagerMaster: Updated info of block broadcast_62_piece0\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Told master about block broadcast_62_piece0\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Put block broadcast_62_piece0 locally took 0 ms\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Putting block broadcast_62_piece0 without replication took 0 ms\n",
      "25/04/11 09:43:46 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[149] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:43:46 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:43:46 DEBUG TaskSetManager: Epoch for TaskSet 43.0: 5\n",
      "25/04/11 09:43:46 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:43:46 DEBUG TaskSetManager: Valid locality levels for TaskSet 43.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:43:46 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_43.0, runningTasks: 0\n",
      "25/04/11 09:43:46 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 36) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:43:46 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:43:46 INFO Executor: Running task 0.0 in stage 43.0 (TID 36)\n",
      "25/04/11 09:43:46 DEBUG ExecutorMetricsPoller: stageTCMP: (43, 0) -> 1\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Getting local block broadcast_62\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Level for block broadcast_62 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:43:46 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 4\n",
      "25/04/11 09:43:46 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 4, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:43:46 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:43:46 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:43:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:43:46 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_4_35_42_43,0)\n",
      "25/04/11 09:43:46 DEBUG BlockManager: Getting local shuffle block shuffle_4_35_42_43\n",
      "25/04/11 09:43:46 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:43:46 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_8_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 045 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 046 */       double inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 047 */       -1.0 : (inputadapter_row_0.getDouble(3));\n",
      "/* 048 */\n",
      "/* 049 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2, inputadapter_value_3, inputadapter_isNull_3);\n",
      "/* 050 */       // shouldStop check is eliminated\n",
      "/* 051 */     }\n",
      "/* 052 */\n",
      "/* 053 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 054 */   }\n",
      "/* 055 */\n",
      "/* 056 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 057 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 058 */\n",
      "/* 059 */     // generate grouping key\n",
      "/* 060 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 061 */\n",
      "/* 062 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 063 */\n",
      "/* 064 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 065 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 071 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 072 */     } else {\n",
      "/* 073 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 074 */     }\n",
      "/* 075 */\n",
      "/* 076 */     if (hashAgg_exprIsNull_2_0) {\n",
      "/* 077 */       hashAgg_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 078 */     } else {\n",
      "/* 079 */       hashAgg_mutableStateArray_0[0].write(2, hashAgg_expr_2_0);\n",
      "/* 080 */     }\n",
      "/* 081 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 082 */     if (true) {\n",
      "/* 083 */       // try to get the buffer from hash map\n",
      "/* 084 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 085 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 086 */     }\n",
      "/* 087 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 088 */     // aggregation after processing all input rows.\n",
      "/* 089 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 090 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 091 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 092 */       } else {\n",
      "/* 093 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 097 */       // try to allocate buffer again.\n",
      "/* 098 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 099 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 100 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 101 */         // failed to allocate the first page\n",
      "/* 102 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 103 */       }\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     // common sub-expressions\n",
      "/* 107 */\n",
      "/* 108 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 109 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_3_0, hashAgg_exprIsNull_3_0);\n",
      "/* 110 */\n",
      "/* 111 */   }\n",
      "/* 112 */\n",
      "/* 113 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 114 */     hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 115 */     double hashAgg_value_6 = -1.0;\n",
      "/* 116 */     do {\n",
      "/* 117 */       boolean hashAgg_isNull_7 = true;\n",
      "/* 118 */       double hashAgg_value_7 = -1.0;\n",
      "/* 119 */       hashAgg_hashAgg_isNull_8_0 = true;\n",
      "/* 120 */       double hashAgg_value_8 = -1.0;\n",
      "/* 121 */       do {\n",
      "/* 122 */         boolean hashAgg_isNull_9 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 123 */         double hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 124 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 125 */         if (!hashAgg_isNull_9) {\n",
      "/* 126 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 127 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 128 */           continue;\n",
      "/* 129 */         }\n",
      "/* 130 */\n",
      "/* 131 */         if (!false) {\n",
      "/* 132 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 133 */           hashAgg_value_8 = 0.0D;\n",
      "/* 134 */           continue;\n",
      "/* 135 */         }\n",
      "/* 136 */\n",
      "/* 137 */       } while (false);\n",
      "/* 138 */\n",
      "/* 139 */       if (!hashAgg_exprIsNull_3_0) {\n",
      "/* 140 */         hashAgg_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 141 */\n",
      "/* 142 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_3_0;\n",
      "/* 143 */\n",
      "/* 144 */       }\n",
      "/* 145 */       if (!hashAgg_isNull_7) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 147 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */       boolean hashAgg_isNull_12 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 152 */       double hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 153 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 154 */       if (!hashAgg_isNull_12) {\n",
      "/* 155 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 156 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 157 */         continue;\n",
      "/* 158 */       }\n",
      "/* 159 */\n",
      "/* 160 */     } while (false);\n",
      "/* 161 */\n",
      "/* 162 */     if (!hashAgg_hashAgg_isNull_6_0) {\n",
      "/* 163 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 164 */     } else {\n",
      "/* 165 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 166 */     }\n",
      "/* 167 */   }\n",
      "/* 168 */\n",
      "/* 169 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 170 */   throws java.io.IOException {\n",
      "/* 171 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 172 */\n",
      "/* 173 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 174 */     int hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 175 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 176 */     boolean hashAgg_isNull_14 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 177 */     int hashAgg_value_14 = hashAgg_isNull_14 ?\n",
      "/* 178 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 179 */     boolean hashAgg_isNull_15 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 180 */     int hashAgg_value_15 = hashAgg_isNull_15 ?\n",
      "/* 181 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 182 */     boolean hashAgg_isNull_16 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 183 */     double hashAgg_value_16 = hashAgg_isNull_16 ?\n",
      "/* 184 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 185 */\n",
      "/* 186 */     UTF8String hashAgg_value_18;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_18 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     UTF8String hashAgg_value_20;\n",
      "/* 193 */     if (hashAgg_isNull_14) {\n",
      "/* 194 */       hashAgg_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 195 */     } else {\n",
      "/* 196 */       hashAgg_value_20 = UTF8String.fromString(String.valueOf(hashAgg_value_14));\n",
      "/* 197 */     }\n",
      "/* 198 */     UTF8String hashAgg_value_22;\n",
      "/* 199 */     if (hashAgg_isNull_15) {\n",
      "/* 200 */       hashAgg_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 201 */     } else {\n",
      "/* 202 */       hashAgg_value_22 = UTF8String.fromString(String.valueOf(hashAgg_value_15));\n",
      "/* 203 */     }\n",
      "/* 204 */     UTF8String hashAgg_value_24;\n",
      "/* 205 */     if (hashAgg_isNull_16) {\n",
      "/* 206 */       hashAgg_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 207 */     } else {\n",
      "/* 208 */       hashAgg_value_24 = UTF8String.fromString(String.valueOf(hashAgg_value_16));\n",
      "/* 209 */     }\n",
      "/* 210 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 211 */\n",
      "/* 212 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_18);\n",
      "/* 213 */\n",
      "/* 214 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_20);\n",
      "/* 215 */\n",
      "/* 216 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_22);\n",
      "/* 217 */\n",
      "/* 218 */     hashAgg_mutableStateArray_0[1].write(3, hashAgg_value_24);\n",
      "/* 219 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 220 */\n",
      "/* 221 */   }\n",
      "/* 222 */\n",
      "/* 223 */   protected void processNext() throws java.io.IOException {\n",
      "/* 224 */     if (!hashAgg_initAgg_0) {\n",
      "/* 225 */       hashAgg_initAgg_0 = true;\n",
      "/* 226 */\n",
      "/* 227 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 228 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 229 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 230 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 231 */     }\n",
      "/* 232 */     // output the result\n",
      "/* 233 */\n",
      "/* 234 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 235 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 236 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 237 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 238 */       if (shouldStop()) return;\n",
      "/* 239 */     }\n",
      "/* 240 */     hashAgg_mapIter_0.close();\n",
      "/* 241 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 242 */       hashAgg_hashMap_0.free();\n",
      "/* 243 */     }\n",
      "/* 244 */   }\n",
      "/* 245 */\n",
      "/* 246 */ }\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+------------------+\n",
      "|year|quarter|month|   sum(saleamount)|\n",
      "+----+-------+-----+------------------+\n",
      "|NULL|   NULL| NULL|108556.34000000004|\n",
      "+----+-------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:43:46 INFO CodeGenerator: Code generated in 16.109219 ms\n",
      "25/04/11 09:43:46 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:43:46 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, int, true],input[2, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     int value_2 = isNull_2 ?\n",
      "/* 051 */     -1 : (i.getInt(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 36 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@60a7627f\n",
      "25/04/11 09:43:46 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 36 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@60a7627f\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 36 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@60a7627f\n",
      "25/04/11 09:43:46 DEBUG TaskMemoryManager: Task 36 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@60a7627f\n",
      "25/04/11 09:43:46 INFO Executor: Finished task 0.0 in stage 43.0 (TID 36). 5162 bytes result sent to driver\n",
      "25/04/11 09:43:46 DEBUG ExecutorMetricsPoller: stageTCMP: (43, 0) -> 0\n",
      "25/04/11 09:43:46 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 36) in 30 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:43:46 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:43:46 INFO DAGScheduler: ResultStage 43 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: After removal of stage 43, remaining stages = 1\n",
      "25/04/11 09:43:46 DEBUG DAGScheduler: After removal of stage 42, remaining stages = 0\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:43:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished\n",
      "25/04/11 09:43:46 INFO DAGScheduler: Job 36 finished: showString at NativeMethodAccessorImpl.java:0, took 0.037689 s\n",
      "25/04/11 09:43:46 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(2) HashAggregate(keys=[year#943, quarter#954, month#966], functions=[sum(saleamount#682)], output=[toprettystring(year)#1001, toprettystring(quarter)#1002, toprettystring(month)#1003, toprettystring(sum(saleamount))#1004])\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange hashpartitioning(year#943, quarter#954, month#966, 200), ENSURE_REQUIREMENTS, [plan_id=755]\n",
      "         +- *(1) HashAggregate(keys=[year#943, quarter#954, month#966], functions=[partial_sum(saleamount#682)], output=[year#943, quarter#954, month#966, sum#1010])\n",
      "            +- *(1) Project [saleamount#682, year(saledate#899) AS year#943, quarter(saledate#899) AS quarter#954, month(saledate#899) AS month#966]\n",
      "               +- *(1) Project [cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(cast(gettimestamp(saledate#677, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date), yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#899, saleamount#682]\n",
      "                  +- FileScan csv [saledate#677,saleamount#682] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:43:46 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, StructField(toprettystring(year),StringType,false), StructField(toprettystring(quarter),StringType,false), StructField(toprettystring(month),StringType,false), StructField(toprettystring(sum(saleamount)),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[4];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */\n",
      "/* 060 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 061 */\n",
      "/* 062 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 063 */     boolean isNull_1 = true;\n",
      "/* 064 */     java.lang.String value_1 = null;\n",
      "/* 065 */     isNull_1 = false;\n",
      "/* 066 */     if (!isNull_1) {\n",
      "/* 067 */\n",
      "/* 068 */       Object funcResult_0 = null;\n",
      "/* 069 */       funcResult_0 = value_2.toString();\n",
      "/* 070 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 071 */\n",
      "/* 072 */     }\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       values_0[0] = null;\n",
      "/* 075 */     } else {\n",
      "/* 076 */       values_0[0] = value_1;\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 080 */     boolean isNull_3 = true;\n",
      "/* 081 */     java.lang.String value_3 = null;\n",
      "/* 082 */     isNull_3 = false;\n",
      "/* 083 */     if (!isNull_3) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_1 = null;\n",
      "/* 086 */       funcResult_1 = value_4.toString();\n",
      "/* 087 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_3) {\n",
      "/* 091 */       values_0[1] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[1] = value_3;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 097 */     boolean isNull_5 = true;\n",
      "/* 098 */     java.lang.String value_5 = null;\n",
      "/* 099 */     isNull_5 = false;\n",
      "/* 100 */     if (!isNull_5) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_2 = null;\n",
      "/* 103 */       funcResult_2 = value_6.toString();\n",
      "/* 104 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_5) {\n",
      "/* 108 */       values_0[2] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[2] = value_5;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */   }\n",
      "/* 114 */\n",
      "/* 115 */ }\n",
      "\n",
      "25/04/11 09:43:46 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[4];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */\n",
      "/* 060 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 061 */\n",
      "/* 062 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 063 */     boolean isNull_1 = true;\n",
      "/* 064 */     java.lang.String value_1 = null;\n",
      "/* 065 */     isNull_1 = false;\n",
      "/* 066 */     if (!isNull_1) {\n",
      "/* 067 */\n",
      "/* 068 */       Object funcResult_0 = null;\n",
      "/* 069 */       funcResult_0 = value_2.toString();\n",
      "/* 070 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 071 */\n",
      "/* 072 */     }\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       values_0[0] = null;\n",
      "/* 075 */     } else {\n",
      "/* 076 */       values_0[0] = value_1;\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 080 */     boolean isNull_3 = true;\n",
      "/* 081 */     java.lang.String value_3 = null;\n",
      "/* 082 */     isNull_3 = false;\n",
      "/* 083 */     if (!isNull_3) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_1 = null;\n",
      "/* 086 */       funcResult_1 = value_4.toString();\n",
      "/* 087 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_3) {\n",
      "/* 091 */       values_0[1] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[1] = value_3;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 097 */     boolean isNull_5 = true;\n",
      "/* 098 */     java.lang.String value_5 = null;\n",
      "/* 099 */     isNull_5 = false;\n",
      "/* 100 */     if (!isNull_5) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_2 = null;\n",
      "/* 103 */       funcResult_2 = value_6.toString();\n",
      "/* 104 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_5) {\n",
      "/* 108 */       values_0[2] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[2] = value_5;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */   }\n",
      "/* 114 */\n",
      "/* 115 */ }\n",
      "\n",
      "25/04/11 09:43:46 INFO CodeGenerator: Code generated in 7.23878 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:43:55 DEBUG ExecutorMetricsPoller: removing (38, 0) from stageTCMP\n",
      "25/04/11 09:43:55 DEBUG ExecutorMetricsPoller: removing (41, 0) from stageTCMP\n",
      "25/04/11 09:43:55 DEBUG ExecutorMetricsPoller: removing (40, 0) from stageTCMP\n",
      "25/04/11 09:43:55 DEBUG ExecutorMetricsPoller: removing (43, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, quarter, month\n",
    "\n",
    "# Step 1: Ensure the `saledate` is in date format if it's a string\n",
    "df_sales = df_sales.withColumn(\"saledate\", to_date(col(\"saledate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Step 2: Filter the data (Make sure `saledate` is in date format for comparison)\n",
    "df_filtered = df_sales.filter(df_sales.saledate >= \"2023-01-01\")\n",
    "\n",
    "# Step 3: Group by `billtype` and `storeid`, sum `amount`\n",
    "df_sales.groupby(\"billtype\", \"storeid\").sum(\"saleamount\").show()\n",
    "\n",
    "# Step 4: Extract `year`, `quarter`, and `month` from `saledate`\n",
    "df_sales = df_sales.withColumn(\"year\", year(df_sales.saledate))\n",
    "df_sales = df_sales.withColumn(\"quarter\", quarter(df_sales.saledate))\n",
    "df_sales = df_sales.withColumn(\"month\", month(df_sales.saledate))\n",
    "\n",
    "# Step 5: Group by `year`, `quarter`, and `month`, sum `saleamount`\n",
    "df_sales.groupby(\"year\", \"quarter\", \"month\").sum(\"saleamount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98a648b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'total_spent'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_top_customers_pd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df48d5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All imports at the top, organized\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.barplot(data=df_top_customers_pd, x=\"Name\", y=\"total_spent\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
