{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4603db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: py4j in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /Users/craigwilcox/Projects/smart-store-craigwilcox/.venv/lib/python3.13/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "! pip install pyspark\n",
    "! pip install py4j\n",
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21a19125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x112e29550>\n"
     ]
    }
   ],
   "source": [
    "# Verify Spark works\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SmartSales\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eeb447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/04/11 09:59:12 DEBUG SparkSession: Configurations that might not take effect:\n",
      "  spark.app.name=sales_data_preparedcsv\n",
      "25/04/11 09:59:12 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "25/04/11 09:59:12 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:59:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:59:12 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#1020\n",
      "25/04/11 09:59:12 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#1027\n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#1020, None)) > 0)\n",
      "25/04/11 09:59:12 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 351.9 KiB, free 364.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_63 locally took 1 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_63 without replication took 1 ms\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 364.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_63_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:12 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMaster: Updated info of block broadcast_63_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Told master about block broadcast_63_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_63_piece0 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_63_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO SparkContext: Created broadcast 63 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:12 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 153 took 0.000218 seconds\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Got job 37 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Final stage: ResultStage 44 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: submitStage(ResultStage 44 (name=load at NativeMethodAccessorImpl.java:0;jobs=37))\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[153] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: submitMissingTasks(ResultStage 44)\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 13.5 KiB, free 364.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_64 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_64 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 364.1 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_64_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:12 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on macbookpro.lan:57375 (size: 6.4 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMaster: Updated info of block broadcast_64_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Told master about block broadcast_64_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_64_piece0 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_64_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[153] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Epoch for TaskSet 44.0: 5\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Valid locality levels for TaskSet 44.0: NO_PREF, ANY\n",
      "25/04/11 09:59:12 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_44.0, runningTasks: 0\n",
      "25/04/11 09:59:12 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 37) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:12 INFO Executor: Running task 0.0 in stage 44.0 (TID 37)\n",
      "25/04/11 09:59:12 DEBUG ExecutorMetricsPoller: stageTCMP: (44, 0) -> 1\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Getting local block broadcast_64\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Level for block broadcast_64 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:12 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:12 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Getting local block broadcast_63\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Level for block broadcast_63 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:12 INFO Executor: Finished task 0.0 in stage 44.0 (TID 37). 1666 bytes result sent to driver\n",
      "25/04/11 09:59:12 DEBUG ExecutorMetricsPoller: stageTCMP: (44, 0) -> 0\n",
      "25/04/11 09:59:12 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 37) in 7 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:12 INFO DAGScheduler: ResultStage 44 (load at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: After removal of stage 44, remaining stages = 0\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Job 37 finished: load at NativeMethodAccessorImpl.java:0, took 0.013047 s\n",
      "25/04/11 09:59:12 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 351.9 KiB, free 363.8 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_65 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_65 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 363.8 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_65_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:12 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.2 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMaster: Updated info of block broadcast_65_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Told master about block broadcast_65_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_65_piece0 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_65_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO SparkContext: Created broadcast 65 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$inferFromDataset$2\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$2\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$2) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$3\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$3) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:59:12 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 159 took 0.000037 seconds\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Got job 38 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Final stage: ResultStage 45 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: submitStage(ResultStage 45 (name=load at NativeMethodAccessorImpl.java:0;jobs=38))\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[159] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: submitMissingTasks(ResultStage 45)\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 28.2 KiB, free 363.7 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_66 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_66 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 363.7 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_66_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:12 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on macbookpro.lan:57375 (size: 13.0 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMaster: Updated info of block broadcast_66_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Told master about block broadcast_66_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_66_piece0 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_66_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[159] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Epoch for TaskSet 45.0: 5\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: Valid locality levels for TaskSet 45.0: NO_PREF, ANY\n",
      "25/04/11 09:59:12 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_45.0, runningTasks: 0\n",
      "25/04/11 09:59:12 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 38) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:59:12 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:12 INFO Executor: Running task 0.0 in stage 45.0 (TID 38)\n",
      "25/04/11 09:59:12 DEBUG ExecutorMetricsPoller: stageTCMP: (45, 0) -> 1\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Getting local block broadcast_66\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Level for block broadcast_66 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:12 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:59:12 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:12 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Getting local block broadcast_65\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Level for block broadcast_65 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:12 INFO Executor: Finished task 0.0 in stage 45.0 (TID 38). 1656 bytes result sent to driver\n",
      "25/04/11 09:59:12 DEBUG ExecutorMetricsPoller: stageTCMP: (45, 0) -> 0\n",
      "25/04/11 09:59:12 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 38) in 10 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:12 INFO DAGScheduler: ResultStage 45 (load at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "25/04/11 09:59:12 DEBUG DAGScheduler: After removal of stage 45, remaining stages = 0\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished\n",
      "25/04/11 09:59:12 INFO DAGScheduler: Job 38 finished: load at NativeMethodAccessorImpl.java:0, took 0.019240 s\n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:59:12 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       // common sub-expressions\n",
      "/* 029 */\n",
      "/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 031 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 032 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 033 */       UTF8String project_value_0;\n",
      "/* 034 */       if (inputadapter_isNull_0) {\n",
      "/* 035 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 036 */       } else {\n",
      "/* 037 */         project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));\n",
      "/* 038 */       }\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 042 */       UTF8String project_value_2;\n",
      "/* 043 */       if (inputadapter_isNull_1) {\n",
      "/* 044 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_2 = inputadapter_value_1;\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 049 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 050 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 051 */       UTF8String project_value_4;\n",
      "/* 052 */       if (inputadapter_isNull_2) {\n",
      "/* 053 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_4 = UTF8String.fromString(String.valueOf(inputadapter_value_2));\n",
      "/* 056 */       }\n",
      "/* 057 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 058 */       int inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 059 */       -1 : (inputadapter_row_0.getInt(3));\n",
      "/* 060 */       UTF8String project_value_6;\n",
      "/* 061 */       if (inputadapter_isNull_3) {\n",
      "/* 062 */         project_value_6 = UTF8String.fromString(\"NULL\");\n",
      "/* 063 */       } else {\n",
      "/* 064 */         project_value_6 = UTF8String.fromString(String.valueOf(inputadapter_value_3));\n",
      "/* 065 */       }\n",
      "/* 066 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n",
      "/* 067 */       int inputadapter_value_4 = inputadapter_isNull_4 ?\n",
      "/* 068 */       -1 : (inputadapter_row_0.getInt(4));\n",
      "/* 069 */       UTF8String project_value_8;\n",
      "/* 070 */       if (inputadapter_isNull_4) {\n",
      "/* 071 */         project_value_8 = UTF8String.fromString(\"NULL\");\n",
      "/* 072 */       } else {\n",
      "/* 073 */         project_value_8 = UTF8String.fromString(String.valueOf(inputadapter_value_4));\n",
      "/* 074 */       }\n",
      "/* 075 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n",
      "/* 076 */       int inputadapter_value_5 = inputadapter_isNull_5 ?\n",
      "/* 077 */       -1 : (inputadapter_row_0.getInt(5));\n",
      "/* 078 */       UTF8String project_value_10;\n",
      "/* 079 */       if (inputadapter_isNull_5) {\n",
      "/* 080 */         project_value_10 = UTF8String.fromString(\"NULL\");\n",
      "/* 081 */       } else {\n",
      "/* 082 */         project_value_10 = UTF8String.fromString(String.valueOf(inputadapter_value_5));\n",
      "/* 083 */       }\n",
      "/* 084 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n",
      "/* 085 */       double inputadapter_value_6 = inputadapter_isNull_6 ?\n",
      "/* 086 */       -1.0 : (inputadapter_row_0.getDouble(6));\n",
      "/* 087 */       UTF8String project_value_12;\n",
      "/* 088 */       if (inputadapter_isNull_6) {\n",
      "/* 089 */         project_value_12 = UTF8String.fromString(\"NULL\");\n",
      "/* 090 */       } else {\n",
      "/* 091 */         project_value_12 = UTF8String.fromString(String.valueOf(inputadapter_value_6));\n",
      "/* 092 */       }\n",
      "/* 093 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n",
      "/* 094 */       int inputadapter_value_7 = inputadapter_isNull_7 ?\n",
      "/* 095 */       -1 : (inputadapter_row_0.getInt(7));\n",
      "/* 096 */       UTF8String project_value_14;\n",
      "/* 097 */       if (inputadapter_isNull_7) {\n",
      "/* 098 */         project_value_14 = UTF8String.fromString(\"NULL\");\n",
      "/* 099 */       } else {\n",
      "/* 100 */         project_value_14 = UTF8String.fromString(String.valueOf(inputadapter_value_7));\n",
      "/* 101 */       }\n",
      "/* 102 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n",
      "/* 103 */       UTF8String inputadapter_value_8 = inputadapter_isNull_8 ?\n",
      "/* 104 */       null : (inputadapter_row_0.getUTF8String(8));\n",
      "/* 105 */       UTF8String project_value_16;\n",
      "/* 106 */       if (inputadapter_isNull_8) {\n",
      "/* 107 */         project_value_16 = UTF8String.fromString(\"NULL\");\n",
      "/* 108 */       } else {\n",
      "/* 109 */         project_value_16 = inputadapter_value_8;\n",
      "/* 110 */       }\n",
      "/* 111 */       project_mutableStateArray_0[0].reset();\n",
      "/* 112 */\n",
      "/* 113 */       project_mutableStateArray_0[0].write(0, project_value_0);\n",
      "/* 114 */\n",
      "/* 115 */       project_mutableStateArray_0[0].write(1, project_value_2);\n",
      "/* 116 */\n",
      "/* 117 */       project_mutableStateArray_0[0].write(2, project_value_4);\n",
      "/* 118 */\n",
      "/* 119 */       project_mutableStateArray_0[0].write(3, project_value_6);\n",
      "/* 120 */\n",
      "/* 121 */       project_mutableStateArray_0[0].write(4, project_value_8);\n",
      "/* 122 */\n",
      "/* 123 */       project_mutableStateArray_0[0].write(5, project_value_10);\n",
      "/* 124 */\n",
      "/* 125 */       project_mutableStateArray_0[0].write(6, project_value_12);\n",
      "/* 126 */\n",
      "/* 127 */       project_mutableStateArray_0[0].write(7, project_value_14);\n",
      "/* 128 */\n",
      "/* 129 */       project_mutableStateArray_0[0].write(8, project_value_16);\n",
      "/* 130 */       append((project_mutableStateArray_0[0].getRow()));\n",
      "/* 131 */       if (shouldStop()) return;\n",
      "/* 132 */     }\n",
      "/* 133 */   }\n",
      "/* 134 */\n",
      "/* 135 */ }\n",
      "\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 351.8 KiB, free 363.4 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_67 locally took 1 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_67 without replication took 1 ms\n",
      "25/04/11 09:59:12 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 363.4 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_67_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "|transactionid|saledate|customerid|productid|storeid|campaignid|saleamount|loyaltypercentage|billtype|\n",
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "|          550|  1/6/24|      1008|      102|    404|         0|      39.1|                5|    Paid|\n",
      "|          551|  1/6/24|      1009|      105|    403|         0|     19.78|                5|    Paid|\n",
      "|          552| 1/16/24|      1004|      107|    404|         0|     335.1|                5|    Paid|\n",
      "|          553| 1/16/24|      1006|      102|    406|         0|     195.5|                5| Invoice|\n",
      "|          554| 1/25/24|      1005|      102|    405|         0|     117.3|                5| Invoice|\n",
      "|          555| 1/25/24|      1001|      101|    401|         0|   2379.36|               20| Invoice|\n",
      "|          556| 1/29/24|      1009|      104|    403|         0|     172.4|                5|    Paid|\n",
      "|          557| 1/29/24|      1010|      101|    402|         0|   3172.48|               20|  Credit|\n",
      "|          558|  2/6/24|      1002|      102|    402|         0|     312.8|                5| Invoice|\n",
      "|          559|  2/6/24|      1001|      106|    401|         0|    622.86|               10|  Credit|\n",
      "|          560|  2/6/24|      1010|      101|    402|         0|   6344.96|               20|  Credit|\n",
      "|          561|  2/6/24|      1005|      107|    405|         0|    469.14|                5|  Credit|\n",
      "|          562|  2/8/24|      1003|      108|    403|         0|     12.56|                5|    Paid|\n",
      "|          563|  2/8/24|      1006|      107|    406|         0|     67.02|                5| Invoice|\n",
      "|          564|  2/9/24|      1009|      107|    403|         0|    469.14|                5|    Paid|\n",
      "|          565|  2/9/24|      1002|      105|    402|         0|    138.46|                5| Invoice|\n",
      "|          566| 2/24/24|      1007|      103|    405|         0|    204.84|                5|    Paid|\n",
      "|          567| 2/24/24|      1007|      106|    405|         0|     444.9|                5|    Paid|\n",
      "|          568| 2/24/24|      1011|      107|    401|         0|    603.18|               10|    Paid|\n",
      "|          569| 2/24/24|      1010|      101|    402|         0|    3965.6|               20| Invoice|\n",
      "+-------------+--------+----------+---------+-------+----------+----------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:12 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:12 DEBUG BlockManagerMaster: Updated info of block broadcast_67_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Told master about block broadcast_67_piece0\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Put block broadcast_67_piece0 locally took 0 ms\n",
      "25/04/11 09:59:12 DEBUG BlockManager: Putting block broadcast_67_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:12 INFO SparkContext: Created broadcast 67 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:12 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 163 took 0.000037 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 39 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 46 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 46 (name=showString at NativeMethodAccessorImpl.java:0;jobs=39))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[163] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 46)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 18.8 KiB, free 363.3 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_68 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_68 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 363.3 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_68_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on macbookpro.lan:57375 (size: 8.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_68_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_68_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_68_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_68_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[163] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 46.0: 5\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 46.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_46.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 39) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9843 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 46.0 (TID 39)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (46, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_68\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_68 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true],input[2, int, true],input[3, int, true],input[4, int, true],input[5, int, true],input[6, double, true],input[7, int, true],input[8, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */     writeFields_0_0(i);\n",
      "/* 031 */     writeFields_0_1(i);\n",
      "/* 032 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 033 */   }\n",
      "/* 034 */\n",
      "/* 035 */\n",
      "/* 036 */   private void writeFields_0_1(InternalRow i) {\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_5 = i.isNullAt(5);\n",
      "/* 039 */     int value_5 = isNull_5 ?\n",
      "/* 040 */     -1 : (i.getInt(5));\n",
      "/* 041 */     if (isNull_5) {\n",
      "/* 042 */       mutableStateArray_0[0].setNullAt(5);\n",
      "/* 043 */     } else {\n",
      "/* 044 */       mutableStateArray_0[0].write(5, value_5);\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     boolean isNull_6 = i.isNullAt(6);\n",
      "/* 048 */     double value_6 = isNull_6 ?\n",
      "/* 049 */     -1.0 : (i.getDouble(6));\n",
      "/* 050 */     if (isNull_6) {\n",
      "/* 051 */       mutableStateArray_0[0].setNullAt(6);\n",
      "/* 052 */     } else {\n",
      "/* 053 */       mutableStateArray_0[0].write(6, value_6);\n",
      "/* 054 */     }\n",
      "/* 055 */\n",
      "/* 056 */     boolean isNull_7 = i.isNullAt(7);\n",
      "/* 057 */     int value_7 = isNull_7 ?\n",
      "/* 058 */     -1 : (i.getInt(7));\n",
      "/* 059 */     if (isNull_7) {\n",
      "/* 060 */       mutableStateArray_0[0].setNullAt(7);\n",
      "/* 061 */     } else {\n",
      "/* 062 */       mutableStateArray_0[0].write(7, value_7);\n",
      "/* 063 */     }\n",
      "/* 064 */\n",
      "/* 065 */     boolean isNull_8 = i.isNullAt(8);\n",
      "/* 066 */     UTF8String value_8 = isNull_8 ?\n",
      "/* 067 */     null : (i.getUTF8String(8));\n",
      "/* 068 */     if (isNull_8) {\n",
      "/* 069 */       mutableStateArray_0[0].setNullAt(8);\n",
      "/* 070 */     } else {\n",
      "/* 071 */       mutableStateArray_0[0].write(8, value_8);\n",
      "/* 072 */     }\n",
      "/* 073 */\n",
      "/* 074 */   }\n",
      "/* 075 */\n",
      "/* 076 */\n",
      "/* 077 */   private void writeFields_0_0(InternalRow i) {\n",
      "/* 078 */\n",
      "/* 079 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 080 */     int value_0 = isNull_0 ?\n",
      "/* 081 */     -1 : (i.getInt(0));\n",
      "/* 082 */     if (isNull_0) {\n",
      "/* 083 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 084 */     } else {\n",
      "/* 085 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 089 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 090 */     null : (i.getUTF8String(1));\n",
      "/* 091 */     if (isNull_1) {\n",
      "/* 092 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 093 */     } else {\n",
      "/* 094 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 098 */     int value_2 = isNull_2 ?\n",
      "/* 099 */     -1 : (i.getInt(2));\n",
      "/* 100 */     if (isNull_2) {\n",
      "/* 101 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 102 */     } else {\n",
      "/* 103 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     boolean isNull_3 = i.isNullAt(3);\n",
      "/* 107 */     int value_3 = isNull_3 ?\n",
      "/* 108 */     -1 : (i.getInt(3));\n",
      "/* 109 */     if (isNull_3) {\n",
      "/* 110 */       mutableStateArray_0[0].setNullAt(3);\n",
      "/* 111 */     } else {\n",
      "/* 112 */       mutableStateArray_0[0].write(3, value_3);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     boolean isNull_4 = i.isNullAt(4);\n",
      "/* 116 */     int value_4 = isNull_4 ?\n",
      "/* 117 */     -1 : (i.getInt(4));\n",
      "/* 118 */     if (isNull_4) {\n",
      "/* 119 */       mutableStateArray_0[0].setNullAt(4);\n",
      "/* 120 */     } else {\n",
      "/* 121 */       mutableStateArray_0[0].write(4, value_4);\n",
      "/* 122 */     }\n",
      "/* 123 */\n",
      "/* 124 */   }\n",
      "/* 125 */\n",
      "/* 126 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_67\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_67 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 46.0 (TID 39). 2495 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (46, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 39) in 10 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 46 (showString at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 46, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 39 finished: showString at NativeMethodAccessorImpl.java:0, took 0.015051 s\n",
      "25/04/11 09:59:13 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, input[4, string, false].toString, input[5, string, false].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, false].toString, StructField(toprettystring(transactionid),StringType,false), StructField(toprettystring(saledate),StringType,false), StructField(toprettystring(customerid),StringType,false), StructField(toprettystring(productid),StringType,false), StructField(toprettystring(storeid),StringType,false), StructField(toprettystring(campaignid),StringType,false), StructField(toprettystring(saleamount),StringType,false), StructField(toprettystring(loyaltypercentage),StringType,false), StructField(toprettystring(billtype),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[9];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     createExternalRow_0_2(i, values_0);\n",
      "/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 028 */     if (false) {\n",
      "/* 029 */       mutableRow.setNullAt(0);\n",
      "/* 030 */     } else {\n",
      "/* 031 */\n",
      "/* 032 */       mutableRow.update(0, value_0);\n",
      "/* 033 */     }\n",
      "/* 034 */\n",
      "/* 035 */     return mutableRow;\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {\n",
      "/* 040 */\n",
      "/* 041 */     UTF8String value_14 = i.getUTF8String(6);\n",
      "/* 042 */     boolean isNull_13 = true;\n",
      "/* 043 */     java.lang.String value_13 = null;\n",
      "/* 044 */     isNull_13 = false;\n",
      "/* 045 */     if (!isNull_13) {\n",
      "/* 046 */\n",
      "/* 047 */       Object funcResult_6 = null;\n",
      "/* 048 */       funcResult_6 = value_14.toString();\n",
      "/* 049 */       value_13 = (java.lang.String) funcResult_6;\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_13) {\n",
      "/* 053 */       values_0[6] = null;\n",
      "/* 054 */     } else {\n",
      "/* 055 */       values_0[6] = value_13;\n",
      "/* 056 */     }\n",
      "/* 057 */\n",
      "/* 058 */     UTF8String value_16 = i.getUTF8String(7);\n",
      "/* 059 */     boolean isNull_15 = true;\n",
      "/* 060 */     java.lang.String value_15 = null;\n",
      "/* 061 */     isNull_15 = false;\n",
      "/* 062 */     if (!isNull_15) {\n",
      "/* 063 */\n",
      "/* 064 */       Object funcResult_7 = null;\n",
      "/* 065 */       funcResult_7 = value_16.toString();\n",
      "/* 066 */       value_15 = (java.lang.String) funcResult_7;\n",
      "/* 067 */\n",
      "/* 068 */     }\n",
      "/* 069 */     if (isNull_15) {\n",
      "/* 070 */       values_0[7] = null;\n",
      "/* 071 */     } else {\n",
      "/* 072 */       values_0[7] = value_15;\n",
      "/* 073 */     }\n",
      "/* 074 */\n",
      "/* 075 */     UTF8String value_18 = i.getUTF8String(8);\n",
      "/* 076 */     boolean isNull_17 = true;\n",
      "/* 077 */     java.lang.String value_17 = null;\n",
      "/* 078 */     isNull_17 = false;\n",
      "/* 079 */     if (!isNull_17) {\n",
      "/* 080 */\n",
      "/* 081 */       Object funcResult_8 = null;\n",
      "/* 082 */       funcResult_8 = value_18.toString();\n",
      "/* 083 */       value_17 = (java.lang.String) funcResult_8;\n",
      "/* 084 */\n",
      "/* 085 */     }\n",
      "/* 086 */     if (isNull_17) {\n",
      "/* 087 */       values_0[8] = null;\n",
      "/* 088 */     } else {\n",
      "/* 089 */       values_0[8] = value_17;\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */\n",
      "/* 095 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 096 */\n",
      "/* 097 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 098 */     boolean isNull_7 = true;\n",
      "/* 099 */     java.lang.String value_7 = null;\n",
      "/* 100 */     isNull_7 = false;\n",
      "/* 101 */     if (!isNull_7) {\n",
      "/* 102 */\n",
      "/* 103 */       Object funcResult_3 = null;\n",
      "/* 104 */       funcResult_3 = value_8.toString();\n",
      "/* 105 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 106 */\n",
      "/* 107 */     }\n",
      "/* 108 */     if (isNull_7) {\n",
      "/* 109 */       values_0[3] = null;\n",
      "/* 110 */     } else {\n",
      "/* 111 */       values_0[3] = value_7;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     UTF8String value_10 = i.getUTF8String(4);\n",
      "/* 115 */     boolean isNull_9 = true;\n",
      "/* 116 */     java.lang.String value_9 = null;\n",
      "/* 117 */     isNull_9 = false;\n",
      "/* 118 */     if (!isNull_9) {\n",
      "/* 119 */\n",
      "/* 120 */       Object funcResult_4 = null;\n",
      "/* 121 */       funcResult_4 = value_10.toString();\n",
      "/* 122 */       value_9 = (java.lang.String) funcResult_4;\n",
      "/* 123 */\n",
      "/* 124 */     }\n",
      "/* 125 */     if (isNull_9) {\n",
      "/* 126 */       values_0[4] = null;\n",
      "/* 127 */     } else {\n",
      "/* 128 */       values_0[4] = value_9;\n",
      "/* 129 */     }\n",
      "/* 130 */\n",
      "/* 131 */     UTF8String value_12 = i.getUTF8String(5);\n",
      "/* 132 */     boolean isNull_11 = true;\n",
      "/* 133 */     java.lang.String value_11 = null;\n",
      "/* 134 */     isNull_11 = false;\n",
      "/* 135 */     if (!isNull_11) {\n",
      "/* 136 */\n",
      "/* 137 */       Object funcResult_5 = null;\n",
      "/* 138 */       funcResult_5 = value_12.toString();\n",
      "/* 139 */       value_11 = (java.lang.String) funcResult_5;\n",
      "/* 140 */\n",
      "/* 141 */     }\n",
      "/* 142 */     if (isNull_11) {\n",
      "/* 143 */       values_0[5] = null;\n",
      "/* 144 */     } else {\n",
      "/* 145 */       values_0[5] = value_11;\n",
      "/* 146 */     }\n",
      "/* 147 */\n",
      "/* 148 */   }\n",
      "/* 149 */\n",
      "/* 150 */\n",
      "/* 151 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 152 */\n",
      "/* 153 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 154 */     boolean isNull_1 = true;\n",
      "/* 155 */     java.lang.String value_1 = null;\n",
      "/* 156 */     isNull_1 = false;\n",
      "/* 157 */     if (!isNull_1) {\n",
      "/* 158 */\n",
      "/* 159 */       Object funcResult_0 = null;\n",
      "/* 160 */       funcResult_0 = value_2.toString();\n",
      "/* 161 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 162 */\n",
      "/* 163 */     }\n",
      "/* 164 */     if (isNull_1) {\n",
      "/* 165 */       values_0[0] = null;\n",
      "/* 166 */     } else {\n",
      "/* 167 */       values_0[0] = value_1;\n",
      "/* 168 */     }\n",
      "/* 169 */\n",
      "/* 170 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 171 */     boolean isNull_3 = true;\n",
      "/* 172 */     java.lang.String value_3 = null;\n",
      "/* 173 */     isNull_3 = false;\n",
      "/* 174 */     if (!isNull_3) {\n",
      "/* 175 */\n",
      "/* 176 */       Object funcResult_1 = null;\n",
      "/* 177 */       funcResult_1 = value_4.toString();\n",
      "/* 178 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 179 */\n",
      "/* 180 */     }\n",
      "/* 181 */     if (isNull_3) {\n",
      "/* 182 */       values_0[1] = null;\n",
      "/* 183 */     } else {\n",
      "/* 184 */       values_0[1] = value_3;\n",
      "/* 185 */     }\n",
      "/* 186 */\n",
      "/* 187 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 188 */     boolean isNull_5 = true;\n",
      "/* 189 */     java.lang.String value_5 = null;\n",
      "/* 190 */     isNull_5 = false;\n",
      "/* 191 */     if (!isNull_5) {\n",
      "/* 192 */\n",
      "/* 193 */       Object funcResult_2 = null;\n",
      "/* 194 */       funcResult_2 = value_6.toString();\n",
      "/* 195 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 196 */\n",
      "/* 197 */     }\n",
      "/* 198 */     if (isNull_5) {\n",
      "/* 199 */       values_0[2] = null;\n",
      "/* 200 */     } else {\n",
      "/* 201 */       values_0[2] = value_5;\n",
      "/* 202 */     }\n",
      "/* 203 */\n",
      "/* 204 */   }\n",
      "/* 205 */\n",
      "/* 206 */ }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sales Table\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sales_data_preparedcsv\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_path = \"/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv\"\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df_sales = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "# Show data\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3283aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:13 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "25/04/11 09:59:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:59:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/04/11 09:59:13 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#1102\n",
      "25/04/11 09:59:13 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#1109\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#1102, None)) > 0)\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 351.9 KiB, free 363.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_69 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_69 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 362.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_69_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_69_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_69_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_69_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_69_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 69 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 167 took 0.000034 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 40 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 47 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 47 (name=load at NativeMethodAccessorImpl.java:0;jobs=40))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[167] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 47)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 13.5 KiB, free 362.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_70 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_70 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 362.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_70_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on macbookpro.lan:57375 (size: 6.4 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_70_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_70_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_70_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_70_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[167] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 47.0: 5\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 47.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_47.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 40) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 47.0 (TID 40)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (47, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_70\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_70 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_69\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_69 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 47.0 (TID 40). 1633 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (47, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 40) in 8 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 47 (load at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 47, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 40 finished: load at NativeMethodAccessorImpl.java:0, took 0.012856 s\n",
      "25/04/11 09:59:13 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 351.9 KiB, free 362.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_71 locally took 1 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_71 without replication took 1 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 362.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_71_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on macbookpro.lan:57375 (size: 34.8 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_71_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_71_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_71_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_71_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 71 from load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$inferFromDataset$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$3\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$3) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 173 took 0.000047 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 41 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 48 (load at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 48 (name=load at NativeMethodAccessorImpl.java:0;jobs=41))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[173] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 48)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 28.2 KiB, free 362.5 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_72 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_72 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 362.5 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_72_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on macbookpro.lan:57375 (size: 13.0 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_72_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_72_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_72_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_72_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[173] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 48.0: 5\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 48.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_48.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 41) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 48.0 (TID 41)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (48, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_72\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_72 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_71\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_71 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 48.0 (TID 41). 1588 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (48, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 41) in 9 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 48 (load at NativeMethodAccessorImpl.java:0) finished in 0.014 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 48, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 41 finished: load at NativeMethodAccessorImpl.java:0, took 0.016209 s\n",
      "25/04/11 09:59:13 DEBUG SparkSqlParser: Parsing command: customer\n"
     ]
    }
   ],
   "source": [
    "# Customers Tables\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n",
    "\n",
    "# Path to your customer CSV file\n",
    "customer_csv_path = \"/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv\"\n",
    "\n",
    "# Load customer data into DataFrame\n",
    "df_customer = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customer_csv_path)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df_customer.createOrReplaceTempView(\"customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e548fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/04/11 09:59:13 DEBUG SparkSession: Configurations that might not take effect:\n",
      "  spark.app.name=SmartSales\n",
      "25/04/11 09:59:13 DEBUG SparkSqlParser: Parsing command: sales\n",
      "25/04/11 09:59:13 DEBUG SparkSqlParser: Parsing command: customer\n",
      "25/04/11 09:59:13 DEBUG SparkSqlParser: Parsing command: \n",
      "    SELECT c.Name, SUM(s.saleamount) AS total_spent\n",
      "    FROM sales s\n",
      "    JOIN customer c ON s.customerid = c.CustomerID\n",
      "    GROUP BY c.Name\n",
      "    ORDER BY total_spent DESC\n",
      "\n",
      "25/04/11 09:59:13 DEBUG Analyzer$ResolveReferences: Resolving 's.customerid to customerid#1039\n",
      "25/04/11 09:59:13 DEBUG Analyzer$ResolveReferences: Resolving 'c.CustomerID to CustomerID#1119\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'c.Name to Name#1120\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'c.Name to Name#1120\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 's.saleamount to saleamount#1043\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInSort: Resolving 'total_spent to total_spent#1131\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#1039)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: TakeOrderedAndProject(limit=21, orderBy=[total_spent#1131 DESC NULLS LAST], output=[toprettystring(Name)#1137,toprettystring(total_spent)#1138])\n",
      "+- HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "   +- HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "      +- Project [saleamount#1043, Name#1120]\n",
      "         +- BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "            :- Project [customerid#1039, saleamount#1043]\n",
      "            :  +- Filter isnotnull(customerid#1039)\n",
      "            :     +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "            +- Project [CustomerID#1119, Name#1120]\n",
      "               +- Filter isnotnull(CustomerID#1119)\n",
      "                  +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BroadcastQueryStageExec: Materialize query stage BroadcastQueryStageExec: 0\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 351.8 KiB, free 362.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_73 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_73 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 362.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_73_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_73_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_73_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_73_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_73_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 73 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 177 took 0.000078 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 42 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 49 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=42))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[177] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 49)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 15.5 KiB, free 362.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_74 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_74 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 362.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_74_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on macbookpro.lan:57375 (size: 7.6 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_74_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_74_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_74_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_74_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[177] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 49.0: 5\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 49.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_49.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 42) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 49.0 (TID 42)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (49, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_74\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_74 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_73\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_73 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 49.0 (TID 42). 1862 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (49, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 42) in 10 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 49 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.014 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 49, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 42 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.015828 s\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 1024.3 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@69ab75e3\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for cast(input[0, int, false] as bigint):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     int value_1 = i.getInt(0);\n",
      "/* 032 */     boolean isNull_0 = false;\n",
      "/* 033 */     long value_0 = -1L;\n",
      "/* 034 */     if (!false) {\n",
      "/* 035 */       value_0 = (long) value_1;\n",
      "/* 036 */     }\n",
      "/* 037 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 512.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@69ab75e3\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 release 256.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@69ab75e3\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 88.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@69ab75e3\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 release 512.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@69ab75e3\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 1024.1 KiB, free 361.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_75 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_75 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 509.0 B, free 361.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_75_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on macbookpro.lan:57375 (size: 509.0 B, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_75_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_75_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_75_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_75_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 75 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#1039)\n",
      "25/04/11 09:59:13 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_75\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_75 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 351.8 KiB, free 360.8 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_76 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_76 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_76_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_76_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_76_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_76_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_76_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 76 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 181 took 0.000028 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Registering RDD 181 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got map stage job 43 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ShuffleMapStage 50 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ShuffleMapStage 50 (name=showString at NativeMethodAccessorImpl.java:0;jobs=43))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[181] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 50)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 50.3 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_77 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_77 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_77_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on macbookpro.lan:57375 (size: 23.2 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_77_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_77_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_77_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_77_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[181] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 50.0: 5\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 50.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_50.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 43) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 50.0 (TID 43)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (50, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_77\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_77 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 200;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 43 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@2f19e15f\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 43 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@5a61661d\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_76\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_76 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 43 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@5a61661d\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2660)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2660\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2660\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2528)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2528\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2528\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2564)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2564\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2564\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2500)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2500\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2500\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2507)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2507\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2507\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2594)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2594\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2594\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2695)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2695\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2695\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2545)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2545\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2545\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2577)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2577\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2577\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2728)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2728\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2728\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2882)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2882\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2882\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2650)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2650\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2650\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2710)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2710\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2710\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2575)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2575\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2575\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2590)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2590\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2590\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2615)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2615\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2615\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2639)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2639\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2639\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2837)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2837\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2837\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2488)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2488\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2488\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2642)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2642\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2642\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2515)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2515\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2515\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2721)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2721\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2721\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2576)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2576\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2576\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2686)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2686\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2686\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2666)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2666\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2666\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2471)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2471\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2471\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2663)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2663\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2663\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2501)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2501\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2501\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2476)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2476\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2476\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2829)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2829\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2829\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2706)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2706\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2706\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2475)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2475\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2475\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2560)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2560\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2560\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2505)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2505\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2505\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2873)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2873\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2873\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2508)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2508\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2508\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2694)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2694\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2694\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2479)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2479\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2479\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2569)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2569\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2569\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2493)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2493\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2493\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 43 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@2f19e15f\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2536)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2536\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2536\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2522)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2522\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2522\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2823)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2823\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2823\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2870)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2870\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2870\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2614)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2614\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2614\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2671)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2671\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2671\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2700)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2700\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2700\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2654)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2654\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2654\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2685)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2685\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2685\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2818)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2818\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2818\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2581)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2581\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2581\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2691)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2691\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2691\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2494)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2494\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2494\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2562)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2562\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2562\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2563)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2563\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2563\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2606)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2606\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2606\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2833)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2833\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2833\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2876)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2876\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2876\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2499)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2499\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2499\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2552)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2552\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2552\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2811)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2811\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2811\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2600)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2600\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2600\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2649)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2649\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2649\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2558)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2558\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2558\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2566)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2566\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2566\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2715)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2715\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2715\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2613)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2613\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2613\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2640)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2640\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2640\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2674)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2674\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2674\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2657)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2657\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2657\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2806)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2806\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2806\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2525)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2525\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2525\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2697)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2697\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2697\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2524)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2524\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2524\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2596)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2596\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2596\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2664)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2664\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2664\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2622)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2622\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2622\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2531)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2531\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2531\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2616)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2616\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2616\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2670)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2670\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2670\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2689)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2689\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2689\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2805)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2805\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2805\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2504)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2504\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2504\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2482)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2482\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2482\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2889)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2889\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2889\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2877)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2877\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2877\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2834)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2834\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2834\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2597)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2597\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2597\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2853)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2853\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2853\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2855)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2855\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2855\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2541)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2541\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2541\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2556)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2556\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2556\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2810)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2810\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2810\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(74)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 74\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 74\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 74\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 74\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_74\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_74 of size 15896 dropped from memory (free 378196315)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_74_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_74_piece0 of size 7815 dropped from memory (free 378204130)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_74_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_74_piece0 on macbookpro.lan:57375 in memory (size: 7.6 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_74_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_74_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 74, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 74\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2514)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2514\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2514\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2718)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2718\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2718\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2844)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2844\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2844\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2559)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2559\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2559\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2704)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2704\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2704\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2637)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2637\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2637\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2539)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2539\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2539\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2620)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2620\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2620\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2698)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2698\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2698\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2533)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2533\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2533\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2645)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2645\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2645\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2595)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2595\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2595\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2676)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2676\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2676\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2604)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2604\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2604\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2868)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2868\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2868\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2554)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2554\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2554\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2586)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2586\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2586\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2688)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2688\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2688\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2610)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2610\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2610\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2571)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2571\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2571\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2497)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2497\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2497\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2542)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2542\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2542\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2665)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2665\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2665\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(69)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 69\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 69\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 69\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 69\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_69_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_69_piece0 of size 35646 dropped from memory (free 378239776)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_69_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_69_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_69_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_69_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_69\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_69 of size 360328 dropped from memory (free 378600104)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 69, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 69\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2841)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2841\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2841\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2624)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2624\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2624\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2812)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2812\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2812\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2880)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2880\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2880\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2561)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2561\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2561\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2879)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2879\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2879\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2656)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2656\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2656\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2890)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2890\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2890\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2567)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2567\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2567\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2634)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2634\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2634\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2605)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2605\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2605\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(71)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 71\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 71\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 71\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 71\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_71_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_71_piece0 of size 35646 dropped from memory (free 378635750)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_71_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_71_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_71_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_71_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_71\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_71 of size 360328 dropped from memory (free 378996078)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 71, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 71\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2863)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2863\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2863\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2547)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2547\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2547\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2587)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2587\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2587\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2673)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2673\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2673\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2601)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2601\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2601\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2625)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2625\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2625\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2678)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2678\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2678\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2814)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2814\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2814\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2872)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2872\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2872\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2538)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2538\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2538\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(63)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 63\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 63\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 63\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 63\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_63\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_63 of size 360328 dropped from memory (free 379356406)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_63_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_63_piece0 of size 35646 dropped from memory (free 379392052)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_63_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_63_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_63_piece0\n",
      "25/04/11 09:59:13 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 43 with length 200\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_63_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 63, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 63\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2644)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2644\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2644\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2801)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2801\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2801\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2512)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2512\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2512\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2680)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2680\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2680\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2869)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2869\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2869\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2489)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2489\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2489\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2527)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2527\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2527\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2585)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2585\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2585\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2543)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2543\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2543\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2591)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2591\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2591\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2881)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2881\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2881\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2655)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2655\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2655\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2579)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2579\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2579\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2846)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2846\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2846\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2709)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2709\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2709\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2553)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2553\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2553\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2570)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2570\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2570\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(70)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 70\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 70\n",
      "25/04/11 09:59:13 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 43: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,76,0,0,0,0,0,0,0,0,0,83,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 70\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 70\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_70\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_70 of size 13792 dropped from memory (free 379405844)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_70_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_70_piece0 of size 6550 dropped from memory (free 379412394)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_70_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_70_piece0 on macbookpro.lan:57375 in memory (size: 6.4 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_70_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_70_piece0\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 50.0 (TID 43). 3552 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (50, 0) -> 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 70, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 43) in 42 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 70\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2643)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2643\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2643\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2893)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2893\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2893\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2498)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: ShuffleMapStage 50 (showString at NativeMethodAccessorImpl.java:0) finished in 0.045 s\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2498\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2498\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2690)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2690\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2690\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2828)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2828\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2828\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2573)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2573\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2573\n",
      "25/04/11 09:59:13 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2607)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2607\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2607\n",
      "25/04/11 09:59:13 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2472)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2472\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2472\n",
      "25/04/11 09:59:13 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2626)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2626\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2626\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Increasing epoch to 6\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2474)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2474\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2474\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2503)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2503\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2503\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2510)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2510\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2510\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2717)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2717\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2717\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2631)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2631\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2631\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2672)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2672\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2672\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2864)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2864\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2864\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2490)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2490\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2490\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2807)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2807\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2807\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2609)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2609\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2609\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2580)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2580\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2580\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2485)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2485\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2485\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2659)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2659\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2659\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2523)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2523\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2523\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2599)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2599\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2599\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 50, remaining stages = 0\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2565)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2565\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2565\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2568)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2568\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2568\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2668)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2668\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2668\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2816)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2816\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2816\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2874)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2874\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2874\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2633)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2633\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2633\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2598)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2598\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2598\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2617)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2617\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2617\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2520)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2520\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2520\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2669)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2669\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2669\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2540)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2540\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2540\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2723)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2723\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2723\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2518)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2518\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2518\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2883)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2883\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2883\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2487)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2487\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2487\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2632)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2632\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2632\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2646)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2646\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2646\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2532)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2532\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2532\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2486)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2486\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2486\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2602)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2602\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2602\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2628)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2628\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2628\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2662)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2662\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2662\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2629)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2629\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2629\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2682)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2682\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2682\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2521)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2521\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2521\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2808)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2808\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2808\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2627)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2627\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2627\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2850)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2850\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2850\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2894)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2894\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2894\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2707)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2707\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2707\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2683)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2683\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2683\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2648)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2648\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2648\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2803)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2803\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2803\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2658)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2658\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2658\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2675)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2675\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2675\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2848)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2848\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2848\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2835)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2835\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2835\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2481)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2481\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2481\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2730)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2730\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2730\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2849)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2849\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2849\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2884)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2884\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2884\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2584)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2584\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2584\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2483)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2483\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2483\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2827)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2827\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2827\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2851)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2851\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2851\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2506)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2506\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2506\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2574)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2574\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2574\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2826)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2826\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2826\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2480)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2480\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2480\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2722)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2722\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2722\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2815)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2815\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2815\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2804)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2804\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2804\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2653)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2653\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2653\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2549)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2549\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2549\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2692)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2692\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2692\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2824)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2824\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2824\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2724)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2724\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2724\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2546)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2546\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2546\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2817)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2817\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2817\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2593)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2593\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2593\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2578)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2578\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2578\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2729)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2729\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2729\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2703)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2703\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2703\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2537)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2537\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2537\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2679)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2679\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2679\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2517)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2517\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2517\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2699)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2699\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2699\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2516)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2516\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2516\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2720)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2720\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2720\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2714)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2714\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2714\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2603)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2603\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2603\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2641)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2641\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2641\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2708)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2708\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2708\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2696)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2696\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2696\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2875)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2875\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2875\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2612)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2612\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2612\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2572)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2572\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2572\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2897)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2897\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2897\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2727)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2727\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2727\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2887)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2887\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2887\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2836)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2836\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2836\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2667)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2667\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2667\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2588)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2588\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2588\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2896)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2896\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2896\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2820)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2820\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2820\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2608)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2608\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2608\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2550)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2550\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2550\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2830)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2830\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2830\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2726)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2726\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2726\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2519)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2519\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2519\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2725)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2725\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2725\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2495)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2495\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2495\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2635)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2635\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2635\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2713)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2713\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2713\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2809)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2809\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2809\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2866)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2866\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2866\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2878)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2878\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2878\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2623)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2623\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2623\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2544)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2544\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2544\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2847)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2847\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2847\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2677)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2677\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2677\n",
      "25/04/11 09:59:13 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "+- ShuffleQueryStage 1\n",
      "   +- Exchange hashpartitioning(Name#1120, 200), ENSURE_REQUIREMENTS, [plan_id=954]\n",
      "      +- *(2) HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "         +- *(2) Project [saleamount#1043, Name#1120]\n",
      "            +- *(2) BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "               :- *(2) Filter isnotnull(customerid#1039)\n",
      "               :  +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "               +- BroadcastQueryStage 0\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=899]\n",
      "                     +- *(1) Filter isnotnull(CustomerID#1119)\n",
      "                        +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2716)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2716\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2716\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2839)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2839\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2839\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2867)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2867\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2867\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2832)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2832\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2832\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2652)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2652\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2652\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2492)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2492\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2492\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2831)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2831\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2831\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2895)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2895\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2895\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2701)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2701\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2701\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2502)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2502\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2502\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2473)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2473\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2473\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2636)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2636\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2636\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2852)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2852\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2852\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2843)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2843\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2843\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2854)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2854\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2854\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2484)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2484\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2484\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(66)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 66\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 66\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 66\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 66\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_66_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_66_piece0 of size 13308 dropped from memory (free 379425702)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_66_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_66_piece0 on macbookpro.lan:57375 in memory (size: 13.0 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_66_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_66_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_66\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_66 of size 28920 dropped from memory (free 379454622)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 66, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 66\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2821)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2821\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2821\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2891)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2891\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2891\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2583)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2583\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2583\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2842)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2842\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2842\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2619)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2619\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2619\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2526)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2526\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2526\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2892)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2892\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2892\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2496)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2496\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2496\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2535)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2535\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2535\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2684)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2684\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2684\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2705)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2705\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2705\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2478)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2478\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2478\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(64)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 64\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 64\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 64\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 64\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_64\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_64 of size 13792 dropped from memory (free 379468414)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_64_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_64_piece0 of size 6553 dropped from memory (free 379474967)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_64_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_64_piece0 on macbookpro.lan:57375 in memory (size: 6.4 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_64_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_64_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 64, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 64\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2592)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2592\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2592\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2534)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2534\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2534\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2555)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2555\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2555\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2719)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2719\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2719\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2845)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2845\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2845\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2529)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2529\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2529\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2711)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2711\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2711\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(68)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 68\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 68\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 68\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 68\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_68_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_68_piece0 of size 8523 dropped from memory (free 379483490)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_68_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_68_piece0 on macbookpro.lan:57375 in memory (size: 8.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_68_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_68_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_68\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_68 of size 19296 dropped from memory (free 379502786)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 68, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 68\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2477)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2477\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2477\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2509)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2509\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2509\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2819)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2819\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2819\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2840)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2840\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2840\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2802)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2802\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2802\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2712)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2712\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2712\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2825)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2825\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2825\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2557)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2557\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2557\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(65)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 65\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 65\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 65\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 65\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_65_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_65_piece0 of size 35646 dropped from memory (free 379538432)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_65_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_65_piece0 on macbookpro.lan:57375 in memory (size: 34.8 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_65_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_65_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_65\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_65 of size 360328 dropped from memory (free 379898760)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 65, response is 0\n",
      "25/04/11 09:59:13 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 65\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2491)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2491\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2491\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2885)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2885\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2885\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2871)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2871\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2871\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2822)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2822\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2822\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2621)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2621\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2621\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2638)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2638\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2638\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2702)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2702\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2702\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2651)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2651\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2651\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2511)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2511\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2511\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(67)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 67\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 67\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 67\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 67\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_67_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_67_piece0 of size 35566 dropped from memory (free 379934326)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_67_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_67_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_67_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_67_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_67\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_67 of size 360224 dropped from memory (free 380294550)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 67, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 67\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2630)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2630\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2630\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2865)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2865\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2865\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2647)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2647\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2647\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(72)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning broadcast 72\n",
      "25/04/11 09:59:13 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 72\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: removing broadcast 72\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing broadcast 72\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_72\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_72 of size 28864 dropped from memory (free 380323414)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Removing block broadcast_72_piece0\n",
      "25/04/11 09:59:13 DEBUG MemoryStore: Block broadcast_72_piece0 of size 13287 dropped from memory (free 380336701)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_72_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Removed broadcast_72_piece0 on macbookpro.lan:57375 in memory (size: 13.0 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_72_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_72_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 72, response is 0\n",
      "25/04/11 09:59:13 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned broadcast 72\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2611)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2611\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2611\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2618)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2618\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2618\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2513)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2513\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2513\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2551)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2551\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2551\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2589)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2589\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2589\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2687)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2687\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2687\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2693)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2693\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2693\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2886)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2886\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2886\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2582)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2582\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2582\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2548)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2548\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2548\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2888)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2888\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2888\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2661)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2661\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2661\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2681)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2681\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2681\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2813)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2813\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2813\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2838)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2838\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2838\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2530)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2530\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaned accumulator 2530\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$takeOrdered$2$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$takeOrdered$2$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$takeOrdered$3\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$takeOrdered$3) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 185 took 0.000065 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 44 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 52 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 52 (name=showString at NativeMethodAccessorImpl.java:0;jobs=44))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[185] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 52)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 51.9 KiB, free 362.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_78 locally took 16 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_78 without replication took 16 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 362.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_78_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on macbookpro.lan:57375 (size: 24.1 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_78_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_78_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_78_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_78_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[185] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 52.0: 6\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 52.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_52.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 44) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 52.0 (TID 44)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (52, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_78\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_78 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 5\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 5, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_5_43_22_181,0)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local shuffle block shuffle_5_43_22_181\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 44 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@487be9cd\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 44 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@487be9cd\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 44 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@487be9cd\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 44 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@487be9cd\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 52.0 (TID 44). 6473 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (52, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 44) in 12 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 52 (showString at NativeMethodAccessorImpl.java:0) finished in 0.036 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 52, remaining stages = 1\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 51, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 44 finished: showString at NativeMethodAccessorImpl.java:0, took 0.041453 s\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for toprettystring(input[0, string, true], Some(America/Chicago)),toprettystring(input[1, double, true], Some(America/Chicago)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     UTF8String value_0;\n",
      "/* 035 */     if (isNull_1) {\n",
      "/* 036 */       value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 037 */     } else {\n",
      "/* 038 */       value_0 = value_1;\n",
      "/* 039 */     }\n",
      "/* 040 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 041 */\n",
      "/* 042 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 043 */     double value_3 = isNull_3 ?\n",
      "/* 044 */     -1.0 : (i.getDouble(1));\n",
      "/* 045 */     UTF8String value_2;\n",
      "/* 046 */     if (isNull_3) {\n",
      "/* 047 */       value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 048 */     } else {\n",
      "/* 049 */       value_2 = UTF8String.fromString(String.valueOf(value_3));\n",
      "/* 050 */     }\n",
      "/* 051 */     mutableStateArray_0[0].write(1, value_2);\n",
      "/* 052 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 053 */   }\n",
      "/* 054 */\n",
      "/* 055 */\n",
      "/* 056 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "TakeOrderedAndProject(limit=21, orderBy=[total_spent#1131 DESC NULLS LAST], output=[toprettystring(Name)#1137,toprettystring(total_spent)#1138])\n",
      "+- *(3) HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 1\n",
      "         +- Exchange hashpartitioning(Name#1120, 200), ENSURE_REQUIREMENTS, [plan_id=954]\n",
      "            +- *(2) HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "               +- *(2) Project [saleamount#1043, Name#1120]\n",
      "                  +- *(2) BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "                     :- *(2) Filter isnotnull(customerid#1039)\n",
      "                     :  +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                     +- BroadcastQueryStage 0\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=899]\n",
      "                           +- *(1) Filter isnotnull(CustomerID#1119)\n",
      "                              +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, StructField(toprettystring(Name),StringType,false), StructField(toprettystring(total_spent),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[2];\n",
      "/* 024 */\n",
      "/* 025 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 026 */     boolean isNull_1 = true;\n",
      "/* 027 */     java.lang.String value_1 = null;\n",
      "/* 028 */     isNull_1 = false;\n",
      "/* 029 */     if (!isNull_1) {\n",
      "/* 030 */\n",
      "/* 031 */       Object funcResult_0 = null;\n",
      "/* 032 */       funcResult_0 = value_2.toString();\n",
      "/* 033 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 034 */\n",
      "/* 035 */     }\n",
      "/* 036 */     if (isNull_1) {\n",
      "/* 037 */       values_0[0] = null;\n",
      "/* 038 */     } else {\n",
      "/* 039 */       values_0[0] = value_1;\n",
      "/* 040 */     }\n",
      "/* 041 */\n",
      "/* 042 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 043 */     boolean isNull_3 = true;\n",
      "/* 044 */     java.lang.String value_3 = null;\n",
      "/* 045 */     isNull_3 = false;\n",
      "/* 046 */     if (!isNull_3) {\n",
      "/* 047 */\n",
      "/* 048 */       Object funcResult_1 = null;\n",
      "/* 049 */       funcResult_1 = value_4.toString();\n",
      "/* 050 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 051 */\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_3) {\n",
      "/* 054 */       values_0[1] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[1] = value_3;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 060 */     if (false) {\n",
      "/* 061 */       mutableRow.setNullAt(0);\n",
      "/* 062 */     } else {\n",
      "/* 063 */\n",
      "/* 064 */       mutableRow.update(0, value_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     return mutableRow;\n",
      "/* 068 */   }\n",
      "/* 069 */\n",
      "/* 070 */\n",
      "/* 071 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#1039)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: Sort [total_spent#1131 DESC NULLS LAST], true, 0\n",
      "+- HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "   +- HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "      +- Project [saleamount#1043, Name#1120]\n",
      "         +- BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "            :- Project [customerid#1039, saleamount#1043]\n",
      "            :  +- Filter isnotnull(customerid#1039)\n",
      "            :     +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "            +- Project [CustomerID#1119, Name#1120]\n",
      "               +- Filter isnotnull(CustomerID#1119)\n",
      "                  +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BroadcastQueryStageExec: Materialize query stage BroadcastQueryStageExec: 0\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 034 */         if (!filter_value_2) continue;\n",
      "/* 035 */\n",
      "/* 036 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 037 */\n",
      "/* 038 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 039 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 040 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 041 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 042 */\n",
      "/* 043 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 044 */\n",
      "/* 045 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 046 */\n",
      "/* 047 */         if (inputadapter_isNull_1) {\n",
      "/* 048 */           filter_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */         } else {\n",
      "/* 050 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);\n",
      "/* 051 */         }\n",
      "/* 052 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 053 */\n",
      "/* 054 */       } while(false);\n",
      "/* 055 */       if (shouldStop()) return;\n",
      "/* 056 */     }\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 351.8 KiB, free 362.3 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_79 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_79 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 362.3 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_79_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_79_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_79_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_79_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_79_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 79 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 189 took 0.000042 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 45 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 53 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 53 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=45))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[189] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 53)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 15.5 KiB, free 362.3 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_80 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_80 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 362.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_80_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on macbookpro.lan:57375 (size: 7.6 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_80_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_80_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_80_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_80_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[189] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 53.0: 6\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 53.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_53.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 45) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9847 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 53.0 (TID 45)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (53, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_80\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_80 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/customers_data_prepared.csv, range: 0-523, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_79\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_79 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 53.0 (TID 45). 1862 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (53, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 45) in 8 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 53 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.010 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 53, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 45 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.012222 s\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 1024.3 KiB for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6f21158f\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for cast(input[0, int, false] as bigint):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     int value_1 = i.getInt(0);\n",
      "/* 032 */     boolean isNull_0 = false;\n",
      "/* 033 */     long value_0 = -1L;\n",
      "/* 034 */     if (!false) {\n",
      "/* 035 */       value_0 = (long) value_1;\n",
      "/* 036 */     }\n",
      "/* 037 */     mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */\n",
      "/* 042 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 512.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6f21158f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 release 256.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6f21158f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 acquired 88.0 B for org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6f21158f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 0 release 512.0 B from org.apache.spark.sql.execution.joins.LongToUnsafeRowMap@6f21158f\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 1024.1 KiB, free 361.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_81 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_81 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 509.0 B, free 361.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_81_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on macbookpro.lan:57375 (size: 509.0 B, free: 366.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_81_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_81_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_81_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_81_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 81 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: Considering join on: Some((customerid#1039 = CustomerID#1119))\n",
      "25/04/11 09:59:13 DEBUG ExtractEquiJoinKeys: leftKeys:List(customerid#1039) | rightKeys:List(CustomerID#1119)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(customerid)\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(customerid#1039)\n",
      "25/04/11 09:59:13 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_81\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_81 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 020 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];\n",
      "/* 022 */\n",
      "/* 023 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 024 */     this.references = references;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 028 */     partitionIndex = index;\n",
      "/* 029 */     this.inputs = inputs;\n",
      "/* 030 */\n",
      "/* 031 */     inputadapter_input_0 = inputs[0];\n",
      "/* 032 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 033 */\n",
      "/* 034 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[8] /* broadcast */).value()).asReadOnlyCopy();\n",
      "/* 035 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());\n",
      "/* 036 */\n",
      "/* 037 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);\n",
      "/* 038 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 039 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 040 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 041 */     filter_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 042 */\n",
      "/* 043 */   }\n",
      "/* 044 */\n",
      "/* 045 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 046 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 047 */     private int[] buckets;\n",
      "/* 048 */     private int capacity = 1 << 16;\n",
      "/* 049 */     private double loadFactor = 0.5;\n",
      "/* 050 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 051 */     private int maxSteps = 2;\n",
      "/* 052 */     private int numRows = 0;\n",
      "/* 053 */     private Object emptyVBase;\n",
      "/* 054 */     private long emptyVOff;\n",
      "/* 055 */     private int emptyVLen;\n",
      "/* 056 */     private boolean isBatchFull = false;\n",
      "/* 057 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 058 */\n",
      "/* 059 */     public hashAgg_FastHashMap_0(\n",
      "/* 060 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 061 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 062 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 063 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 064 */\n",
      "/* 065 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 066 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 067 */\n",
      "/* 068 */       emptyVBase = emptyBuffer;\n",
      "/* 069 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 070 */       emptyVLen = emptyBuffer.length;\n",
      "/* 071 */\n",
      "/* 072 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 073 */         1, 32);\n",
      "/* 074 */\n",
      "/* 075 */       buckets = new int[numBuckets];\n",
      "/* 076 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0) {\n",
      "/* 080 */       long h = hash(hashAgg_key_0);\n",
      "/* 081 */       int step = 0;\n",
      "/* 082 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 083 */       while (step < maxSteps) {\n",
      "/* 084 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 085 */         if (buckets[idx] == -1) {\n",
      "/* 086 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 087 */             agg_rowWriter.reset();\n",
      "/* 088 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 089 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 090 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 091 */             = agg_rowWriter.getRow();\n",
      "/* 092 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 093 */             long koff = agg_result.getBaseOffset();\n",
      "/* 094 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 095 */\n",
      "/* 096 */             UnsafeRow vRow\n",
      "/* 097 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 098 */             if (vRow == null) {\n",
      "/* 099 */               isBatchFull = true;\n",
      "/* 100 */             } else {\n",
      "/* 101 */               buckets[idx] = numRows++;\n",
      "/* 102 */             }\n",
      "/* 103 */             return vRow;\n",
      "/* 104 */           } else {\n",
      "/* 105 */             // No more space\n",
      "/* 106 */             return null;\n",
      "/* 107 */           }\n",
      "/* 108 */         } else if (equals(idx, hashAgg_key_0)) {\n",
      "/* 109 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 110 */         }\n",
      "/* 111 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 112 */         step++;\n",
      "/* 113 */       }\n",
      "/* 114 */       // Didn't find it\n",
      "/* 115 */       return null;\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */     private boolean equals(int idx, UTF8String hashAgg_key_0) {\n",
      "/* 119 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 120 */       return (row.getUTF8String(0).equals(hashAgg_key_0));\n",
      "/* 121 */     }\n",
      "/* 122 */\n",
      "/* 123 */     private long hash(UTF8String hashAgg_key_0) {\n",
      "/* 124 */       long hashAgg_hash_0 = 0;\n",
      "/* 125 */\n",
      "/* 126 */       int hashAgg_result_0 = 0;\n",
      "/* 127 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 128 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 129 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 130 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 134 */\n",
      "/* 135 */       return hashAgg_hash_0;\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 139 */       return batch.rowIterator();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */     public void close() {\n",
      "/* 143 */       batch.close();\n",
      "/* 144 */     }\n",
      "/* 145 */\n",
      "/* 146 */   }\n",
      "/* 147 */\n",
      "/* 148 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 149 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 150 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 151 */\n",
      "/* 152 */       do {\n",
      "/* 153 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 154 */         int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 155 */         -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 156 */\n",
      "/* 157 */         boolean filter_value_2 = !inputadapter_isNull_0;\n",
      "/* 158 */         if (!filter_value_2) continue;\n",
      "/* 159 */\n",
      "/* 160 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 161 */\n",
      "/* 162 */         // generate join key for stream side\n",
      "/* 163 */         boolean bhj_isNull_0 = false;\n",
      "/* 164 */         long bhj_value_0 = -1L;\n",
      "/* 165 */         if (!false) {\n",
      "/* 166 */           bhj_value_0 = (long) inputadapter_value_0;\n",
      "/* 167 */         }\n",
      "/* 168 */         // find matches from HashedRelation\n",
      "/* 169 */         UnsafeRow bhj_buildRow_0 = bhj_isNull_0 ? null: (UnsafeRow)bhj_relation_0.getValue(bhj_value_0);\n",
      "/* 170 */         if (bhj_buildRow_0 != null) {\n",
      "/* 171 */           {\n",
      "/* 172 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 173 */\n",
      "/* 174 */             // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */             boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */             double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */             -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */             boolean bhj_isNull_3 = bhj_buildRow_0.isNullAt(1);\n",
      "/* 180 */             UTF8String bhj_value_3 = bhj_isNull_3 ?\n",
      "/* 181 */             null : (bhj_buildRow_0.getUTF8String(1));\n",
      "/* 182 */\n",
      "/* 183 */             hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, bhj_value_3, bhj_isNull_3);\n",
      "/* 184 */\n",
      "/* 185 */           }\n",
      "/* 186 */         }\n",
      "/* 187 */\n",
      "/* 188 */       } while(false);\n",
      "/* 189 */       // shouldStop check is eliminated\n",
      "/* 190 */     }\n",
      "/* 191 */\n",
      "/* 192 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 193 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 194 */\n",
      "/* 195 */   }\n",
      "/* 196 */\n",
      "/* 197 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, UTF8String hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 198 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 199 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 200 */\n",
      "/* 201 */     if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 202 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 203 */         hashAgg_expr_1_0);\n",
      "/* 204 */     }\n",
      "/* 205 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 206 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 207 */       // generate grouping key\n",
      "/* 208 */       filter_mutableStateArray_0[4].reset();\n",
      "/* 209 */\n",
      "/* 210 */       filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 211 */\n",
      "/* 212 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 213 */         filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 214 */       } else {\n",
      "/* 215 */         filter_mutableStateArray_0[4].write(0, hashAgg_expr_1_0);\n",
      "/* 216 */       }\n",
      "/* 217 */       int hashAgg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[4].getRow()).hashCode();\n",
      "/* 218 */       if (true) {\n",
      "/* 219 */         // try to get the buffer from hash map\n",
      "/* 220 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 221 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 222 */       }\n",
      "/* 223 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 224 */       // aggregation after processing all input rows.\n",
      "/* 225 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 226 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 227 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 228 */         } else {\n",
      "/* 229 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 230 */         }\n",
      "/* 231 */\n",
      "/* 232 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 233 */         // try to allocate buffer again.\n",
      "/* 234 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 235 */           (filter_mutableStateArray_0[4].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 236 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 237 */           // failed to allocate the first page\n",
      "/* 238 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 239 */         }\n",
      "/* 240 */       }\n",
      "/* 241 */\n",
      "/* 242 */     }\n",
      "/* 243 */\n",
      "/* 244 */     // Updates the proper row buffer\n",
      "/* 245 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 246 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 247 */     }\n",
      "/* 248 */\n",
      "/* 249 */     // common sub-expressions\n",
      "/* 250 */\n",
      "/* 251 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 252 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 253 */\n",
      "/* 254 */   }\n",
      "/* 255 */\n",
      "/* 256 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 257 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 258 */     double hashAgg_value_6 = -1.0;\n",
      "/* 259 */     do {\n",
      "/* 260 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 261 */       double hashAgg_value_7 = -1.0;\n",
      "/* 262 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 263 */       double hashAgg_value_8 = -1.0;\n",
      "/* 264 */       do {\n",
      "/* 265 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 266 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 267 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 268 */         if (!hashAgg_isNull_8) {\n",
      "/* 269 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 270 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 271 */           continue;\n",
      "/* 272 */         }\n",
      "/* 273 */\n",
      "/* 274 */         if (!false) {\n",
      "/* 275 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 276 */           hashAgg_value_8 = 0.0D;\n",
      "/* 277 */           continue;\n",
      "/* 278 */         }\n",
      "/* 279 */\n",
      "/* 280 */       } while (false);\n",
      "/* 281 */\n",
      "/* 282 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 283 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 284 */\n",
      "/* 285 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_0_0;\n",
      "/* 286 */\n",
      "/* 287 */       }\n",
      "/* 288 */       if (!hashAgg_isNull_6) {\n",
      "/* 289 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 290 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 291 */         continue;\n",
      "/* 292 */       }\n",
      "/* 293 */\n",
      "/* 294 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 295 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 296 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 297 */       if (!hashAgg_isNull_11) {\n",
      "/* 298 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 299 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 300 */         continue;\n",
      "/* 301 */       }\n",
      "/* 302 */\n",
      "/* 303 */     } while (false);\n",
      "/* 304 */\n",
      "/* 305 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 306 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 309 */     }\n",
      "/* 310 */   }\n",
      "/* 311 */\n",
      "/* 312 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 313 */   throws java.io.IOException {\n",
      "/* 314 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 315 */\n",
      "/* 316 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 317 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 318 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 319 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 320 */     double hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 321 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 322 */\n",
      "/* 323 */     filter_mutableStateArray_0[5].reset();\n",
      "/* 324 */\n",
      "/* 325 */     filter_mutableStateArray_0[5].zeroOutNullBytes();\n",
      "/* 326 */\n",
      "/* 327 */     if (hashAgg_isNull_12) {\n",
      "/* 328 */       filter_mutableStateArray_0[5].setNullAt(0);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       filter_mutableStateArray_0[5].write(0, hashAgg_value_13);\n",
      "/* 331 */     }\n",
      "/* 332 */\n",
      "/* 333 */     if (hashAgg_isNull_13) {\n",
      "/* 334 */       filter_mutableStateArray_0[5].setNullAt(1);\n",
      "/* 335 */     } else {\n",
      "/* 336 */       filter_mutableStateArray_0[5].write(1, hashAgg_value_14);\n",
      "/* 337 */     }\n",
      "/* 338 */     append((filter_mutableStateArray_0[5].getRow()));\n",
      "/* 339 */\n",
      "/* 340 */   }\n",
      "/* 341 */\n",
      "/* 342 */   protected void processNext() throws java.io.IOException {\n",
      "/* 343 */     if (!hashAgg_initAgg_0) {\n",
      "/* 344 */       hashAgg_initAgg_0 = true;\n",
      "/* 345 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 346 */\n",
      "/* 347 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 348 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 349 */           @Override\n",
      "/* 350 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 351 */             hashAgg_fastHashMap_0.close();\n",
      "/* 352 */           }\n",
      "/* 353 */         });\n",
      "/* 354 */\n",
      "/* 355 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 356 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 357 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 358 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 359 */     }\n",
      "/* 360 */     // output the result\n",
      "/* 361 */\n",
      "/* 362 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 363 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 364 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 365 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 366 */\n",
      "/* 367 */       if (shouldStop()) return;\n",
      "/* 368 */     }\n",
      "/* 369 */     hashAgg_fastHashMap_0.close();\n",
      "/* 370 */\n",
      "/* 371 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 372 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 373 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 374 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 375 */       if (shouldStop()) return;\n",
      "/* 376 */     }\n",
      "/* 377 */     hashAgg_mapIter_0.close();\n",
      "/* 378 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 379 */       hashAgg_hashMap_0.free();\n",
      "/* 380 */     }\n",
      "/* 381 */   }\n",
      "/* 382 */\n",
      "/* 383 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 351.8 KiB, free 360.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_82 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_82 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 360.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_82_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_82_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_82_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_82_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_82_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 82 from toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 193 took 0.000029 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Registering RDD 193 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) as input to shuffle 6\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got map stage job 46 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ShuffleMapStage 54 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ShuffleMapStage 54 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=46))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[193] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 54)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 50.5 KiB, free 360.8 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_83 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_83 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 360.8 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_83_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on macbookpro.lan:57375 (size: 23.3 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_83_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_83_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_83_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_83_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[193] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 54.0: 6\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 54.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_54.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 46) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 54.0 (TID 46)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (54, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_83\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_83 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */\n",
      "/* 044 */       int remainder_0 = value_1 % 200;\n",
      "/* 045 */       if (remainder_0 < 0) {\n",
      "/* 046 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 047 */       } else {\n",
      "/* 048 */         value_0=remainder_0;\n",
      "/* 049 */       }\n",
      "/* 050 */\n",
      "/* 051 */     }\n",
      "/* 052 */     if (isNull_0) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 46 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@394f2ace\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 46 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@604a2005\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_82\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_82 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GeneratePredicate: Generated predicate 'isnotnull(input[0, int, true])':\n",
      "/* 001 */ public SpecificPredicate generate(Object[] references) {\n",
      "/* 002 */   return new SpecificPredicate(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {\n",
      "/* 006 */   private final Object[] references;\n",
      "/* 007 */\n",
      "/* 008 */\n",
      "/* 009 */   public SpecificPredicate(Object[] references) {\n",
      "/* 010 */     this.references = references;\n",
      "/* 011 */\n",
      "/* 012 */   }\n",
      "/* 013 */\n",
      "/* 014 */   public void initialize(int partitionIndex) {\n",
      "/* 015 */\n",
      "/* 016 */   }\n",
      "/* 017 */\n",
      "/* 018 */   public boolean eval(InternalRow i) {\n",
      "/* 019 */\n",
      "/* 020 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 021 */     int value_1 = isNull_1 ?\n",
      "/* 022 */     -1 : (i.getInt(0));\n",
      "/* 023 */     boolean value_2 = !isNull_1;\n",
      "/* 024 */     return !false && value_2;\n",
      "/* 025 */   }\n",
      "/* 026 */\n",
      "/* 027 */\n",
      "/* 028 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 46 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@604a2005\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 46 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@394f2ace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            Name|       total_spent|\n",
      "+----------------+------------------+\n",
      "|   William White|23752.520000000004|\n",
      "|Hermione Granger|          22822.54|\n",
      "|   Susan Johnson|           12422.6|\n",
      "|       Chewbacca|11813.439999999999|\n",
      "|   Tiffany James|          11715.82|\n",
      "| Hermione Grager|           8750.94|\n",
      "|    Wylie Coyote|           7434.44|\n",
      "|          Dr Who|4064.8599999999997|\n",
      "|       Dan Brown|2427.2999999999997|\n",
      "|    Jason Bourne|           1806.34|\n",
      "|      Tony Stark|           1545.54|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:13 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 46 with length 200\n",
      "25/04/11 09:59:13 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 46: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,84,0,0,0,0,0,0,76,0,0,0,0,0,0,0,0,0,83,84,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 54.0 (TID 46). 3509 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (54, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 46) in 33 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:59:13 INFO DAGScheduler: ShuffleMapStage 54 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.037 s\n",
      "25/04/11 09:59:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:59:13 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Increasing epoch to 7\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 54, remaining stages = 0\n",
      "25/04/11 09:59:13 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "+- ShuffleQueryStage 1\n",
      "   +- Exchange hashpartitioning(Name#1120, 200), ENSURE_REQUIREMENTS, [plan_id=1123]\n",
      "      +- *(2) HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "         +- *(2) Project [saleamount#1043, Name#1120]\n",
      "            +- *(2) BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "               :- *(2) Filter isnotnull(customerid#1039)\n",
      "               :  +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "               +- BroadcastQueryStage 0\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1060]\n",
      "                     +- *(1) Filter isnotnull(CustomerID#1119)\n",
      "                        +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:59:13 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 2\n",
      "25/04/11 09:59:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=3\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_2_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 042 */\n",
      "/* 043 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1);\n",
      "/* 044 */       // shouldStop check is eliminated\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 048 */   }\n",
      "/* 049 */\n",
      "/* 050 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0) throws java.io.IOException {\n",
      "/* 051 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 052 */\n",
      "/* 053 */     // generate grouping key\n",
      "/* 054 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 055 */\n",
      "/* 056 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 057 */\n",
      "/* 058 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 059 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 064 */     if (true) {\n",
      "/* 065 */       // try to get the buffer from hash map\n",
      "/* 066 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 067 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 070 */     // aggregation after processing all input rows.\n",
      "/* 071 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 072 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 073 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 074 */       } else {\n",
      "/* 075 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 076 */       }\n",
      "/* 077 */\n",
      "/* 078 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 079 */       // try to allocate buffer again.\n",
      "/* 080 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 081 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 082 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 083 */         // failed to allocate the first page\n",
      "/* 084 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 085 */       }\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     // common sub-expressions\n",
      "/* 089 */\n",
      "/* 090 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 091 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 092 */\n",
      "/* 093 */   }\n",
      "/* 094 */\n",
      "/* 095 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 096 */     hashAgg_hashAgg_isNull_2_0 = true;\n",
      "/* 097 */     double hashAgg_value_2 = -1.0;\n",
      "/* 098 */     do {\n",
      "/* 099 */       boolean hashAgg_isNull_3 = true;\n",
      "/* 100 */       double hashAgg_value_3 = -1.0;\n",
      "/* 101 */       hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 102 */       double hashAgg_value_4 = -1.0;\n",
      "/* 103 */       do {\n",
      "/* 104 */         boolean hashAgg_isNull_5 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 105 */         double hashAgg_value_5 = hashAgg_isNull_5 ?\n",
      "/* 106 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 107 */         if (!hashAgg_isNull_5) {\n",
      "/* 108 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 109 */           hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 110 */           continue;\n",
      "/* 111 */         }\n",
      "/* 112 */\n",
      "/* 113 */         if (!false) {\n",
      "/* 114 */           hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 115 */           hashAgg_value_4 = 0.0D;\n",
      "/* 116 */           continue;\n",
      "/* 117 */         }\n",
      "/* 118 */\n",
      "/* 119 */       } while (false);\n",
      "/* 120 */\n",
      "/* 121 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 122 */         hashAgg_isNull_3 = false; // resultCode could change nullability.\n",
      "/* 123 */\n",
      "/* 124 */         hashAgg_value_3 = hashAgg_value_4 + hashAgg_expr_1_0;\n",
      "/* 125 */\n",
      "/* 126 */       }\n",
      "/* 127 */       if (!hashAgg_isNull_3) {\n",
      "/* 128 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 129 */         hashAgg_value_2 = hashAgg_value_3;\n",
      "/* 130 */         continue;\n",
      "/* 131 */       }\n",
      "/* 132 */\n",
      "/* 133 */       boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 134 */       double hashAgg_value_8 = hashAgg_isNull_8 ?\n",
      "/* 135 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 136 */       if (!hashAgg_isNull_8) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_2_0 = false;\n",
      "/* 138 */         hashAgg_value_2 = hashAgg_value_8;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */     } while (false);\n",
      "/* 143 */\n",
      "/* 144 */     if (!hashAgg_hashAgg_isNull_2_0) {\n",
      "/* 145 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 148 */     }\n",
      "/* 149 */   }\n",
      "/* 150 */\n",
      "/* 151 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 152 */   throws java.io.IOException {\n",
      "/* 153 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 154 */\n",
      "/* 155 */     boolean hashAgg_isNull_9 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 156 */     UTF8String hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 157 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 158 */     boolean hashAgg_isNull_10 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 159 */     double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 160 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 161 */\n",
      "/* 162 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 163 */\n",
      "/* 164 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 165 */\n",
      "/* 166 */     if (hashAgg_isNull_9) {\n",
      "/* 167 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 168 */     } else {\n",
      "/* 169 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_9);\n",
      "/* 170 */     }\n",
      "/* 171 */\n",
      "/* 172 */     if (hashAgg_isNull_10) {\n",
      "/* 173 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 174 */     } else {\n",
      "/* 175 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_10);\n",
      "/* 176 */     }\n",
      "/* 177 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 178 */\n",
      "/* 179 */   }\n",
      "/* 180 */\n",
      "/* 181 */   protected void processNext() throws java.io.IOException {\n",
      "/* 182 */     if (!hashAgg_initAgg_0) {\n",
      "/* 183 */       hashAgg_initAgg_0 = true;\n",
      "/* 184 */\n",
      "/* 185 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 186 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 187 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 188 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 189 */     }\n",
      "/* 190 */     // output the result\n",
      "/* 191 */\n",
      "/* 192 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 193 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 194 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 195 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 196 */       if (shouldStop()) return;\n",
      "/* 197 */     }\n",
      "/* 198 */     hashAgg_mapIter_0.close();\n",
      "/* 199 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 200 */       hashAgg_hashMap_0.free();\n",
      "/* 201 */     }\n",
      "/* 202 */   }\n",
      "/* 203 */\n",
      "/* 204 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG GenerateOrdering: Generated Ordering by input[0, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rangeBounds$1\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rangeBounds$1) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$sketch$1$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$sketch$1$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 198 took 0.000039 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 47 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 56 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 56 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=47))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[198] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 56)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 52.3 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_84 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_84 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_84_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on macbookpro.lan:57375 (size: 24.2 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_84_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_84_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_84_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_84_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[198] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 56.0: 7\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 56.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_56.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 47) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 56.0 (TID 47)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (56, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_84\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_84 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 6\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 6, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_6_46_22_181,0)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local shuffle block shuffle_6_46_22_181\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(1);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 47 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@7430d3f\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 47 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@7430d3f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 47 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@7430d3f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 47 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@7430d3f\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 56.0 (TID 47). 6373 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (56, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 47) in 12 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 56 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.016 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 56, remaining stages = 1\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 55, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 47 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.017378 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 199 took 0.000035 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Registering RDD 199 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) as input to shuffle 7\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got map stage job 48 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ShuffleMapStage 58 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ShuffleMapStage 58 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=48))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ShuffleMapStage 58 (MapPartitionsRDD[199] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 58)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 52.6 KiB, free 360.7 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_85 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_85 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 24.3 KiB, free 360.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_85_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on macbookpro.lan:57375 (size: 24.3 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_85_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_85_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_85_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_85_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[199] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 58.0: 7\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 58.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_58.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 48) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9173 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 58.0 (TID 48)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (58, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_85\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_85 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateOrdering: Generated Ordering by input[0, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(0);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(0));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(0);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(0));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 6\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 6, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_6_46_22_181,0)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local shuffle block shuffle_6_46_22_181\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(1);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 48 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@1abe35d6\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 48 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@1abe35d6\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 48 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@1abe35d6\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 48 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@1abe35d6\n",
      "25/04/11 09:59:13 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 48 with length 11\n",
      "25/04/11 09:59:13 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 48: [84,84,84,84,84,84,84,76,83,84,84]\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 58.0 (TID 48). 6056 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (58, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 48) in 24 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:59:13 INFO DAGScheduler: ShuffleMapStage 58 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.029 s\n",
      "25/04/11 09:59:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:59:13 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Increasing epoch to 8\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 58, remaining stages = 1\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 57, remaining stages = 0\n",
      "25/04/11 09:59:13 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=432.0 B, rowCount=11) for plan: ShuffleQueryStage 2\n",
      "+- Exchange rangepartitioning(total_spent#1131 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1157]\n",
      "   +- *(3) HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "      +- AQEShuffleRead coalesced\n",
      "         +- ShuffleQueryStage 1\n",
      "            +- Exchange hashpartitioning(Name#1120, 200), ENSURE_REQUIREMENTS, [plan_id=1123]\n",
      "               +- *(2) HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "                  +- *(2) Project [saleamount#1043, Name#1120]\n",
      "                     +- *(2) BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "                        :- *(2) Filter isnotnull(customerid#1039)\n",
      "                        :  +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                        +- BroadcastQueryStage 0\n",
      "                           +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1060]\n",
      "                              +- *(1) Filter isnotnull(CustomerID#1119)\n",
      "                                 +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n",
      "25/04/11 09:59:13 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=4\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean sort_needToSort_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n",
      "/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n",
      "/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     sort_needToSort_0 = true;\n",
      "/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n",
      "/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n",
      "/* 031 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 033 */\n",
      "/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n",
      "/* 035 */       // shouldStop check is eliminated\n",
      "/* 036 */     }\n",
      "/* 037 */\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */   protected void processNext() throws java.io.IOException {\n",
      "/* 041 */     if (sort_needToSort_0) {\n",
      "/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n",
      "/* 043 */       sort_addToSorter_0();\n",
      "/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n",
      "/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n",
      "/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n",
      "/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n",
      "/* 049 */       sort_needToSort_0 = false;\n",
      "/* 050 */     }\n",
      "/* 051 */\n",
      "/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n",
      "/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n",
      "/* 054 */\n",
      "/* 055 */       append(sort_outputRow_0);\n",
      "/* 056 */\n",
      "/* 057 */       if (shouldStop()) return;\n",
      "/* 058 */     }\n",
      "/* 059 */   }\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 202 took 0.000053 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 49 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 61 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 60)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 61 (name=toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24;jobs=49))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[202] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 61)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 49.1 KiB, free 360.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_86 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_86 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 360.6 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_86_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on macbookpro.lan:57375 (size: 23.2 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_86_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_86_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_86_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_86_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[202] at toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 61.0: 8\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 61.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_61.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 49) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 61.0 (TID 49)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (61, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_86\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_86 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 7\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 7, mappers 0-1, partitions 0-11\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Getting 1 (960.0 B) non-empty blocks including 1 (960.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_7_48_0_11,0)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local shuffle block shuffle_7_48_0_11\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:13 DEBUG GenerateOrdering: Generated Ordering by input[1, double, true] DESC NULLS LAST:\n",
      "/* 001 */ public SpecificOrdering generate(Object[] references) {\n",
      "/* 002 */   return new SpecificOrdering(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificOrdering(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */\n",
      "/* 013 */   }\n",
      "/* 014 */\n",
      "/* 015 */   public int compare(InternalRow a, InternalRow b) {\n",
      "/* 016 */\n",
      "/* 017 */     boolean isNull_0 = a.isNullAt(1);\n",
      "/* 018 */     double value_0 = isNull_0 ?\n",
      "/* 019 */     -1.0 : (a.getDouble(1));\n",
      "/* 020 */     boolean isNull_1 = b.isNullAt(1);\n",
      "/* 021 */     double value_1 = isNull_1 ?\n",
      "/* 022 */     -1.0 : (b.getDouble(1));\n",
      "/* 023 */     if (isNull_0 && isNull_1) {\n",
      "/* 024 */       // Nothing\n",
      "/* 025 */     } else if (isNull_0) {\n",
      "/* 026 */       return 1;\n",
      "/* 027 */     } else if (isNull_1) {\n",
      "/* 028 */       return -1;\n",
      "/* 029 */     } else {\n",
      "/* 030 */       int comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(value_0, value_1);\n",
      "/* 031 */       if (comp != 0) {\n",
      "/* 032 */         return -comp;\n",
      "/* 033 */       }\n",
      "/* 034 */     }\n",
      "/* 035 */\n",
      "/* 036 */     return 0;\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */\n",
      "/* 040 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for sortprefix(input[1, double, true] DESC NULLS LAST):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 032 */     double value_1 = isNull_1 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(1));\n",
      "/* 034 */     long value_0 = 0L;\n",
      "/* 035 */     boolean isNull_0 = isNull_1;\n",
      "/* 036 */     if (!isNull_1) {\n",
      "/* 037 */       value_0 = org.apache.spark.util.collection.unsafe.sort.PrefixComparators$DoublePrefixComparator.computePrefix((double)value_1);\n",
      "/* 038 */     }\n",
      "/* 039 */     if (isNull_0) {\n",
      "/* 040 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 041 */     } else {\n",
      "/* 042 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 045 */   }\n",
      "/* 046 */\n",
      "/* 047 */\n",
      "/* 048 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 49 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@49338321\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 49 acquired 1024.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@49338321\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 49 release 1024.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@49338321\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 49 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@49338321\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 61.0 (TID 49). 7680 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (61, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 49) in 10 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 61 (toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24) finished in 0.012 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 59, remaining stages = 2\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 61, remaining stages = 1\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 60, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 49 finished: toPandas at /var/folders/vt/1glhq4pd72s2pq4r1wl_lx840000gn/T/ipykernel_7889/514782847.py:24, took 0.014252 s\n",
      "25/04/11 09:59:13 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(4) Sort [total_spent#1131 DESC NULLS LAST], true, 0\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 2\n",
      "      +- Exchange rangepartitioning(total_spent#1131 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1157]\n",
      "         +- *(3) HashAggregate(keys=[Name#1120], functions=[sum(saleamount#1043)], output=[Name#1120, total_spent#1131])\n",
      "            +- AQEShuffleRead coalesced\n",
      "               +- ShuffleQueryStage 1\n",
      "                  +- Exchange hashpartitioning(Name#1120, 200), ENSURE_REQUIREMENTS, [plan_id=1123]\n",
      "                     +- *(2) HashAggregate(keys=[Name#1120], functions=[partial_sum(saleamount#1043)], output=[Name#1120, sum#1142])\n",
      "                        +- *(2) Project [saleamount#1043, Name#1120]\n",
      "                           +- *(2) BroadcastHashJoin [customerid#1039], [CustomerID#1119], Inner, BuildRight, false\n",
      "                              :- *(2) Filter isnotnull(customerid#1039)\n",
      "                              :  +- FileScan csv [customerid#1039,saleamount#1043] Batched: false, DataFilters: [isnotnull(customerid#1039)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(customerid)], ReadSchema: struct<customerid:int,saleamount:double>\n",
      "                              +- BroadcastQueryStage 0\n",
      "                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1060]\n",
      "                                    +- *(1) Filter isnotnull(CustomerID#1119)\n",
      "                                       +- FileScan csv [CustomerID#1119,Name#1120] Batched: false, DataFilters: [isnotnull(CustomerID#1119)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [IsNotNull(CustomerID)], ReadSchema: struct<CustomerID:int,Name:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"SmartSales\").getOrCreate()\n",
    "\n",
    "# Register DataFrames as temporary views (if not already done)\n",
    "df_sales.createOrReplaceTempView(\"sales\")\n",
    "df_customer.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "# Write query using Spark SQL\n",
    "df_top_customers = spark.sql(\"\"\"\n",
    "    SELECT c.Name, SUM(s.saleamount) AS total_spent\n",
    "    FROM sales s\n",
    "    JOIN customer c ON s.customerid = c.CustomerID\n",
    "    GROUP BY c.Name\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show Spark results\n",
    "df_top_customers.show()\n",
    "\n",
    "# Convert to Pandas for use with charts\n",
    "import pandas as pd\n",
    "df_top_customers_pd = df_top_customers.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0819cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:13 DEBUG Analyzer$ResolveReferences: Resolving 'saledate to saledate#1038\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'billtype to billtype#1045\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'storeid to storeid#1041\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'billtype to billtype#1045\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'storeid to storeid#1041\n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:59:13 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: CollectLimit 21\n",
      "+- HashAggregate(keys=[billtype#1045, storeid#1041], functions=[sum(saleamount#1043)], output=[toprettystring(billtype)#1189, toprettystring(storeid)#1190, toprettystring(sum(saleamount))#1191])\n",
      "   +- HashAggregate(keys=[billtype#1045, storeid#1041], functions=[partial_sum(saleamount#1043)], output=[billtype#1045, storeid#1041, sum#1196])\n",
      "      +- FileScan csv [storeid#1041,saleamount#1043,billtype#1045] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_5_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_7_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 032 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 033 */\n",
      "/* 034 */   }\n",
      "/* 035 */\n",
      "/* 036 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 037 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 038 */     private int[] buckets;\n",
      "/* 039 */     private int capacity = 1 << 16;\n",
      "/* 040 */     private double loadFactor = 0.5;\n",
      "/* 041 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 042 */     private int maxSteps = 2;\n",
      "/* 043 */     private int numRows = 0;\n",
      "/* 044 */     private Object emptyVBase;\n",
      "/* 045 */     private long emptyVOff;\n",
      "/* 046 */     private int emptyVLen;\n",
      "/* 047 */     private boolean isBatchFull = false;\n",
      "/* 048 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 049 */\n",
      "/* 050 */     public hashAgg_FastHashMap_0(\n",
      "/* 051 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 052 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 053 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 054 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 055 */\n",
      "/* 056 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 057 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 058 */\n",
      "/* 059 */       emptyVBase = emptyBuffer;\n",
      "/* 060 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 061 */       emptyVLen = emptyBuffer.length;\n",
      "/* 062 */\n",
      "/* 063 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 064 */         2, 32);\n",
      "/* 065 */\n",
      "/* 066 */       buckets = new int[numBuckets];\n",
      "/* 067 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 071 */       long h = hash(hashAgg_key_0, hashAgg_key_1);\n",
      "/* 072 */       int step = 0;\n",
      "/* 073 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 074 */       while (step < maxSteps) {\n",
      "/* 075 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 076 */         if (buckets[idx] == -1) {\n",
      "/* 077 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 078 */             agg_rowWriter.reset();\n",
      "/* 079 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 080 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 081 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(hashAgg_key_0)) && (row.getInt(1) == hashAgg_key_1);\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String hashAgg_key_0, int hashAgg_key_1) {\n",
      "/* 116 */       long hashAgg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int hashAgg_result_0 = 0;\n",
      "/* 119 */       byte[] hashAgg_bytes_0 = hashAgg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < hashAgg_bytes_0.length; i++) {\n",
      "/* 121 */         int hashAgg_hash_1 = hashAgg_bytes_0[i];\n",
      "/* 122 */         hashAgg_result_0 = (hashAgg_result_0 ^ (0x9e3779b9)) + hashAgg_hash_1 + (hashAgg_result_0 << 6) + (hashAgg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 128 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 129 */\n",
      "/* 130 */       return hashAgg_hash_0;\n",
      "/* 131 */     }\n",
      "/* 132 */\n",
      "/* 133 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 134 */       return batch.rowIterator();\n",
      "/* 135 */     }\n",
      "/* 136 */\n",
      "/* 137 */     public void close() {\n",
      "/* 138 */       batch.close();\n",
      "/* 139 */     }\n",
      "/* 140 */\n",
      "/* 141 */   }\n",
      "/* 142 */\n",
      "/* 143 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 144 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 145 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 146 */\n",
      "/* 147 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 148 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 149 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 150 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 151 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 152 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 153 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 154 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 155 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 156 */\n",
      "/* 157 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 158 */       // shouldStop check is eliminated\n",
      "/* 159 */     }\n",
      "/* 160 */\n",
      "/* 161 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 162 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 163 */\n",
      "/* 164 */   }\n",
      "/* 165 */\n",
      "/* 166 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, UTF8String hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 167 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 168 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 169 */\n",
      "/* 170 */     if (!hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_0_0) {\n",
      "/* 171 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 172 */         hashAgg_expr_2_0, hashAgg_expr_0_0);\n",
      "/* 173 */     }\n",
      "/* 174 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 175 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 176 */       // generate grouping key\n",
      "/* 177 */       hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 178 */\n",
      "/* 179 */       hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 180 */\n",
      "/* 181 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 182 */         hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 183 */       } else {\n",
      "/* 184 */         hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_2_0);\n",
      "/* 185 */       }\n",
      "/* 186 */\n",
      "/* 187 */       if (hashAgg_exprIsNull_0_0) {\n",
      "/* 188 */         hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 189 */       } else {\n",
      "/* 190 */         hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_0_0);\n",
      "/* 191 */       }\n",
      "/* 192 */       int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 193 */       if (true) {\n",
      "/* 194 */         // try to get the buffer from hash map\n",
      "/* 195 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 196 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 197 */       }\n",
      "/* 198 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 199 */       // aggregation after processing all input rows.\n",
      "/* 200 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 201 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 202 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 203 */         } else {\n",
      "/* 204 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 205 */         }\n",
      "/* 206 */\n",
      "/* 207 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 208 */         // try to allocate buffer again.\n",
      "/* 209 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 210 */           (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 211 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 212 */           // failed to allocate the first page\n",
      "/* 213 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 214 */         }\n",
      "/* 215 */       }\n",
      "/* 216 */\n",
      "/* 217 */     }\n",
      "/* 218 */\n",
      "/* 219 */     // Updates the proper row buffer\n",
      "/* 220 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 221 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 222 */     }\n",
      "/* 223 */\n",
      "/* 224 */     // common sub-expressions\n",
      "/* 225 */\n",
      "/* 226 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 227 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_1_0, hashAgg_expr_1_0);\n",
      "/* 228 */\n",
      "/* 229 */   }\n",
      "/* 230 */\n",
      "/* 231 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_1_0) throws java.io.IOException {\n",
      "/* 232 */     hashAgg_hashAgg_isNull_5_0 = true;\n",
      "/* 233 */     double hashAgg_value_6 = -1.0;\n",
      "/* 234 */     do {\n",
      "/* 235 */       boolean hashAgg_isNull_6 = true;\n",
      "/* 236 */       double hashAgg_value_7 = -1.0;\n",
      "/* 237 */       hashAgg_hashAgg_isNull_7_0 = true;\n",
      "/* 238 */       double hashAgg_value_8 = -1.0;\n",
      "/* 239 */       do {\n",
      "/* 240 */         boolean hashAgg_isNull_8 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 241 */         double hashAgg_value_9 = hashAgg_isNull_8 ?\n",
      "/* 242 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 243 */         if (!hashAgg_isNull_8) {\n",
      "/* 244 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 245 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 246 */           continue;\n",
      "/* 247 */         }\n",
      "/* 248 */\n",
      "/* 249 */         if (!false) {\n",
      "/* 250 */           hashAgg_hashAgg_isNull_7_0 = false;\n",
      "/* 251 */           hashAgg_value_8 = 0.0D;\n",
      "/* 252 */           continue;\n",
      "/* 253 */         }\n",
      "/* 254 */\n",
      "/* 255 */       } while (false);\n",
      "/* 256 */\n",
      "/* 257 */       if (!hashAgg_exprIsNull_1_0) {\n",
      "/* 258 */         hashAgg_isNull_6 = false; // resultCode could change nullability.\n",
      "/* 259 */\n",
      "/* 260 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_1_0;\n",
      "/* 261 */\n",
      "/* 262 */       }\n",
      "/* 263 */       if (!hashAgg_isNull_6) {\n",
      "/* 264 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 265 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 266 */         continue;\n",
      "/* 267 */       }\n",
      "/* 268 */\n",
      "/* 269 */       boolean hashAgg_isNull_11 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 270 */       double hashAgg_value_12 = hashAgg_isNull_11 ?\n",
      "/* 271 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 272 */       if (!hashAgg_isNull_11) {\n",
      "/* 273 */         hashAgg_hashAgg_isNull_5_0 = false;\n",
      "/* 274 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 275 */         continue;\n",
      "/* 276 */       }\n",
      "/* 277 */\n",
      "/* 278 */     } while (false);\n",
      "/* 279 */\n",
      "/* 280 */     if (!hashAgg_hashAgg_isNull_5_0) {\n",
      "/* 281 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 282 */     } else {\n",
      "/* 283 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 284 */     }\n",
      "/* 285 */   }\n",
      "/* 286 */\n",
      "/* 287 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 288 */   throws java.io.IOException {\n",
      "/* 289 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 290 */\n",
      "/* 291 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 292 */     UTF8String hashAgg_value_13 = hashAgg_isNull_12 ?\n",
      "/* 293 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 294 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 295 */     int hashAgg_value_14 = hashAgg_isNull_13 ?\n",
      "/* 296 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 297 */     boolean hashAgg_isNull_14 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 298 */     double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 299 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 300 */\n",
      "/* 301 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 302 */\n",
      "/* 303 */     hashAgg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 304 */\n",
      "/* 305 */     if (hashAgg_isNull_12) {\n",
      "/* 306 */       hashAgg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 307 */     } else {\n",
      "/* 308 */       hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_13);\n",
      "/* 309 */     }\n",
      "/* 310 */\n",
      "/* 311 */     if (hashAgg_isNull_13) {\n",
      "/* 312 */       hashAgg_mutableStateArray_0[1].setNullAt(1);\n",
      "/* 313 */     } else {\n",
      "/* 314 */       hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_14);\n",
      "/* 315 */     }\n",
      "/* 316 */\n",
      "/* 317 */     if (hashAgg_isNull_14) {\n",
      "/* 318 */       hashAgg_mutableStateArray_0[1].setNullAt(2);\n",
      "/* 319 */     } else {\n",
      "/* 320 */       hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_15);\n",
      "/* 321 */     }\n",
      "/* 322 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 323 */\n",
      "/* 324 */   }\n",
      "/* 325 */\n",
      "/* 326 */   protected void processNext() throws java.io.IOException {\n",
      "/* 327 */     if (!hashAgg_initAgg_0) {\n",
      "/* 328 */       hashAgg_initAgg_0 = true;\n",
      "/* 329 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 330 */\n",
      "/* 331 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 332 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 333 */           @Override\n",
      "/* 334 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 335 */             hashAgg_fastHashMap_0.close();\n",
      "/* 336 */           }\n",
      "/* 337 */         });\n",
      "/* 338 */\n",
      "/* 339 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 340 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 341 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 342 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 343 */     }\n",
      "/* 344 */     // output the result\n",
      "/* 345 */\n",
      "/* 346 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 347 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 348 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 349 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 350 */\n",
      "/* 351 */       if (shouldStop()) return;\n",
      "/* 352 */     }\n",
      "/* 353 */     hashAgg_fastHashMap_0.close();\n",
      "/* 354 */\n",
      "/* 355 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 356 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 357 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 358 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 359 */       if (shouldStop()) return;\n",
      "/* 360 */     }\n",
      "/* 361 */     hashAgg_mapIter_0.close();\n",
      "/* 362 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 363 */       hashAgg_hashMap_0.free();\n",
      "/* 364 */     }\n",
      "/* 365 */   }\n",
      "/* 366 */\n",
      "/* 367 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 351.8 KiB, free 360.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_87 locally took 1 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_87 without replication took 1 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 360.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_87_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_87_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_87_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_87_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_87_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 87 from showString at <unknown>:0\n",
      "25/04/11 09:59:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 206 took 0.000043 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Registering RDD 206 (showString at <unknown>:0) as input to shuffle 8\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got map stage job 50 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (showString at <unknown>:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ShuffleMapStage 62 (name=showString at <unknown>:0;jobs=50))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[206] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 62)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 42.7 KiB, free 360.2 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_88 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_88 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 360.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_88_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on macbookpro.lan:57375 (size: 19.8 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_88_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_88_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_88_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_88_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[206] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 62.0: 8\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 62.0: NO_PREF, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_62.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 50) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 62.0 (TID 50)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (62, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_88\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_88 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, string, true], input[1, int, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       UTF8String value_2 = isNull_2 ?\n",
      "/* 039 */       null : (i.getUTF8String(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(value_2.getBaseObject(), value_2.getBaseOffset(), value_2.numBytes(), value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */\n",
      "/* 050 */       int remainder_0 = value_1 % 200;\n",
      "/* 051 */       if (remainder_0 < 0) {\n",
      "/* 052 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 053 */       } else {\n",
      "/* 054 */         value_0=remainder_0;\n",
      "/* 055 */       }\n",
      "/* 056 */\n",
      "/* 057 */     }\n",
      "/* 058 */     if (isNull_0) {\n",
      "/* 059 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 060 */     } else {\n",
      "/* 061 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 062 */     }\n",
      "/* 063 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 064 */   }\n",
      "/* 065 */\n",
      "/* 066 */\n",
      "/* 067 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 50 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@54582708\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 50 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@1b186833\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, double, true],input[2, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 051 */     null : (i.getUTF8String(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_87\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_87 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 50 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@1b186833\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 50 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch@54582708\n",
      "25/04/11 09:59:13 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 50 with length 200\n",
      "25/04/11 09:59:13 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 50: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,81,0,0,0,0,0,0,81,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,81,0,0,0,0,0,0,0,0,81,81,0,0,81,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,0,0,0,0]\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 62.0 (TID 50). 2724 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (62, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 50) in 31 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:59:13 INFO DAGScheduler: ShuffleMapStage 62 (showString at <unknown>:0) finished in 0.035 s\n",
      "25/04/11 09:59:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:59:13 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Increasing epoch to 9\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 62, remaining stages = 0\n",
      "25/04/11 09:59:13 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=720.0 B, rowCount=18) for plan: HashAggregate(keys=[billtype#1045, storeid#1041], functions=[sum(saleamount#1043)], output=[toprettystring(billtype)#1189, toprettystring(storeid)#1190, toprettystring(sum(saleamount))#1191])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange hashpartitioning(billtype#1045, storeid#1041, 200), ENSURE_REQUIREMENTS, [plan_id=1201]\n",
      "      +- *(1) HashAggregate(keys=[billtype#1045, storeid#1041], functions=[partial_sum(saleamount#1043)], output=[billtype#1045, storeid#1041, sum#1196])\n",
      "         +- FileScan csv [storeid#1041,saleamount#1043,billtype#1045] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG AdaptiveSparkPlanExec: Plan changed:\n",
      "!CollectLimit 21                                                                                                                                                                                                                                                                                                                                     HashAggregate(keys=[billtype#1045, storeid#1041], functions=[sum(saleamount#1043)], output=[toprettystring(billtype)#1189, toprettystring(storeid)#1190, toprettystring(sum(saleamount))#1191])\n",
      "!+- HashAggregate(keys=[billtype#1045, storeid#1041], functions=[sum(saleamount#1043)], output=[toprettystring(billtype)#1189, toprettystring(storeid)#1190, toprettystring(sum(saleamount))#1191])                                                                                                                                                  +- ShuffleQueryStage 0\n",
      "!   +- ShuffleQueryStage 0                                                                                                                                                                                                                                                                                                                              +- Exchange hashpartitioning(billtype#1045, storeid#1041, 200), ENSURE_REQUIREMENTS, [plan_id=1201]\n",
      "!      +- Exchange hashpartitioning(billtype#1045, storeid#1041, 200), ENSURE_REQUIREMENTS, [plan_id=1201]                                                                                                                                                                                                                                                 +- *(1) HashAggregate(keys=[billtype#1045, storeid#1041], functions=[partial_sum(saleamount#1043)], output=[billtype#1045, storeid#1041, sum#1196])\n",
      "!         +- *(1) HashAggregate(keys=[billtype#1045, storeid#1041], functions=[partial_sum(saleamount#1043)], output=[billtype#1045, storeid#1041, sum#1196])                                                                                                                                                                                                 +- FileScan csv [storeid#1041,saleamount#1043,billtype#1045] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "!            +- FileScan csv [storeid#1041,saleamount#1043,billtype#1045] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>   \n",
      "25/04/11 09:59:13 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:59:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:59:13 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_4_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       double inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1.0 : (inputadapter_row_0.getDouble(2));\n",
      "/* 045 */\n",
      "/* 046 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);\n",
      "/* 047 */       // shouldStop check is eliminated\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 051 */   }\n",
      "/* 052 */\n",
      "/* 053 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, UTF8String hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 054 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 055 */\n",
      "/* 056 */     // generate grouping key\n",
      "/* 057 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 062 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */     } else {\n",
      "/* 064 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 068 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 069 */     } else {\n",
      "/* 070 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 071 */     }\n",
      "/* 072 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 073 */     if (true) {\n",
      "/* 074 */       // try to get the buffer from hash map\n",
      "/* 075 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 076 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 077 */     }\n",
      "/* 078 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 079 */     // aggregation after processing all input rows.\n",
      "/* 080 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 081 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 082 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 083 */       } else {\n",
      "/* 084 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 085 */       }\n",
      "/* 086 */\n",
      "/* 087 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 088 */       // try to allocate buffer again.\n",
      "/* 089 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 090 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 091 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 092 */         // failed to allocate the first page\n",
      "/* 093 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 094 */       }\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     // common sub-expressions\n",
      "/* 098 */\n",
      "/* 099 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 100 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_2_0, hashAgg_exprIsNull_2_0);\n",
      "/* 101 */\n",
      "/* 102 */   }\n",
      "/* 103 */\n",
      "/* 104 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0) throws java.io.IOException {\n",
      "/* 105 */     hashAgg_hashAgg_isNull_4_0 = true;\n",
      "/* 106 */     double hashAgg_value_4 = -1.0;\n",
      "/* 107 */     do {\n",
      "/* 108 */       boolean hashAgg_isNull_5 = true;\n",
      "/* 109 */       double hashAgg_value_5 = -1.0;\n",
      "/* 110 */       hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 111 */       double hashAgg_value_6 = -1.0;\n",
      "/* 112 */       do {\n",
      "/* 113 */         boolean hashAgg_isNull_7 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 114 */         double hashAgg_value_7 = hashAgg_isNull_7 ?\n",
      "/* 115 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 116 */         if (!hashAgg_isNull_7) {\n",
      "/* 117 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 118 */           hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 119 */           continue;\n",
      "/* 120 */         }\n",
      "/* 121 */\n",
      "/* 122 */         if (!false) {\n",
      "/* 123 */           hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 124 */           hashAgg_value_6 = 0.0D;\n",
      "/* 125 */           continue;\n",
      "/* 126 */         }\n",
      "/* 127 */\n",
      "/* 128 */       } while (false);\n",
      "/* 129 */\n",
      "/* 130 */       if (!hashAgg_exprIsNull_2_0) {\n",
      "/* 131 */         hashAgg_isNull_5 = false; // resultCode could change nullability.\n",
      "/* 132 */\n",
      "/* 133 */         hashAgg_value_5 = hashAgg_value_6 + hashAgg_expr_2_0;\n",
      "/* 134 */\n",
      "/* 135 */       }\n",
      "/* 136 */       if (!hashAgg_isNull_5) {\n",
      "/* 137 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 138 */         hashAgg_value_4 = hashAgg_value_5;\n",
      "/* 139 */         continue;\n",
      "/* 140 */       }\n",
      "/* 141 */\n",
      "/* 142 */       boolean hashAgg_isNull_10 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 143 */       double hashAgg_value_10 = hashAgg_isNull_10 ?\n",
      "/* 144 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 145 */       if (!hashAgg_isNull_10) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_4_0 = false;\n",
      "/* 147 */         hashAgg_value_4 = hashAgg_value_10;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */     } while (false);\n",
      "/* 152 */\n",
      "/* 153 */     if (!hashAgg_hashAgg_isNull_4_0) {\n",
      "/* 154 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_4);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 157 */     }\n",
      "/* 158 */   }\n",
      "/* 159 */\n",
      "/* 160 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 161 */   throws java.io.IOException {\n",
      "/* 162 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 163 */\n",
      "/* 164 */     boolean hashAgg_isNull_11 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 165 */     UTF8String hashAgg_value_11 = hashAgg_isNull_11 ?\n",
      "/* 166 */     null : (hashAgg_keyTerm_0.getUTF8String(0));\n",
      "/* 167 */     boolean hashAgg_isNull_12 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 168 */     int hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 169 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 170 */     boolean hashAgg_isNull_13 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 171 */     double hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 172 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 173 */\n",
      "/* 174 */     UTF8String hashAgg_value_15;\n",
      "/* 175 */     if (hashAgg_isNull_11) {\n",
      "/* 176 */       hashAgg_value_15 = UTF8String.fromString(\"NULL\");\n",
      "/* 177 */     } else {\n",
      "/* 178 */       hashAgg_value_15 = hashAgg_value_11;\n",
      "/* 179 */     }\n",
      "/* 180 */     UTF8String hashAgg_value_17;\n",
      "/* 181 */     if (hashAgg_isNull_12) {\n",
      "/* 182 */       hashAgg_value_17 = UTF8String.fromString(\"NULL\");\n",
      "/* 183 */     } else {\n",
      "/* 184 */       hashAgg_value_17 = UTF8String.fromString(String.valueOf(hashAgg_value_12));\n",
      "/* 185 */     }\n",
      "/* 186 */     UTF8String hashAgg_value_19;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_19 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_19 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 193 */\n",
      "/* 194 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_15);\n",
      "/* 195 */\n",
      "/* 196 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_17);\n",
      "/* 197 */\n",
      "/* 198 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_19);\n",
      "/* 199 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 200 */\n",
      "/* 201 */   }\n",
      "/* 202 */\n",
      "/* 203 */   protected void processNext() throws java.io.IOException {\n",
      "/* 204 */     if (!hashAgg_initAgg_0) {\n",
      "/* 205 */       hashAgg_initAgg_0 = true;\n",
      "/* 206 */\n",
      "/* 207 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 208 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 209 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 210 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 211 */     }\n",
      "/* 212 */     // output the result\n",
      "/* 213 */\n",
      "/* 214 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 215 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 216 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 217 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 218 */       if (shouldStop()) return;\n",
      "/* 219 */     }\n",
      "/* 220 */     hashAgg_mapIter_0.close();\n",
      "/* 221 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 222 */       hashAgg_hashMap_0.free();\n",
      "/* 223 */     }\n",
      "/* 224 */   }\n",
      "/* 225 */\n",
      "/* 226 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:13 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:13 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 209 took 0.000094 seconds\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Got job 51 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Final stage: ResultStage 64 (showString at <unknown>:0)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitStage(ResultStage 64 (name=showString at <unknown>:0;jobs=51))\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[209] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: submitMissingTasks(ResultStage 64)\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 45.4 KiB, free 360.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_89 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_89 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 21.1 KiB, free 360.1 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_89_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:13 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on macbookpro.lan:57375 (size: 21.1 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:13 DEBUG BlockManagerMaster: Updated info of block broadcast_89_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Told master about block broadcast_89_piece0\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Put block broadcast_89_piece0 locally took 0 ms\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Putting block broadcast_89_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:13 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[209] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Epoch for TaskSet 64.0: 9\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: Valid locality levels for TaskSet 64.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:13 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_64.0, runningTasks: 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 51) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:59:13 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:13 INFO Executor: Running task 0.0 in stage 64.0 (TID 51)\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (64, 0) -> 1\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local block broadcast_89\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Level for block broadcast_89 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 8\n",
      "25/04/11 09:59:13 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 8, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Getting 1 (1576.0 B) non-empty blocks including 1 (1576.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_8_50_14_196,0)\n",
      "25/04/11 09:59:13 DEBUG BlockManager: Getting local shuffle block shuffle_8_50_14_196\n",
      "25/04/11 09:59:13 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 51 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@46bed0f\n",
      "25/04/11 09:59:13 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 51 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@46bed0f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 51 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@46bed0f\n",
      "25/04/11 09:59:13 DEBUG TaskMemoryManager: Task 51 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@46bed0f\n",
      "25/04/11 09:59:13 INFO Executor: Finished task 0.0 in stage 64.0 (TID 51). 5613 bytes result sent to driver\n",
      "25/04/11 09:59:13 DEBUG ExecutorMetricsPoller: stageTCMP: (64, 0) -> 0\n",
      "25/04/11 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 51) in 9 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:13 INFO DAGScheduler: ResultStage 64 (showString at <unknown>:0) finished in 0.013 s\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 64, remaining stages = 1\n",
      "25/04/11 09:59:13 DEBUG DAGScheduler: After removal of stage 63, remaining stages = 0\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished\n",
      "25/04/11 09:59:13 INFO DAGScheduler: Job 51 finished: showString at <unknown>:0, took 0.014601 s\n",
      "25/04/11 09:59:13 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(2) HashAggregate(keys=[billtype#1045, storeid#1041], functions=[sum(saleamount#1043)], output=[toprettystring(billtype)#1189, toprettystring(storeid)#1190, toprettystring(sum(saleamount))#1191])\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange hashpartitioning(billtype#1045, storeid#1041, 200), ENSURE_REQUIREMENTS, [plan_id=1201]\n",
      "         +- *(1) HashAggregate(keys=[billtype#1045, storeid#1041], functions=[partial_sum(saleamount#1043)], output=[billtype#1045, storeid#1041, sum#1196])\n",
      "            +- FileScan csv [storeid#1041,saleamount#1043,billtype#1045] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<storeid:int,saleamount:double,billtype:string>\n",
      "\n",
      "25/04/11 09:59:13 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, StructField(toprettystring(billtype),StringType,false), StructField(toprettystring(storeid),StringType,false), StructField(toprettystring(sum(saleamount)),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'year to year#1206\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'quarter to quarter#1217\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'month to month#1229\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'year to year#1206\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'quarter to quarter#1217\n",
      "25/04/11 09:59:13 DEBUG ResolveReferencesInAggregate: Resolving 'month to month#1229\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Got cleaning task CleanAccum(2931)\n",
      "25/04/11 09:59:13 DEBUG ContextCleaner: Cleaning accumulator 2931\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2931\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2934)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2934\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2934\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2921)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2921\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2921\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3255)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3255\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3255\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3538)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3538\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3538\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3645)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3645\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3645\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3314)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3314\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3314\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3385)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3385\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3385\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3188)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3188\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3188\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3387)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3387\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3387\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2927)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2927\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2927\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3410)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3410\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3410\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3468)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3468\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3468\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3317)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3317\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3317\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3295)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3295\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3295\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3012)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3012\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3012\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(80)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 80\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 80\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 80\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 80\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_80\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_80 of size 15896 dropped from memory (free 377575769)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_80_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_80_piece0 of size 7810 dropped from memory (free 377583579)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_80_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_80_piece0 on macbookpro.lan:57375 in memory (size: 7.6 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_80_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_80_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 80, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 80\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3655)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3655\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3655\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3023)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3023\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3023\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3143)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3143\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3143\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3540)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3540\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3540\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2941)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2941\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2941\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2917)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2917\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2917\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3365)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3365\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3365\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3614)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3614\n",
      "25/04/11 09:59:14 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3614\n",
      "25/04/11 09:59:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3644)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3644\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3644\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3425)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3425\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3425\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2983)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2983\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2983\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2792)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2792\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2792\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2777)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2777\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2777\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3321)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3321\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3321\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2795)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2795\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2795\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3388)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3388\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3388\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2735)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2735\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2735\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3427)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3427\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3427\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3036)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3036\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3036\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3593)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3593\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3593\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2786)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2786\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2786\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2923)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2923\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2923\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3370)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3370\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3370\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3395)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3395\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3395\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3411)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3411\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3411\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2743)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2743\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2743\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2769)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2769\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2769\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3543)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3543\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3543\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3417)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3417\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3417\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2861)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2861\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2861\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3160)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3160\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3160\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2800)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2800\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2800\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3383)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3383\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3383\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3408)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3408\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3408\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2734)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2734\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2734\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3164)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3164\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3164\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2799)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2799\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2799\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3247)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3247\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3247\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3551)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3551\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3551\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2965)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2965\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2965\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3591)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3591\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3591\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3649)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3649\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3649\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3303)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3303\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3303\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3399)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3399\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3399\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2911)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2911\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2911\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3133)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3133\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3133\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2912)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2912\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2912\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2989)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2989\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2989\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3552)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3552\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3552\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2916)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2916\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2916\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2980)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2980\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2980\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3144)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3144\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3144\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3227)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3227\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3227\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3658)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3658\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3658\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2790)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2790\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2790\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3547)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3547\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3547\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2969)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2969\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2969\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3053)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3053\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3053\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3166)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3166\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3166\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2738)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2738\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2738\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3003)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3003\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3003\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3451)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3451\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3451\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3232)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3232\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3232\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2750)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2750\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2750\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2952)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2952\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2952\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3132)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3132\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3132\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3216)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3216\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3216\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2961)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2961\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2961\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(77)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 77\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 77\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 77\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 77\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_77_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_77_piece0 of size 23793 dropped from memory (free 377607372)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_77_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: CollectLimit 21\n",
      "+- HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[sum(saleamount#1043)], output=[toprettystring(year)#1264, toprettystring(quarter)#1265, toprettystring(month)#1266, toprettystring(sum(saleamount))#1267])\n",
      "   +- HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[partial_sum(saleamount#1043)], output=[year#1206, quarter#1217, month#1229, sum#1273])\n",
      "      +- Project [saleamount#1043, year(saledate#1162) AS year#1206, quarter(saledate#1162) AS quarter#1217, month(saledate#1162) AS month#1229]\n",
      "         +- Project [cast(gettimestamp(saledate#1038, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#1162, saleamount#1043]\n",
      "            +- FileScan csv [saledate#1038,saleamount#1043] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_77_piece0 on macbookpro.lan:57375 in memory (size: 23.2 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_77_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_77_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_77\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_77 of size 51544 dropped from memory (free 377658916)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 77, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 77\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3534)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3534\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3534\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3602)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3602\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3602\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2977)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2977\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2977\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3412)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3412\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3412\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3014)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3014\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3014\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3207)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3207\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3207\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2742)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2742\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2742\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3199)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3199\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3199\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2967)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2967\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2967\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3539)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3539\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3539\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2900)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2900\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2900\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2753)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2753\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2753\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3174)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3174\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3174\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2748)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2748\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2748\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3215)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3215\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3215\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3137)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3137\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3137\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2922)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2922\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2922\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2957)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2957\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2957\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3381)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3381\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3381\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3467)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3467\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3467\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3135)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3135\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3135\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3179)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3179\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3179\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3413)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3413\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3413\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2776)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2776\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2776\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2901)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2901\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2901\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2936)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2936\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2936\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3418)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3418\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3418\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2756)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2756\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2756\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3390)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3390\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3390\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3455)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3455\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3455\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2787)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2787\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2787\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3170)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3170\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3170\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3024)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3024\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3024\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2798)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2798\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2798\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3553)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3553\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3553\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2747)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2747\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2747\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3441)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3441\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3441\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2939)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2939\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2939\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2784)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2784\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2784\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3380)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3380\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3380\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3603)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3603\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3603\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3139)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3139\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3139\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(89)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 89\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 89\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 89\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 89\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_89\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_89 of size 46528 dropped from memory (free 377705444)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_89_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_89_piece0 of size 21605 dropped from memory (free 377727049)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_89_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_89_piece0 on macbookpro.lan:57375 in memory (size: 21.1 KiB, free: 365.9 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_89_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_89_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 89, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 89\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3646)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3646\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3646\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2793)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2793\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2793\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3141)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3141\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3141\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3304)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3304\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3304\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2943)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2943\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2943\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2940)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2940\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2940\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3052)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3052\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3052\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3022)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3022\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3022\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3237)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3237\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3237\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3424)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3424\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3424\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2970)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2970\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2970\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3604)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3604\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3604\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3316)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3316\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3316\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3214)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3214\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3214\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3550)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3550\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3550\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3601)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3601\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3601\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2933)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2933\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2933\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2772)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2772\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2772\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3041)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3041\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3041\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3452)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3452\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3452\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3254)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3254\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3254\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2857)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2857\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2857\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3223)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3223\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3223\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3308)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3308\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3308\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3634)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3634\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3634\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3449)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3449\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3449\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3000)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3000\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3000\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2744)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2744\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2744\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3542)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3542\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3542\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3161)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3161\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3161\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3221)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3221\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3221\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3460)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3460\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3460\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3011)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3011\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3011\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3402)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3402\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3402\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3545)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3545\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3545\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3145)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3145\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3145\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2981)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2981\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2981\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3028)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3028\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3028\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3040)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3040\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3040\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3432)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3432\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3432\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3377)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3377\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3377\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3324)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3324\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3324\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2908)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2908\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2908\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2755)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2755\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2755\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3168)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3168\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3168\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3637)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3637\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3637\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3153)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3153\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3153\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2770)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2770\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2770\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3666)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3666\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3666\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3661)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3661\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3661\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3423)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3423\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3423\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3005)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3005\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3005\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2975)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2975\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2975\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2794)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2794\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2794\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3302)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3302\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3302\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3158)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3158\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3158\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3184)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3184\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3184\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3647)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3647\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3647\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2782)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2782\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2782\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3590)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3590\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3590\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3660)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3660\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3660\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3443)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3443\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3443\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3242)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3242\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3242\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3034)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3034\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3034\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2946)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2946\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2946\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3323)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3323\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3323\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3472)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3472\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3472\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3401)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3401\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3401\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2991)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2991\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2991\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2932)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2932\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2932\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3656)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3656\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3656\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3331)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3331\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3331\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2974)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2974\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2974\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2754)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2754\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2754\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3296)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3296\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3296\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2935)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2935\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2935\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(73)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 73\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 73\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 73\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 73\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_73\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_73 of size 360200 dropped from memory (free 378087249)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_73_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_73_piece0 of size 35539 dropped from memory (free 378122788)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_73_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_73_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_73_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_73_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 73, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 73\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3186)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3186\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3186\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3549)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3549\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3549\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3654)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3654\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3654\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2972)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2972\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2972\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3391)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3391\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3391\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2902)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2902\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2902\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3326)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3326\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3326\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3384)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3384\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3384\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3004)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3004\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3004\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3379)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3379\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3379\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3130)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3130\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3130\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3653)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3653\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3653\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3375)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3375\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3375\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2993)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2993\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2993\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3382)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3382\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3382\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2746)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2746\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2746\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3652)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3652\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3652\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3589)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3589\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3589\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3249)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3249\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3249\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3431)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3431\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3431\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3209)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3209\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3209\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3235)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3235\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3235\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3183)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3183\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3183\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3635)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3635\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3635\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3471)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3471\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3471\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2966)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2966\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2966\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2797)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2797\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2797\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3225)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3225\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3225\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3330)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3330\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3330\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3241)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3241\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3241\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3220)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3220\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3220\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3374)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3374\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3374\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2765)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2765\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2765\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3312)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3312\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3312\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3213)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3213\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3213\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2909)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2909\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2909\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3605)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3605\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3605\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3038)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3038\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3038\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2762)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2762\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2762\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3310)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3310\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3310\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3181)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3181\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3181\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2955)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2955\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2955\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2988)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2988\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2988\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3329)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3329\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3329\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3228)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3228\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3228\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3367)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3367\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3367\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3030)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3030\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3030\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3042)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3042\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3042\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2971)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2971\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2971\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3322)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3322\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3322\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3465)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3465\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3465\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3156)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3156\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3156\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2907)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2907\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2907\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3447)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3447\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3447\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3246)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3246\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3246\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3607)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3607\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3607\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3609)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3609\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3609\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3315)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3315\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3315\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3622)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3622\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3622\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2998)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2998\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2998\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3592)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3592\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3592\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3456)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3456\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3456\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3648)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3648\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3648\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2859)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2859\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2859\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3177)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3177\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3177\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2992)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2992\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2992\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2960)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2960\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2960\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3229)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3229\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3229\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2976)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2976\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2976\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3313)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3313\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3313\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3205)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3205\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3205\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2741)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2741\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2741\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3245)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3245\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3245\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2985)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2985\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2985\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2736)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2736\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2736\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3638)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3638\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3638\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3043)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3043\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3043\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2858)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2858\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2858\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3045)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3045\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3045\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3448)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3448\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3448\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3050)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3050\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3050\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3459)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3459\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3459\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2788)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2788\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2788\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2918)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2918\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2918\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3398)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3398\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3398\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3366)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3366\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3366\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2964)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2964\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2964\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3039)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3039\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3039\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2929)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2929\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2929\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3298)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3298\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3298\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2862)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2862\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2862\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3612)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3612\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3612\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3253)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3253\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3253\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2944)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2944\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2944\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3252)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3252\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3252\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3618)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3618\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3618\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3397)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3397\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3397\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3173)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3173\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3173\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3611)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3611\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3611\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3009)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3009\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3009\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3376)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3376\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3376\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3007)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3007\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3007\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3201)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3201\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3201\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3148)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3148\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3148\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3198)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3198\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3198\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3662)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3662\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3662\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3409)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3409\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3409\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3151)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3151\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3151\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3422)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3422\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3422\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3650)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3650\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3650\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2948)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2948\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2948\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3594)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3594\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3594\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3297)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3297\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3297\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2778)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2778\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2778\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3450)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3450\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3450\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2951)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2951\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2951\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3165)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3165\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3165\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3208)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3208\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3208\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3386)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3386\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3386\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3182)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3182\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3182\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3210)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3210\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3210\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3328)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3328\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3328\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2789)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2789\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2789\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2997)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2997\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2997\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3138)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3138\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3138\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3454)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3454\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3454\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3406)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3406\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3406\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2768)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2768\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2768\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3175)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3175\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3175\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(84)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 84\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 84\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 84\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 84\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_84\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_84 of size 53584 dropped from memory (free 378176372)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_84_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_84_piece0 of size 24763 dropped from memory (free 378201135)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_84_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_84_piece0 on macbookpro.lan:57375 in memory (size: 24.2 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_84_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_84_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 84, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 84\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3305)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3305\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3305\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3640)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3640\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3640\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3234)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3234\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3234\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3233)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3233\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3233\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3037)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3037\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3037\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3026)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3026\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3026\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3606)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3606\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3606\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3600)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3600\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3600\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3473)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3473\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3473\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3027)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3027\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3027\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3619)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3619\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3619\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3433)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3433\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3433\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2949)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2949\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2949\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3046)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3046\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3046\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3048)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3048\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3048\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2903)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2903\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2903\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3470)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3470\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3470\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2757)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2757\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2757\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3306)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3306\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3306\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2919)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2919\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2919\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3154)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3154\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3154\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3150)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3150\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3150\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2986)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2986\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2986\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2737)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2737\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2737\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(83)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 83\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 83\n",
      "25/04/11 09:59:14 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 83\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 83\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_83_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_83_piece0 of size 23897 dropped from memory (free 378225032)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_83_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_83_piece0 on macbookpro.lan:57375 in memory (size: 23.3 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_83_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_83_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_83\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_83 of size 51704 dropped from memory (free 378276736)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 83, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 83\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3369)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3369\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3369\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3248)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3248\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3248\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3595)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3595\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3595\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3392)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3392\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3392\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2779)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2779\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2779\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2984)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2984\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2984\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2987)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2987\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2987\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3236)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3236\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3236\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3169)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3169\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3169\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3033)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3033\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3033\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3226)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3226\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3226\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2775)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2775\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2775\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3010)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3010\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3010\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3231)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3231\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3231\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3243)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3243\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3243\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2785)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2785\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2785\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2947)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2947\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2947\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3049)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3049\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3049\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3668)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3668\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3668\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2924)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2924\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2924\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3462)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3462\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3462\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3131)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3131\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3131\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3035)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3035\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3035\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(78)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 78\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 78\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 78\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 78\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_78\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_78 of size 53128 dropped from memory (free 378329864)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_78_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_78_piece0 of size 24714 dropped from memory (free 378354578)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_78_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_78_piece0 on macbookpro.lan:57375 in memory (size: 24.1 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_78_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_78_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 78, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 78\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3641)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3641\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3641\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3002)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3002\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3002\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2928)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2928\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2928\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3187)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3187\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3187\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2953)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2953\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2953\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2860)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2860\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2860\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3474)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3474\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3474\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3309)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3309\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3309\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3008)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3008\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3008\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3446)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3446\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3446\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3463)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3463\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3463\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3466)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3466\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3466\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3389)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3389\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3389\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3371)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3371\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3371\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3134)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3134\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3134\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2996)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2996\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2996\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2958)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2958\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2958\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3299)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3299\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3299\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3197)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3197\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3197\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2733)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2733\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2733\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2763)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2763\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2763\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3444)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3444\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3444\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3203)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3203\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3203\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3054)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3054\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3054\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3664)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3664\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3664\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3015)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3015\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3015\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3373)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3373\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3373\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2995)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2995\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2995\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3536)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3536\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3536\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3453)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3453\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3453\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3211)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3211\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3211\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2994)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2994\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2994\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2913)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2913\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2913\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3032)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3032\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3032\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3327)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3327\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3327\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3416)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3416\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3416\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3178)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3178\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3178\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3044)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3044\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3044\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3364)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3364\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3364\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3224)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3224\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3224\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2796)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2796\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2796\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2732)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2732\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2732\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3016)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3016\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3016\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3393)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3393\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3393\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3172)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3172\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3172\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3300)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3300\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3300\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3464)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3464\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3464\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3250)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3250\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3250\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2759)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2759\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2759\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2739)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2739\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2739\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3421)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3421\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3421\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2781)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2781\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2781\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3613)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3613\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3613\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3610)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3610\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3610\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3665)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3665\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3665\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3222)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3222\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3222\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3636)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3636\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3636\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3217)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3217\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3217\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3311)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3311\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3311\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3013)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3013\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3013\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3458)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3458\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3458\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3001)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3001\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3001\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3420)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3420\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3420\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3029)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3029\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3029\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2926)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2926\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2926\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3621)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3621\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3621\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3617)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3617\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3617\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3136)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3136\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3136\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2925)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2925\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2925\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3219)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3219\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3219\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2954)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2954\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2954\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3396)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3396\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3396\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3206)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3206\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3206\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3659)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3659\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3659\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2982)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2982\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2982\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3616)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3616\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3616\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3372)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2749)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2749\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2749\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3428)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3428\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3428\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3185)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3185\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3185\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3202)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3202\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3202\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2963)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2963\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2963\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2731)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2731\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2731\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2791)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2791\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2791\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3294)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3294\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3294\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2780)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2780\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2780\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2962)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2962\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2962\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3368)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3368\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3368\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3400)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3400\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3400\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3667)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3667\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3667\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3152)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3152\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3152\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(75)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 75\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 75\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 75\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 75\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_75_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_75_piece0 of size 509 dropped from memory (free 378355087)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_75_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_75_piece0 on macbookpro.lan:57375 in memory (size: 509.0 B, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_75_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_75_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_75\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_75 of size 1048688 dropped from memory (free 379403775)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 75, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 75\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2915)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2915\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2915\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3414)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3414\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3414\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3608)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3608\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3608\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3157)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3157\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3157\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3189)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3189\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3189\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2959)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2959\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2959\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2942)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2942\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2942\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2950)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2950\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2950\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3620)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3620\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3620\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3651)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3651\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3651\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3598)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3598\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3598\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3535)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3535\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3535\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3244)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3244\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3244\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3544)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3544\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3544\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3643)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3643\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3643\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3171)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3171\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3171\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2979)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2979\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2979\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3445)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3445\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3445\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3442)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3442\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3442\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3200)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3200\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3200\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2773)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2773\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2773\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3162)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3162\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3162\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2906)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2906\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2906\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3469)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3469\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3469\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3319)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3319\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3319\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3430)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3430\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3430\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2774)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2774\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2774\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2899)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2899\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2899\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3155)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3155\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3155\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3167)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3167\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3167\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2968)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2968\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2968\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3163)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3163\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3163\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3018)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3018\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3018\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3642)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3642\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3642\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3256)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3256\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3256\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2945)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2945\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2945\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3240)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3240\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3240\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3239)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3239\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3239\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2930)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2930\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2930\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3230)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3230\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3230\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3019)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3019\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3019\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2973)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2973\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2973\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3159)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3159\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3159\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3597)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3597\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3597\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2752)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2752\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2752\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2766)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2766\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2766\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3419)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3419\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3419\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(88)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 88\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 88\n",
      "25/04/11 09:59:14 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_1 = true;\n",
      "/* 151 */       long project_value_1 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_1 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_1 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[7] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_1 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_1 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 168 */       int project_value_0 = -1;\n",
      "/* 169 */       if (!project_isNull_1) {\n",
      "/* 170 */         project_value_0 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[9] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */\n",
      "/* 174 */       // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */       boolean project_isNull_8 = project_isNull_0;\n",
      "/* 180 */       int project_value_8 = -1;\n",
      "/* 181 */\n",
      "/* 182 */       if (!project_isNull_0) {\n",
      "/* 183 */         project_value_8 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 184 */       }\n",
      "/* 185 */       boolean project_isNull_10 = project_isNull_0;\n",
      "/* 186 */       int project_value_10 = -1;\n",
      "/* 187 */\n",
      "/* 188 */       if (!project_isNull_0) {\n",
      "/* 189 */         project_value_10 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 190 */       }\n",
      "/* 191 */       boolean project_isNull_12 = project_isNull_0;\n",
      "/* 192 */       int project_value_12 = -1;\n",
      "/* 193 */\n",
      "/* 194 */       if (!project_isNull_0) {\n",
      "/* 195 */         project_value_12 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 196 */       }\n",
      "/* 197 */\n",
      "/* 198 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_8, project_isNull_8, project_value_10, project_isNull_10, project_value_12, project_isNull_12);\n",
      "/* 199 */       // shouldStop check is eliminated\n",
      "/* 200 */     }\n",
      "/* 201 */\n",
      "/* 202 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 203 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 204 */\n",
      "/* 205 */   }\n",
      "/* 206 */\n",
      "/* 207 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 208 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 209 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 210 */\n",
      "/* 211 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 212 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 213 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 214 */     }\n",
      "/* 215 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 216 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 217 */       // generate grouping key\n",
      "/* 218 */       project_mutableStateArray_0[3].reset();\n",
      "/* 219 */\n",
      "/* 220 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 221 */\n",
      "/* 222 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 223 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 224 */       } else {\n",
      "/* 225 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 226 */       }\n",
      "/* 227 */\n",
      "/* 228 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 229 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 230 */       } else {\n",
      "/* 231 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 232 */       }\n",
      "/* 233 */\n",
      "/* 234 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 235 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 236 */       } else {\n",
      "/* 237 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 238 */       }\n",
      "/* 239 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 240 */       if (true) {\n",
      "/* 241 */         // try to get the buffer from hash map\n",
      "/* 242 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 243 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 244 */       }\n",
      "/* 245 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 246 */       // aggregation after processing all input rows.\n",
      "/* 247 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 248 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 249 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 250 */         } else {\n",
      "/* 251 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 252 */         }\n",
      "/* 253 */\n",
      "/* 254 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 255 */         // try to allocate buffer again.\n",
      "/* 256 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 257 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 258 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 259 */           // failed to allocate the first page\n",
      "/* 260 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 261 */         }\n",
      "/* 262 */       }\n",
      "/* 263 */\n",
      "/* 264 */     }\n",
      "/* 265 */\n",
      "/* 266 */     // Updates the proper row buffer\n",
      "/* 267 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 268 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 269 */     }\n",
      "/* 270 */\n",
      "/* 271 */     // common sub-expressions\n",
      "/* 272 */\n",
      "/* 273 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 274 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 275 */\n",
      "/* 276 */   }\n",
      "/* 277 */\n",
      "/* 278 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 279 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 280 */     double hashAgg_value_12 = -1.0;\n",
      "/* 281 */     do {\n",
      "/* 282 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 283 */       double hashAgg_value_13 = -1.0;\n",
      "/* 284 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 285 */       double hashAgg_value_14 = -1.0;\n",
      "/* 286 */       do {\n",
      "/* 287 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 288 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 289 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 290 */         if (!hashAgg_isNull_14) {\n",
      "/* 291 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 292 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 293 */           continue;\n",
      "/* 294 */         }\n",
      "/* 295 */\n",
      "/* 296 */         if (!false) {\n",
      "/* 297 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 298 */           hashAgg_value_14 = 0.0D;\n",
      "/* 299 */           continue;\n",
      "/* 300 */         }\n",
      "/* 301 */\n",
      "/* 302 */       } while (false);\n",
      "/* 303 */\n",
      "/* 304 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 305 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 306 */\n",
      "/* 307 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 308 */\n",
      "/* 309 */       }\n",
      "/* 310 */       if (!hashAgg_isNull_12) {\n",
      "/* 311 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 312 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 313 */         continue;\n",
      "/* 314 */       }\n",
      "/* 315 */\n",
      "/* 316 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 317 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 318 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 319 */       if (!hashAgg_isNull_17) {\n",
      "/* 320 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 321 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 322 */         continue;\n",
      "/* 323 */       }\n",
      "/* 324 */\n",
      "/* 325 */     } while (false);\n",
      "/* 326 */\n",
      "/* 327 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 328 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 331 */     }\n",
      "/* 332 */   }\n",
      "/* 333 */\n",
      "/* 334 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 335 */   throws java.io.IOException {\n",
      "/* 336 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 337 */\n",
      "/* 338 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 339 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 340 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 341 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 342 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 343 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 344 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 345 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 346 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 347 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 348 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 349 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 350 */\n",
      "/* 351 */     project_mutableStateArray_0[4].reset();\n",
      "/* 352 */\n",
      "/* 353 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 354 */\n",
      "/* 355 */     if (hashAgg_isNull_18) {\n",
      "/* 356 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 357 */     } else {\n",
      "/* 358 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 359 */     }\n",
      "/* 360 */\n",
      "/* 361 */     if (hashAgg_isNull_19) {\n",
      "/* 362 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 363 */     } else {\n",
      "/* 364 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 365 */     }\n",
      "/* 366 */\n",
      "/* 367 */     if (hashAgg_isNull_20) {\n",
      "/* 368 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 369 */     } else {\n",
      "/* 370 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 371 */     }\n",
      "/* 372 */\n",
      "/* 373 */     if (hashAgg_isNull_21) {\n",
      "/* 374 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 375 */     } else {\n",
      "/* 376 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 377 */     }\n",
      "/* 378 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 379 */\n",
      "/* 380 */   }\n",
      "/* 381 */\n",
      "/* 382 */   protected void processNext() throws java.io.IOException {\n",
      "/* 383 */     if (!hashAgg_initAgg_0) {\n",
      "/* 384 */       hashAgg_initAgg_0 = true;\n",
      "/* 385 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 386 */\n",
      "/* 387 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 388 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 389 */           @Override\n",
      "/* 390 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 391 */             hashAgg_fastHashMap_0.close();\n",
      "/* 392 */           }\n",
      "/* 393 */         });\n",
      "/* 394 */\n",
      "/* 395 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 396 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 397 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 398 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 399 */     }\n",
      "/* 400 */     // output the result\n",
      "/* 401 */\n",
      "/* 402 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 403 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 404 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 405 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 406 */\n",
      "/* 407 */       if (shouldStop()) return;\n",
      "/* 408 */     }\n",
      "/* 409 */     hashAgg_fastHashMap_0.close();\n",
      "/* 410 */\n",
      "/* 411 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 412 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 413 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 414 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 415 */       if (shouldStop()) return;\n",
      "/* 416 */     }\n",
      "/* 417 */     hashAgg_mapIter_0.close();\n",
      "/* 418 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 419 */       hashAgg_hashMap_0.free();\n",
      "/* 420 */     }\n",
      "/* 421 */   }\n",
      "/* 422 */\n",
      "/* 423 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 88\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 88\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_88_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_88_piece0 of size 20252 dropped from memory (free 379424027)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_88_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_88_piece0 on macbookpro.lan:57375 in memory (size: 19.8 KiB, free: 366.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_88_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_88_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_88\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_88 of size 43752 dropped from memory (free 379467779)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 88, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_1 = true;\n",
      "/* 151 */       long project_value_1 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_1 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_1 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[7] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_1 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_1 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 168 */       int project_value_0 = -1;\n",
      "/* 169 */       if (!project_isNull_1) {\n",
      "/* 170 */         project_value_0 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[9] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */\n",
      "/* 174 */       // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */       boolean project_isNull_8 = project_isNull_0;\n",
      "/* 180 */       int project_value_8 = -1;\n",
      "/* 181 */\n",
      "/* 182 */       if (!project_isNull_0) {\n",
      "/* 183 */         project_value_8 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 184 */       }\n",
      "/* 185 */       boolean project_isNull_10 = project_isNull_0;\n",
      "/* 186 */       int project_value_10 = -1;\n",
      "/* 187 */\n",
      "/* 188 */       if (!project_isNull_0) {\n",
      "/* 189 */         project_value_10 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 190 */       }\n",
      "/* 191 */       boolean project_isNull_12 = project_isNull_0;\n",
      "/* 192 */       int project_value_12 = -1;\n",
      "/* 193 */\n",
      "/* 194 */       if (!project_isNull_0) {\n",
      "/* 195 */         project_value_12 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 196 */       }\n",
      "/* 197 */\n",
      "/* 198 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_8, project_isNull_8, project_value_10, project_isNull_10, project_value_12, project_isNull_12);\n",
      "/* 199 */       // shouldStop check is eliminated\n",
      "/* 200 */     }\n",
      "/* 201 */\n",
      "/* 202 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 203 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 204 */\n",
      "/* 205 */   }\n",
      "/* 206 */\n",
      "/* 207 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 208 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 209 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 210 */\n",
      "/* 211 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 212 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 213 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 214 */     }\n",
      "/* 215 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 216 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 217 */       // generate grouping key\n",
      "/* 218 */       project_mutableStateArray_0[3].reset();\n",
      "/* 219 */\n",
      "/* 220 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 221 */\n",
      "/* 222 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 223 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 224 */       } else {\n",
      "/* 225 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 226 */       }\n",
      "/* 227 */\n",
      "/* 228 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 229 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 230 */       } else {\n",
      "/* 231 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 232 */       }\n",
      "/* 233 */\n",
      "/* 234 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 235 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 236 */       } else {\n",
      "/* 237 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 238 */       }\n",
      "/* 239 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 240 */       if (true) {\n",
      "/* 241 */         // try to get the buffer from hash map\n",
      "/* 242 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 243 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 244 */       }\n",
      "/* 245 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 246 */       // aggregation after processing all input rows.\n",
      "/* 247 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 248 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 249 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 250 */         } else {\n",
      "/* 251 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 252 */         }\n",
      "/* 253 */\n",
      "/* 254 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 255 */         // try to allocate buffer again.\n",
      "/* 256 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 257 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 258 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 259 */           // failed to allocate the first page\n",
      "/* 260 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 261 */         }\n",
      "/* 262 */       }\n",
      "/* 263 */\n",
      "/* 264 */     }\n",
      "/* 265 */\n",
      "/* 266 */     // Updates the proper row buffer\n",
      "/* 267 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 268 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 269 */     }\n",
      "/* 270 */\n",
      "/* 271 */     // common sub-expressions\n",
      "/* 272 */\n",
      "/* 273 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 274 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 275 */\n",
      "/* 276 */   }\n",
      "/* 277 */\n",
      "/* 278 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 279 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 280 */     double hashAgg_value_12 = -1.0;\n",
      "/* 281 */     do {\n",
      "/* 282 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 283 */       double hashAgg_value_13 = -1.0;\n",
      "/* 284 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 285 */       double hashAgg_value_14 = -1.0;\n",
      "/* 286 */       do {\n",
      "/* 287 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 288 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 289 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 290 */         if (!hashAgg_isNull_14) {\n",
      "/* 291 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 292 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 293 */           continue;\n",
      "/* 294 */         }\n",
      "/* 295 */\n",
      "/* 296 */         if (!false) {\n",
      "/* 297 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 298 */           hashAgg_value_14 = 0.0D;\n",
      "/* 299 */           continue;\n",
      "/* 300 */         }\n",
      "/* 301 */\n",
      "/* 302 */       } while (false);\n",
      "/* 303 */\n",
      "/* 304 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 305 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 306 */\n",
      "/* 307 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 308 */\n",
      "/* 309 */       }\n",
      "/* 310 */       if (!hashAgg_isNull_12) {\n",
      "/* 311 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 312 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 313 */         continue;\n",
      "/* 314 */       }\n",
      "/* 315 */\n",
      "/* 316 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 317 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 318 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 319 */       if (!hashAgg_isNull_17) {\n",
      "/* 320 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 321 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 322 */         continue;\n",
      "/* 323 */       }\n",
      "/* 324 */\n",
      "/* 325 */     } while (false);\n",
      "/* 326 */\n",
      "/* 327 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 328 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 331 */     }\n",
      "/* 332 */   }\n",
      "/* 333 */\n",
      "/* 334 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 335 */   throws java.io.IOException {\n",
      "/* 336 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 337 */\n",
      "/* 338 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 339 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 340 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 341 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 342 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 343 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 344 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 345 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 346 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 347 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 348 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 349 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 350 */\n",
      "/* 351 */     project_mutableStateArray_0[4].reset();\n",
      "/* 352 */\n",
      "/* 353 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 354 */\n",
      "/* 355 */     if (hashAgg_isNull_18) {\n",
      "/* 356 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 357 */     } else {\n",
      "/* 358 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 359 */     }\n",
      "/* 360 */\n",
      "/* 361 */     if (hashAgg_isNull_19) {\n",
      "/* 362 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 363 */     } else {\n",
      "/* 364 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 365 */     }\n",
      "/* 366 */\n",
      "/* 367 */     if (hashAgg_isNull_20) {\n",
      "/* 368 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 369 */     } else {\n",
      "/* 370 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 371 */     }\n",
      "/* 372 */\n",
      "/* 373 */     if (hashAgg_isNull_21) {\n",
      "/* 374 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 375 */     } else {\n",
      "/* 376 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 377 */     }\n",
      "/* 378 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 379 */\n",
      "/* 380 */   }\n",
      "/* 381 */\n",
      "/* 382 */   protected void processNext() throws java.io.IOException {\n",
      "/* 383 */     if (!hashAgg_initAgg_0) {\n",
      "/* 384 */       hashAgg_initAgg_0 = true;\n",
      "/* 385 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 386 */\n",
      "/* 387 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 388 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 389 */           @Override\n",
      "/* 390 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 391 */             hashAgg_fastHashMap_0.close();\n",
      "/* 392 */           }\n",
      "/* 393 */         });\n",
      "/* 394 */\n",
      "/* 395 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 396 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 397 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 398 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 399 */     }\n",
      "/* 400 */     // output the result\n",
      "/* 401 */\n",
      "/* 402 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 403 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 404 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 405 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 406 */\n",
      "/* 407 */       if (shouldStop()) return;\n",
      "/* 408 */     }\n",
      "/* 409 */     hashAgg_fastHashMap_0.close();\n",
      "/* 410 */\n",
      "/* 411 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 412 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 413 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 414 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 415 */       if (shouldStop()) return;\n",
      "/* 416 */     }\n",
      "/* 417 */     hashAgg_mapIter_0.close();\n",
      "/* 418 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 419 */       hashAgg_hashMap_0.free();\n",
      "/* 420 */     }\n",
      "/* 421 */   }\n",
      "/* 422 */\n",
      "/* 423 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 88\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3663)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3663\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3663\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2914)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2914\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2914\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2745)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2745\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2745\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3176)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3176\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3176\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3006)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3006\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3006\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3025)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3025\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3025\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2898)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2898\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2898\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(76)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 76\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 76\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 76\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 76\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_76\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_76 of size 360200 dropped from memory (free 379827979)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_76_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_76_piece0 of size 35539 dropped from memory (free 379863518)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_76_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_76_piece0 on macbookpro.lan:57375 in memory (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_76_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_76_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 76, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 76\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3457)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3457\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3457\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2978)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2978\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2978\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3537)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3537\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3537\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3149)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3149\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3149\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3021)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3021\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3021\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(86)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 86\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 86\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 86\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 86\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_86\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_86 of size 50256 dropped from memory (free 379913774)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_86_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_86_piece0 of size 23777 dropped from memory (free 379937551)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_86_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_86_piece0 on macbookpro.lan:57375 in memory (size: 23.2 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_86_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_86_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 86, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 86\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3623)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3623\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3623\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2990)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2990\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2990\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2767)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2767\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2767\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3238)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3238\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3238\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3596)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3596\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3596\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanShuffle(5)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning shuffle 5\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned shuffle 5\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2905)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2905\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2905\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3394)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3394\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3394\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3429)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3429\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing shuffle 5\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3429\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2783)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2783\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2783\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3180)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3180\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3180\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(85)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning broadcast 85\n",
      "25/04/11 09:59:14 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 85\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: removing broadcast 85\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing broadcast 85\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_85\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_85 of size 53856 dropped from memory (free 379991407)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Removing block broadcast_85_piece0\n",
      "25/04/11 09:59:14 DEBUG MemoryStore: Block broadcast_85_piece0 of size 24892 dropped from memory (free 380016299)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_85_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Removed broadcast_85_piece0 on macbookpro.lan:57375 in memory (size: 24.3 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_85_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_85_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing shuffle 5, response is true\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: true to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 85, response is 0\n",
      "25/04/11 09:59:14 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to macbookpro.lan:57372\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned broadcast 85\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3146)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3146\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3146\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3017)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3017\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3017\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3405)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3405\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3405\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2937)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2937\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2937\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3307)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3307\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3307\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3615)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3615\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3615\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3140)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3140\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3140\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2751)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2751\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2751\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2956)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2956\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2956\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3657)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3657\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3657\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3407)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3407\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3407\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3404)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3404\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3404\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3461)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3461\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3461\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3378)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3378\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3378\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3051)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3051\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3051\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3320)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3320\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3320\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3548)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3548\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3548\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2904)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2904\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2904\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3212)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3212\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3212\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3142)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3142\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3142\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3204)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3204\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3204\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3047)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3047\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3047\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2740)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2740\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2740\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3475)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3475\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3475\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3020)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3020\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3020\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2920)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2920\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2920\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3318)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3318\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3318\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3426)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3426\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3426\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3031)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3031\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3031\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2758)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2758\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2758\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3403)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3403\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3403\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2910)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2910\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2910\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3546)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3546\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3546\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2856)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2856\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2856\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3541)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3541\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3541\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3599)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3599\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3599\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2760)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2760\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2760\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3325)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3325\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3325\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3415)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3415\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3415\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3639)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3639\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3639\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2761)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2761\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2761\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3147)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3147\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3147\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3218)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3218\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3218\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3251)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3251\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3251\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2938)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2938\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2938\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2771)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2771\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2771\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2764)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2764\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2764\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(2999)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 2999\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 2999\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Got cleaning task CleanAccum(3301)\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaning accumulator 3301\n",
      "25/04/11 09:59:14 DEBUG ContextCleaner: Cleaned accumulator 3301\n",
      "25/04/11 09:59:14 INFO CodeGenerator: Code generated in 29.970077 ms\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 351.8 KiB, free 362.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_90 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_90 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 362.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_90_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on macbookpro.lan:57375 (size: 34.7 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_90_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_90_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_90_piece0 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_90_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO SparkContext: Created broadcast 90 from showString at <unknown>:0\n",
      "25/04/11 09:59:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------------+\n",
      "|billtype|storeid|   sum(saleamount)|\n",
      "+--------+-------+------------------+\n",
      "| Invoice|    406|            391.82|\n",
      "| Invoice|    402| 4940.400000000001|\n",
      "| Invoice|    404|             431.0|\n",
      "|  Credit|    406|3375.6800000000003|\n",
      "|  Credit|    401|            7889.6|\n",
      "|  Credit|    402|          23673.66|\n",
      "| Invoice|    403|            593.98|\n",
      "|  Credit|    405|1367.6399999999999|\n",
      "|  Credit|    404|          12474.66|\n",
      "| Invoice|    405|           6640.54|\n",
      "|    Paid|    403|           2846.54|\n",
      "|    Paid|    402|1642.9199999999998|\n",
      "|  Credit|    403|            793.12|\n",
      "|    Paid|    404|453.32000000000005|\n",
      "| Invoice|    401|16515.920000000002|\n",
      "|    Paid|    406|           7948.32|\n",
      "|    Paid|    401|           8097.94|\n",
      "|    Paid|    405| 8479.279999999999|\n",
      "+--------+-------+------------------+\n",
      "\n",
      "+----+-------+-----+------------------+\n",
      "|year|quarter|month|   sum(saleamount)|\n",
      "+----+-------+-----+------------------+\n",
      "|NULL|   NULL| NULL|108556.34000000004|\n",
      "+----+-------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:14 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 213 took 0.000027 seconds\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Registering RDD 213 (showString at <unknown>:0) as input to shuffle 9\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Got map stage job 52 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Final stage: ShuffleMapStage 65 (showString at <unknown>:0)\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: submitStage(ShuffleMapStage 65 (name=showString at <unknown>:0;jobs=52))\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[213] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 65)\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 48.4 KiB, free 362.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_91 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_91 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 362.0 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_91_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on macbookpro.lan:57375 (size: 21.8 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_91_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_91_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_91_piece0 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_91_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[213] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:14 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Epoch for TaskSet 65.0: 9\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Valid locality levels for TaskSet 65.0: NO_PREF, ANY\n",
      "25/04/11 09:59:14 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_65.0, runningTasks: 0\n",
      "25/04/11 09:59:14 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 52) (macbookpro.lan, executor driver, partition 0, PROCESS_LOCAL, 9832 bytes) \n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/04/11 09:59:14 INFO Executor: Running task 0.0 in stage 65.0 (TID 52)\n",
      "25/04/11 09:59:14 DEBUG ExecutorMetricsPoller: stageTCMP: (65, 0) -> 1\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Getting local block broadcast_91\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Level for block broadcast_91 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:14 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private boolean hashAgg_bufIsNull_0;\n",
      "/* 011 */   private double hashAgg_bufValue_0;\n",
      "/* 012 */   private hashAgg_FastHashMap_0 hashAgg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> hashAgg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private boolean hashAgg_hashAgg_isNull_11_0;\n",
      "/* 019 */   private boolean hashAgg_hashAgg_isNull_13_0;\n",
      "/* 020 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 021 */\n",
      "/* 022 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 023 */     this.references = references;\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 027 */     partitionIndex = index;\n",
      "/* 028 */     this.inputs = inputs;\n",
      "/* 029 */\n",
      "/* 030 */     inputadapter_input_0 = inputs[0];\n",
      "/* 031 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n",
      "/* 032 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 033 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 034 */     project_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 035 */     project_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 0);\n",
      "/* 036 */\n",
      "/* 037 */   }\n",
      "/* 038 */\n",
      "/* 039 */   public class hashAgg_FastHashMap_0 {\n",
      "/* 040 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 041 */     private int[] buckets;\n",
      "/* 042 */     private int capacity = 1 << 16;\n",
      "/* 043 */     private double loadFactor = 0.5;\n",
      "/* 044 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 045 */     private int maxSteps = 2;\n",
      "/* 046 */     private int numRows = 0;\n",
      "/* 047 */     private Object emptyVBase;\n",
      "/* 048 */     private long emptyVOff;\n",
      "/* 049 */     private int emptyVLen;\n",
      "/* 050 */     private boolean isBatchFull = false;\n",
      "/* 051 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 052 */\n",
      "/* 053 */     public hashAgg_FastHashMap_0(\n",
      "/* 054 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 055 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 056 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 057 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 058 */\n",
      "/* 059 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 060 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 061 */\n",
      "/* 062 */       emptyVBase = emptyBuffer;\n",
      "/* 063 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 064 */       emptyVLen = emptyBuffer.length;\n",
      "/* 065 */\n",
      "/* 066 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 067 */         3, 0);\n",
      "/* 068 */\n",
      "/* 069 */       buckets = new int[numBuckets];\n",
      "/* 070 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 071 */     }\n",
      "/* 072 */\n",
      "/* 073 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 074 */       long h = hash(hashAgg_key_0, hashAgg_key_1, hashAgg_key_2);\n",
      "/* 075 */       int step = 0;\n",
      "/* 076 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 077 */       while (step < maxSteps) {\n",
      "/* 078 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 079 */         if (buckets[idx] == -1) {\n",
      "/* 080 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 081 */             agg_rowWriter.reset();\n",
      "/* 082 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 083 */             agg_rowWriter.write(0, hashAgg_key_0);\n",
      "/* 084 */             agg_rowWriter.write(1, hashAgg_key_1);\n",
      "/* 085 */             agg_rowWriter.write(2, hashAgg_key_2);\n",
      "/* 086 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 087 */             = agg_rowWriter.getRow();\n",
      "/* 088 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 089 */             long koff = agg_result.getBaseOffset();\n",
      "/* 090 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 091 */\n",
      "/* 092 */             UnsafeRow vRow\n",
      "/* 093 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 094 */             if (vRow == null) {\n",
      "/* 095 */               isBatchFull = true;\n",
      "/* 096 */             } else {\n",
      "/* 097 */               buckets[idx] = numRows++;\n",
      "/* 098 */             }\n",
      "/* 099 */             return vRow;\n",
      "/* 100 */           } else {\n",
      "/* 101 */             // No more space\n",
      "/* 102 */             return null;\n",
      "/* 103 */           }\n",
      "/* 104 */         } else if (equals(idx, hashAgg_key_0, hashAgg_key_1, hashAgg_key_2)) {\n",
      "/* 105 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 106 */         }\n",
      "/* 107 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 108 */         step++;\n",
      "/* 109 */       }\n",
      "/* 110 */       // Didn't find it\n",
      "/* 111 */       return null;\n",
      "/* 112 */     }\n",
      "/* 113 */\n",
      "/* 114 */     private boolean equals(int idx, int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 115 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 116 */       return (row.getInt(0) == hashAgg_key_0) && (row.getInt(1) == hashAgg_key_1) && (row.getInt(2) == hashAgg_key_2);\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     private long hash(int hashAgg_key_0, int hashAgg_key_1, int hashAgg_key_2) {\n",
      "/* 120 */       long hashAgg_hash_0 = 0;\n",
      "/* 121 */\n",
      "/* 122 */       int hashAgg_result_0 = hashAgg_key_0;\n",
      "/* 123 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_0 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 124 */\n",
      "/* 125 */       int hashAgg_result_1 = hashAgg_key_1;\n",
      "/* 126 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_1 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 127 */\n",
      "/* 128 */       int hashAgg_result_2 = hashAgg_key_2;\n",
      "/* 129 */       hashAgg_hash_0 = (hashAgg_hash_0 ^ (0x9e3779b9)) + hashAgg_result_2 + (hashAgg_hash_0 << 6) + (hashAgg_hash_0 >>> 2);\n",
      "/* 130 */\n",
      "/* 131 */       return hashAgg_hash_0;\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 135 */       return batch.rowIterator();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */     public void close() {\n",
      "/* 139 */       batch.close();\n",
      "/* 140 */     }\n",
      "/* 141 */\n",
      "/* 142 */   }\n",
      "/* 143 */\n",
      "/* 144 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 145 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 146 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 147 */\n",
      "/* 148 */       // common sub-expressions\n",
      "/* 149 */\n",
      "/* 150 */       boolean project_isNull_1 = true;\n",
      "/* 151 */       long project_value_1 = -1L;\n",
      "/* 152 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 153 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 154 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 155 */       if (!inputadapter_isNull_0) {\n",
      "/* 156 */         project_isNull_1 = false; // resultCode could change nullability.\n",
      "/* 157 */\n",
      "/* 158 */         try {\n",
      "/* 159 */           project_value_1 = ((org.apache.spark.sql.catalyst.util.TimestampFormatter) references[7] /* formatter */).parse(inputadapter_value_0.toString()) / 1;\n",
      "/* 160 */         } catch (java.time.DateTimeException e) {\n",
      "/* 161 */           project_isNull_1 = true;\n",
      "/* 162 */         } catch (java.text.ParseException e) {\n",
      "/* 163 */           project_isNull_1 = true;\n",
      "/* 164 */         }\n",
      "/* 165 */\n",
      "/* 166 */       }\n",
      "/* 167 */       boolean project_isNull_0 = project_isNull_1;\n",
      "/* 168 */       int project_value_0 = -1;\n",
      "/* 169 */       if (!project_isNull_1) {\n",
      "/* 170 */         project_value_0 =\n",
      "/* 171 */         org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(project_value_1, ((java.time.ZoneId) references[9] /* zoneId */));\n",
      "/* 172 */       }\n",
      "/* 173 */\n",
      "/* 174 */       // common sub-expressions\n",
      "/* 175 */\n",
      "/* 176 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 177 */       double inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 178 */       -1.0 : (inputadapter_row_0.getDouble(1));\n",
      "/* 179 */       boolean project_isNull_8 = project_isNull_0;\n",
      "/* 180 */       int project_value_8 = -1;\n",
      "/* 181 */\n",
      "/* 182 */       if (!project_isNull_0) {\n",
      "/* 183 */         project_value_8 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getYear(project_value_0);\n",
      "/* 184 */       }\n",
      "/* 185 */       boolean project_isNull_10 = project_isNull_0;\n",
      "/* 186 */       int project_value_10 = -1;\n",
      "/* 187 */\n",
      "/* 188 */       if (!project_isNull_0) {\n",
      "/* 189 */         project_value_10 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getQuarter(project_value_0);\n",
      "/* 190 */       }\n",
      "/* 191 */       boolean project_isNull_12 = project_isNull_0;\n",
      "/* 192 */       int project_value_12 = -1;\n",
      "/* 193 */\n",
      "/* 194 */       if (!project_isNull_0) {\n",
      "/* 195 */         project_value_12 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getMonth(project_value_0);\n",
      "/* 196 */       }\n",
      "/* 197 */\n",
      "/* 198 */       hashAgg_doConsume_0(inputadapter_value_1, inputadapter_isNull_1, project_value_8, project_isNull_8, project_value_10, project_isNull_10, project_value_12, project_isNull_12);\n",
      "/* 199 */       // shouldStop check is eliminated\n",
      "/* 200 */     }\n",
      "/* 201 */\n",
      "/* 202 */     hashAgg_fastHashMapIter_0 = hashAgg_fastHashMap_0.rowIterator();\n",
      "/* 203 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 204 */\n",
      "/* 205 */   }\n",
      "/* 206 */\n",
      "/* 207 */   private void hashAgg_doConsume_0(double hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, int hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 208 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 209 */     UnsafeRow hashAgg_fastAggBuffer_0 = null;\n",
      "/* 210 */\n",
      "/* 211 */     if (!hashAgg_exprIsNull_1_0 && !hashAgg_exprIsNull_2_0 && !hashAgg_exprIsNull_3_0) {\n",
      "/* 212 */       hashAgg_fastAggBuffer_0 = hashAgg_fastHashMap_0.findOrInsert(\n",
      "/* 213 */         hashAgg_expr_1_0, hashAgg_expr_2_0, hashAgg_expr_3_0);\n",
      "/* 214 */     }\n",
      "/* 215 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 216 */     if (hashAgg_fastAggBuffer_0 == null) {\n",
      "/* 217 */       // generate grouping key\n",
      "/* 218 */       project_mutableStateArray_0[3].reset();\n",
      "/* 219 */\n",
      "/* 220 */       project_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 221 */\n",
      "/* 222 */       if (hashAgg_exprIsNull_1_0) {\n",
      "/* 223 */         project_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 224 */       } else {\n",
      "/* 225 */         project_mutableStateArray_0[3].write(0, hashAgg_expr_1_0);\n",
      "/* 226 */       }\n",
      "/* 227 */\n",
      "/* 228 */       if (hashAgg_exprIsNull_2_0) {\n",
      "/* 229 */         project_mutableStateArray_0[3].setNullAt(1);\n",
      "/* 230 */       } else {\n",
      "/* 231 */         project_mutableStateArray_0[3].write(1, hashAgg_expr_2_0);\n",
      "/* 232 */       }\n",
      "/* 233 */\n",
      "/* 234 */       if (hashAgg_exprIsNull_3_0) {\n",
      "/* 235 */         project_mutableStateArray_0[3].setNullAt(2);\n",
      "/* 236 */       } else {\n",
      "/* 237 */         project_mutableStateArray_0[3].write(2, hashAgg_expr_3_0);\n",
      "/* 238 */       }\n",
      "/* 239 */       int hashAgg_unsafeRowKeyHash_0 = (project_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 240 */       if (true) {\n",
      "/* 241 */         // try to get the buffer from hash map\n",
      "/* 242 */         hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 243 */         hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 244 */       }\n",
      "/* 245 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 246 */       // aggregation after processing all input rows.\n",
      "/* 247 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 248 */         if (hashAgg_sorter_0 == null) {\n",
      "/* 249 */           hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 250 */         } else {\n",
      "/* 251 */           hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 252 */         }\n",
      "/* 253 */\n",
      "/* 254 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 255 */         // try to allocate buffer again.\n",
      "/* 256 */         hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 257 */           (project_mutableStateArray_0[3].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 258 */         if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 259 */           // failed to allocate the first page\n",
      "/* 260 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 261 */         }\n",
      "/* 262 */       }\n",
      "/* 263 */\n",
      "/* 264 */     }\n",
      "/* 265 */\n",
      "/* 266 */     // Updates the proper row buffer\n",
      "/* 267 */     if (hashAgg_fastAggBuffer_0 != null) {\n",
      "/* 268 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_fastAggBuffer_0;\n",
      "/* 269 */     }\n",
      "/* 270 */\n",
      "/* 271 */     // common sub-expressions\n",
      "/* 272 */\n",
      "/* 273 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 274 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_exprIsNull_0_0, hashAgg_expr_0_0);\n",
      "/* 275 */\n",
      "/* 276 */   }\n",
      "/* 277 */\n",
      "/* 278 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, boolean hashAgg_exprIsNull_0_0, double hashAgg_expr_0_0) throws java.io.IOException {\n",
      "/* 279 */     hashAgg_hashAgg_isNull_11_0 = true;\n",
      "/* 280 */     double hashAgg_value_12 = -1.0;\n",
      "/* 281 */     do {\n",
      "/* 282 */       boolean hashAgg_isNull_12 = true;\n",
      "/* 283 */       double hashAgg_value_13 = -1.0;\n",
      "/* 284 */       hashAgg_hashAgg_isNull_13_0 = true;\n",
      "/* 285 */       double hashAgg_value_14 = -1.0;\n",
      "/* 286 */       do {\n",
      "/* 287 */         boolean hashAgg_isNull_14 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 288 */         double hashAgg_value_15 = hashAgg_isNull_14 ?\n",
      "/* 289 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 290 */         if (!hashAgg_isNull_14) {\n",
      "/* 291 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 292 */           hashAgg_value_14 = hashAgg_value_15;\n",
      "/* 293 */           continue;\n",
      "/* 294 */         }\n",
      "/* 295 */\n",
      "/* 296 */         if (!false) {\n",
      "/* 297 */           hashAgg_hashAgg_isNull_13_0 = false;\n",
      "/* 298 */           hashAgg_value_14 = 0.0D;\n",
      "/* 299 */           continue;\n",
      "/* 300 */         }\n",
      "/* 301 */\n",
      "/* 302 */       } while (false);\n",
      "/* 303 */\n",
      "/* 304 */       if (!hashAgg_exprIsNull_0_0) {\n",
      "/* 305 */         hashAgg_isNull_12 = false; // resultCode could change nullability.\n",
      "/* 306 */\n",
      "/* 307 */         hashAgg_value_13 = hashAgg_value_14 + hashAgg_expr_0_0;\n",
      "/* 308 */\n",
      "/* 309 */       }\n",
      "/* 310 */       if (!hashAgg_isNull_12) {\n",
      "/* 311 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 312 */         hashAgg_value_12 = hashAgg_value_13;\n",
      "/* 313 */         continue;\n",
      "/* 314 */       }\n",
      "/* 315 */\n",
      "/* 316 */       boolean hashAgg_isNull_17 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 317 */       double hashAgg_value_18 = hashAgg_isNull_17 ?\n",
      "/* 318 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 319 */       if (!hashAgg_isNull_17) {\n",
      "/* 320 */         hashAgg_hashAgg_isNull_11_0 = false;\n",
      "/* 321 */         hashAgg_value_12 = hashAgg_value_18;\n",
      "/* 322 */         continue;\n",
      "/* 323 */       }\n",
      "/* 324 */\n",
      "/* 325 */     } while (false);\n",
      "/* 326 */\n",
      "/* 327 */     if (!hashAgg_hashAgg_isNull_11_0) {\n",
      "/* 328 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_12);\n",
      "/* 329 */     } else {\n",
      "/* 330 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 331 */     }\n",
      "/* 332 */   }\n",
      "/* 333 */\n",
      "/* 334 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 335 */   throws java.io.IOException {\n",
      "/* 336 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* numOutputRows */).add(1);\n",
      "/* 337 */\n",
      "/* 338 */     boolean hashAgg_isNull_18 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 339 */     int hashAgg_value_19 = hashAgg_isNull_18 ?\n",
      "/* 340 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 341 */     boolean hashAgg_isNull_19 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 342 */     int hashAgg_value_20 = hashAgg_isNull_19 ?\n",
      "/* 343 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 344 */     boolean hashAgg_isNull_20 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 345 */     int hashAgg_value_21 = hashAgg_isNull_20 ?\n",
      "/* 346 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 347 */     boolean hashAgg_isNull_21 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 348 */     double hashAgg_value_22 = hashAgg_isNull_21 ?\n",
      "/* 349 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 350 */\n",
      "/* 351 */     project_mutableStateArray_0[4].reset();\n",
      "/* 352 */\n",
      "/* 353 */     project_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 354 */\n",
      "/* 355 */     if (hashAgg_isNull_18) {\n",
      "/* 356 */       project_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 357 */     } else {\n",
      "/* 358 */       project_mutableStateArray_0[4].write(0, hashAgg_value_19);\n",
      "/* 359 */     }\n",
      "/* 360 */\n",
      "/* 361 */     if (hashAgg_isNull_19) {\n",
      "/* 362 */       project_mutableStateArray_0[4].setNullAt(1);\n",
      "/* 363 */     } else {\n",
      "/* 364 */       project_mutableStateArray_0[4].write(1, hashAgg_value_20);\n",
      "/* 365 */     }\n",
      "/* 366 */\n",
      "/* 367 */     if (hashAgg_isNull_20) {\n",
      "/* 368 */       project_mutableStateArray_0[4].setNullAt(2);\n",
      "/* 369 */     } else {\n",
      "/* 370 */       project_mutableStateArray_0[4].write(2, hashAgg_value_21);\n",
      "/* 371 */     }\n",
      "/* 372 */\n",
      "/* 373 */     if (hashAgg_isNull_21) {\n",
      "/* 374 */       project_mutableStateArray_0[4].setNullAt(3);\n",
      "/* 375 */     } else {\n",
      "/* 376 */       project_mutableStateArray_0[4].write(3, hashAgg_value_22);\n",
      "/* 377 */     }\n",
      "/* 378 */     append((project_mutableStateArray_0[4].getRow()));\n",
      "/* 379 */\n",
      "/* 380 */   }\n",
      "/* 381 */\n",
      "/* 382 */   protected void processNext() throws java.io.IOException {\n",
      "/* 383 */     if (!hashAgg_initAgg_0) {\n",
      "/* 384 */       hashAgg_initAgg_0 = true;\n",
      "/* 385 */       hashAgg_fastHashMap_0 = new hashAgg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 386 */\n",
      "/* 387 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 388 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 389 */           @Override\n",
      "/* 390 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 391 */             hashAgg_fastHashMap_0.close();\n",
      "/* 392 */           }\n",
      "/* 393 */         });\n",
      "/* 394 */\n",
      "/* 395 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 396 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 397 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 398 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[11] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 399 */     }\n",
      "/* 400 */     // output the result\n",
      "/* 401 */\n",
      "/* 402 */     while ( hashAgg_fastHashMapIter_0.next()) {\n",
      "/* 403 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getKey();\n",
      "/* 404 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_fastHashMapIter_0.getValue();\n",
      "/* 405 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 406 */\n",
      "/* 407 */       if (shouldStop()) return;\n",
      "/* 408 */     }\n",
      "/* 409 */     hashAgg_fastHashMap_0.close();\n",
      "/* 410 */\n",
      "/* 411 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 412 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 413 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 414 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 415 */       if (shouldStop()) return;\n",
      "/* 416 */     }\n",
      "/* 417 */     hashAgg_mapIter_0.close();\n",
      "/* 418 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 419 */       hashAgg_hashMap_0.free();\n",
      "/* 420 */     }\n",
      "/* 421 */   }\n",
      "/* 422 */\n",
      "/* 423 */ }\n",
      "\n",
      "25/04/11 09:59:14 INFO CodeGenerator: Code generated in 23.822711 ms\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int, true], input[1, int, true], input[2, int, true], 42), 200):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = false;\n",
      "/* 032 */     int value_0 = -1;\n",
      "/* 033 */     if (200 == 0) {\n",
      "/* 034 */       isNull_0 = true;\n",
      "/* 035 */     } else {\n",
      "/* 036 */       int value_1 = 42;\n",
      "/* 037 */       boolean isNull_2 = i.isNullAt(0);\n",
      "/* 038 */       int value_2 = isNull_2 ?\n",
      "/* 039 */       -1 : (i.getInt(0));\n",
      "/* 040 */       if (!isNull_2) {\n",
      "/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);\n",
      "/* 042 */       }\n",
      "/* 043 */       boolean isNull_3 = i.isNullAt(1);\n",
      "/* 044 */       int value_3 = isNull_3 ?\n",
      "/* 045 */       -1 : (i.getInt(1));\n",
      "/* 046 */       if (!isNull_3) {\n",
      "/* 047 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_3, value_1);\n",
      "/* 048 */       }\n",
      "/* 049 */       boolean isNull_4 = i.isNullAt(2);\n",
      "/* 050 */       int value_4 = isNull_4 ?\n",
      "/* 051 */       -1 : (i.getInt(2));\n",
      "/* 052 */       if (!isNull_4) {\n",
      "/* 053 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_4, value_1);\n",
      "/* 054 */       }\n",
      "/* 055 */\n",
      "/* 056 */       int remainder_0 = value_1 % 200;\n",
      "/* 057 */       if (remainder_0 < 0) {\n",
      "/* 058 */         value_0=(remainder_0 + 200) % 200;\n",
      "/* 059 */       } else {\n",
      "/* 060 */         value_0=remainder_0;\n",
      "/* 061 */       }\n",
      "/* 062 */\n",
      "/* 063 */     }\n",
      "/* 064 */     if (isNull_0) {\n",
      "/* 065 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 068 */     }\n",
      "/* 069 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 070 */   }\n",
      "/* 071 */\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 acquired 1024.0 KiB for org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@62239101\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, int, true],input[2, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     int value_2 = isNull_2 ?\n",
      "/* 051 */     -1 : (i.getInt(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@348ceb7d\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:14 INFO FileScanRDD: Reading File path: file:///Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared/sales_data_prepared.csv, range: 0-4068, partition values: [empty row]\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, string, true],input[1, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     double value_1 = isNull_1 ?\n",
      "/* 042 */     -1.0 : (i.getDouble(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 049 */   }\n",
      "/* 050 */\n",
      "/* 051 */\n",
      "/* 052 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Getting local block broadcast_90\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Level for block broadcast_90 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@348ceb7d\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@348ceb7d\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 release 1024.0 KiB from org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch@62239101\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 52 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@348ceb7d\n",
      "25/04/11 09:59:14 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 52 with length 200\n",
      "25/04/11 09:59:14 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 52: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "25/04/11 09:59:14 INFO Executor: Finished task 0.0 in stage 65.0 (TID 52). 2724 bytes result sent to driver\n",
      "25/04/11 09:59:14 DEBUG ExecutorMetricsPoller: stageTCMP: (65, 0) -> 0\n",
      "25/04/11 09:59:14 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 52) in 61 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:14 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: ShuffleMapTask finished on driver\n",
      "25/04/11 09:59:14 INFO DAGScheduler: ShuffleMapStage 65 (showString at <unknown>:0) finished in 0.064 s\n",
      "25/04/11 09:59:14 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/04/11 09:59:14 INFO DAGScheduler: running: Set()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: waiting: Set()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: failed: Set()\n",
      "25/04/11 09:59:14 DEBUG MapOutputTrackerMaster: Increasing epoch to 10\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: After removal of stage 65, remaining stages = 0\n",
      "25/04/11 09:59:14 DEBUG LogicalQueryStage: Physical stats available as Statistics(sizeInBytes=40.0 B, rowCount=1) for plan: HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[sum(saleamount#1043)], output=[toprettystring(year)#1264, toprettystring(quarter)#1265, toprettystring(month)#1266, toprettystring(sum(saleamount))#1267])\n",
      "+- ShuffleQueryStage 0\n",
      "   +- Exchange hashpartitioning(year#1206, quarter#1217, month#1229, 200), ENSURE_REQUIREMENTS, [plan_id=1254]\n",
      "      +- *(1) HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[partial_sum(saleamount#1043)], output=[year#1206, quarter#1217, month#1229, sum#1273])\n",
      "         +- *(1) Project [saleamount#1043, year(saledate#1162) AS year#1206, quarter(saledate#1162) AS quarter#1217, month(saledate#1162) AS month#1229]\n",
      "            +- *(1) Project [cast(gettimestamp(saledate#1038, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#1162, saleamount#1043]\n",
      "               +- FileScan csv [saledate#1038,saleamount#1043] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:59:14 DEBUG AdaptiveSparkPlanExec: Plan changed:\n",
      "!CollectLimit 21                                                                                                                                                                                                                                                                                                                  HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[sum(saleamount#1043)], output=[toprettystring(year)#1264, toprettystring(quarter)#1265, toprettystring(month)#1266, toprettystring(sum(saleamount))#1267])\n",
      "!+- HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[sum(saleamount#1043)], output=[toprettystring(year)#1264, toprettystring(quarter)#1265, toprettystring(month)#1266, toprettystring(sum(saleamount))#1267])                                                                                               +- ShuffleQueryStage 0\n",
      "!   +- ShuffleQueryStage 0                                                                                                                                                                                                                                                                                                           +- Exchange hashpartitioning(year#1206, quarter#1217, month#1229, 200), ENSURE_REQUIREMENTS, [plan_id=1254]\n",
      "!      +- Exchange hashpartitioning(year#1206, quarter#1217, month#1229, 200), ENSURE_REQUIREMENTS, [plan_id=1254]                                                                                                                                                                                                                      +- *(1) HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[partial_sum(saleamount#1043)], output=[year#1206, quarter#1217, month#1229, sum#1273])\n",
      "!         +- *(1) HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[partial_sum(saleamount#1043)], output=[year#1206, quarter#1217, month#1229, sum#1273])                                                                                                                                                              +- *(1) Project [saleamount#1043, year(saledate#1162) AS year#1206, quarter(saledate#1162) AS quarter#1217, month(saledate#1162) AS month#1229]\n",
      "!            +- *(1) Project [saleamount#1043, year(saledate#1162) AS year#1206, quarter(saledate#1162) AS quarter#1217, month(saledate#1162) AS month#1229]                                                                                                                                                                                  +- *(1) Project [cast(gettimestamp(saledate#1038, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#1162, saleamount#1043]\n",
      "!               +- *(1) Project [cast(gettimestamp(saledate#1038, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#1162, saleamount#1043]                                                                                                                                                                           +- FileScan csv [saledate#1038,saleamount#1043] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "!                  +- FileScan csv [saledate#1038,saleamount#1043] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>   \n",
      "25/04/11 09:59:14 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/04/11 09:59:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/04/11 09:59:14 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean hashAgg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private boolean hashAgg_hashAgg_isNull_6_0;\n",
      "/* 015 */   private boolean hashAgg_hashAgg_isNull_8_0;\n",
      "/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 017 */\n",
      "/* 018 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 019 */     this.references = references;\n",
      "/* 020 */   }\n",
      "/* 021 */\n",
      "/* 022 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 023 */     partitionIndex = index;\n",
      "/* 024 */     this.inputs = inputs;\n",
      "/* 025 */\n",
      "/* 026 */     inputadapter_input_0 = inputs[0];\n",
      "/* 027 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 028 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 128);\n",
      "/* 029 */\n",
      "/* 030 */   }\n",
      "/* 031 */\n",
      "/* 032 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 033 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 035 */\n",
      "/* 036 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 037 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 038 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       int inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       -1 : (inputadapter_row_0.getInt(1));\n",
      "/* 042 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 043 */       int inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 044 */       -1 : (inputadapter_row_0.getInt(2));\n",
      "/* 045 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n",
      "/* 046 */       double inputadapter_value_3 = inputadapter_isNull_3 ?\n",
      "/* 047 */       -1.0 : (inputadapter_row_0.getDouble(3));\n",
      "/* 048 */\n",
      "/* 049 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2, inputadapter_value_3, inputadapter_isNull_3);\n",
      "/* 050 */       // shouldStop check is eliminated\n",
      "/* 051 */     }\n",
      "/* 052 */\n",
      "/* 053 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 054 */   }\n",
      "/* 055 */\n",
      "/* 056 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0, boolean hashAgg_exprIsNull_0_0, int hashAgg_expr_1_0, boolean hashAgg_exprIsNull_1_0, int hashAgg_expr_2_0, boolean hashAgg_exprIsNull_2_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 057 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;\n",
      "/* 058 */\n",
      "/* 059 */     // generate grouping key\n",
      "/* 060 */     hashAgg_mutableStateArray_0[0].reset();\n",
      "/* 061 */\n",
      "/* 062 */     hashAgg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 063 */\n",
      "/* 064 */     if (hashAgg_exprIsNull_0_0) {\n",
      "/* 065 */       hashAgg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     if (hashAgg_exprIsNull_1_0) {\n",
      "/* 071 */       hashAgg_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 072 */     } else {\n",
      "/* 073 */       hashAgg_mutableStateArray_0[0].write(1, hashAgg_expr_1_0);\n",
      "/* 074 */     }\n",
      "/* 075 */\n",
      "/* 076 */     if (hashAgg_exprIsNull_2_0) {\n",
      "/* 077 */       hashAgg_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 078 */     } else {\n",
      "/* 079 */       hashAgg_mutableStateArray_0[0].write(2, hashAgg_expr_2_0);\n",
      "/* 080 */     }\n",
      "/* 081 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 082 */     if (true) {\n",
      "/* 083 */       // try to get the buffer from hash map\n",
      "/* 084 */       hashAgg_unsafeRowAggBuffer_0 =\n",
      "/* 085 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 086 */     }\n",
      "/* 087 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 088 */     // aggregation after processing all input rows.\n",
      "/* 089 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 090 */       if (hashAgg_sorter_0 == null) {\n",
      "/* 091 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 092 */       } else {\n",
      "/* 093 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 094 */       }\n",
      "/* 095 */\n",
      "/* 096 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 097 */       // try to allocate buffer again.\n",
      "/* 098 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 099 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);\n",
      "/* 100 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 101 */         // failed to allocate the first page\n",
      "/* 102 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 103 */       }\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */     // common sub-expressions\n",
      "/* 107 */\n",
      "/* 108 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 109 */     hashAgg_doAggregate_sum_0(hashAgg_unsafeRowAggBuffer_0, hashAgg_expr_3_0, hashAgg_exprIsNull_3_0);\n",
      "/* 110 */\n",
      "/* 111 */   }\n",
      "/* 112 */\n",
      "/* 113 */   private void hashAgg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow hashAgg_unsafeRowAggBuffer_0, double hashAgg_expr_3_0, boolean hashAgg_exprIsNull_3_0) throws java.io.IOException {\n",
      "/* 114 */     hashAgg_hashAgg_isNull_6_0 = true;\n",
      "/* 115 */     double hashAgg_value_6 = -1.0;\n",
      "/* 116 */     do {\n",
      "/* 117 */       boolean hashAgg_isNull_7 = true;\n",
      "/* 118 */       double hashAgg_value_7 = -1.0;\n",
      "/* 119 */       hashAgg_hashAgg_isNull_8_0 = true;\n",
      "/* 120 */       double hashAgg_value_8 = -1.0;\n",
      "/* 121 */       do {\n",
      "/* 122 */         boolean hashAgg_isNull_9 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 123 */         double hashAgg_value_9 = hashAgg_isNull_9 ?\n",
      "/* 124 */         -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 125 */         if (!hashAgg_isNull_9) {\n",
      "/* 126 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 127 */           hashAgg_value_8 = hashAgg_value_9;\n",
      "/* 128 */           continue;\n",
      "/* 129 */         }\n",
      "/* 130 */\n",
      "/* 131 */         if (!false) {\n",
      "/* 132 */           hashAgg_hashAgg_isNull_8_0 = false;\n",
      "/* 133 */           hashAgg_value_8 = 0.0D;\n",
      "/* 134 */           continue;\n",
      "/* 135 */         }\n",
      "/* 136 */\n",
      "/* 137 */       } while (false);\n",
      "/* 138 */\n",
      "/* 139 */       if (!hashAgg_exprIsNull_3_0) {\n",
      "/* 140 */         hashAgg_isNull_7 = false; // resultCode could change nullability.\n",
      "/* 141 */\n",
      "/* 142 */         hashAgg_value_7 = hashAgg_value_8 + hashAgg_expr_3_0;\n",
      "/* 143 */\n",
      "/* 144 */       }\n",
      "/* 145 */       if (!hashAgg_isNull_7) {\n",
      "/* 146 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 147 */         hashAgg_value_6 = hashAgg_value_7;\n",
      "/* 148 */         continue;\n",
      "/* 149 */       }\n",
      "/* 150 */\n",
      "/* 151 */       boolean hashAgg_isNull_12 = hashAgg_unsafeRowAggBuffer_0.isNullAt(0);\n",
      "/* 152 */       double hashAgg_value_12 = hashAgg_isNull_12 ?\n",
      "/* 153 */       -1.0 : (hashAgg_unsafeRowAggBuffer_0.getDouble(0));\n",
      "/* 154 */       if (!hashAgg_isNull_12) {\n",
      "/* 155 */         hashAgg_hashAgg_isNull_6_0 = false;\n",
      "/* 156 */         hashAgg_value_6 = hashAgg_value_12;\n",
      "/* 157 */         continue;\n",
      "/* 158 */       }\n",
      "/* 159 */\n",
      "/* 160 */     } while (false);\n",
      "/* 161 */\n",
      "/* 162 */     if (!hashAgg_hashAgg_isNull_6_0) {\n",
      "/* 163 */       hashAgg_unsafeRowAggBuffer_0.setDouble(0, hashAgg_value_6);\n",
      "/* 164 */     } else {\n",
      "/* 165 */       hashAgg_unsafeRowAggBuffer_0.setNullAt(0);\n",
      "/* 166 */     }\n",
      "/* 167 */   }\n",
      "/* 168 */\n",
      "/* 169 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)\n",
      "/* 170 */   throws java.io.IOException {\n",
      "/* 171 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 172 */\n",
      "/* 173 */     boolean hashAgg_isNull_13 = hashAgg_keyTerm_0.isNullAt(0);\n",
      "/* 174 */     int hashAgg_value_13 = hashAgg_isNull_13 ?\n",
      "/* 175 */     -1 : (hashAgg_keyTerm_0.getInt(0));\n",
      "/* 176 */     boolean hashAgg_isNull_14 = hashAgg_keyTerm_0.isNullAt(1);\n",
      "/* 177 */     int hashAgg_value_14 = hashAgg_isNull_14 ?\n",
      "/* 178 */     -1 : (hashAgg_keyTerm_0.getInt(1));\n",
      "/* 179 */     boolean hashAgg_isNull_15 = hashAgg_keyTerm_0.isNullAt(2);\n",
      "/* 180 */     int hashAgg_value_15 = hashAgg_isNull_15 ?\n",
      "/* 181 */     -1 : (hashAgg_keyTerm_0.getInt(2));\n",
      "/* 182 */     boolean hashAgg_isNull_16 = hashAgg_bufferTerm_0.isNullAt(0);\n",
      "/* 183 */     double hashAgg_value_16 = hashAgg_isNull_16 ?\n",
      "/* 184 */     -1.0 : (hashAgg_bufferTerm_0.getDouble(0));\n",
      "/* 185 */\n",
      "/* 186 */     UTF8String hashAgg_value_18;\n",
      "/* 187 */     if (hashAgg_isNull_13) {\n",
      "/* 188 */       hashAgg_value_18 = UTF8String.fromString(\"NULL\");\n",
      "/* 189 */     } else {\n",
      "/* 190 */       hashAgg_value_18 = UTF8String.fromString(String.valueOf(hashAgg_value_13));\n",
      "/* 191 */     }\n",
      "/* 192 */     UTF8String hashAgg_value_20;\n",
      "/* 193 */     if (hashAgg_isNull_14) {\n",
      "/* 194 */       hashAgg_value_20 = UTF8String.fromString(\"NULL\");\n",
      "/* 195 */     } else {\n",
      "/* 196 */       hashAgg_value_20 = UTF8String.fromString(String.valueOf(hashAgg_value_14));\n",
      "/* 197 */     }\n",
      "/* 198 */     UTF8String hashAgg_value_22;\n",
      "/* 199 */     if (hashAgg_isNull_15) {\n",
      "/* 200 */       hashAgg_value_22 = UTF8String.fromString(\"NULL\");\n",
      "/* 201 */     } else {\n",
      "/* 202 */       hashAgg_value_22 = UTF8String.fromString(String.valueOf(hashAgg_value_15));\n",
      "/* 203 */     }\n",
      "/* 204 */     UTF8String hashAgg_value_24;\n",
      "/* 205 */     if (hashAgg_isNull_16) {\n",
      "/* 206 */       hashAgg_value_24 = UTF8String.fromString(\"NULL\");\n",
      "/* 207 */     } else {\n",
      "/* 208 */       hashAgg_value_24 = UTF8String.fromString(String.valueOf(hashAgg_value_16));\n",
      "/* 209 */     }\n",
      "/* 210 */     hashAgg_mutableStateArray_0[1].reset();\n",
      "/* 211 */\n",
      "/* 212 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_18);\n",
      "/* 213 */\n",
      "/* 214 */     hashAgg_mutableStateArray_0[1].write(1, hashAgg_value_20);\n",
      "/* 215 */\n",
      "/* 216 */     hashAgg_mutableStateArray_0[1].write(2, hashAgg_value_22);\n",
      "/* 217 */\n",
      "/* 218 */     hashAgg_mutableStateArray_0[1].write(3, hashAgg_value_24);\n",
      "/* 219 */     append((hashAgg_mutableStateArray_0[1].getRow()));\n",
      "/* 220 */\n",
      "/* 221 */   }\n",
      "/* 222 */\n",
      "/* 223 */   protected void processNext() throws java.io.IOException {\n",
      "/* 224 */     if (!hashAgg_initAgg_0) {\n",
      "/* 225 */       hashAgg_initAgg_0 = true;\n",
      "/* 226 */\n",
      "/* 227 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 228 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 229 */       hashAgg_doAggregateWithKeys_0();\n",
      "/* 230 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 231 */     }\n",
      "/* 232 */     // output the result\n",
      "/* 233 */\n",
      "/* 234 */     while ( hashAgg_mapIter_0.next()) {\n",
      "/* 235 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();\n",
      "/* 236 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();\n",
      "/* 237 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);\n",
      "/* 238 */       if (shouldStop()) return;\n",
      "/* 239 */     }\n",
      "/* 240 */     hashAgg_mapIter_0.close();\n",
      "/* 241 */     if (hashAgg_sorter_0 == null) {\n",
      "/* 242 */       hashAgg_hashMap_0.free();\n",
      "/* 243 */     }\n",
      "/* 244 */   }\n",
      "/* 245 */\n",
      "/* 246 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/04/11 09:59:14 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/04/11 09:59:14 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 216 took 0.000038 seconds\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Got job 53 (showString at <unknown>:0) with 1 output partitions\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Final stage: ResultStage 67 (showString at <unknown>:0)\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: submitStage(ResultStage 67 (name=showString at <unknown>:0;jobs=53))\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: missing: List()\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[216] at showString at <unknown>:0), which has no missing parents\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: submitMissingTasks(ResultStage 67)\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 50.0 KiB, free 361.9 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_92 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_92 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 361.9 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_92_piece0 for BlockManagerId(driver, macbookpro.lan, 57375, None)\n",
      "25/04/11 09:59:14 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on macbookpro.lan:57375 (size: 22.5 KiB, free: 366.1 MiB)\n",
      "25/04/11 09:59:14 DEBUG BlockManagerMaster: Updated info of block broadcast_92_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Told master about block broadcast_92_piece0\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Put block broadcast_92_piece0 locally took 0 ms\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Putting block broadcast_92_piece0 without replication took 0 ms\n",
      "25/04/11 09:59:14 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1585\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[216] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/04/11 09:59:14 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Epoch for TaskSet 67.0: 10\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: Valid locality levels for TaskSet 67.0: NODE_LOCAL, ANY\n",
      "25/04/11 09:59:14 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_67.0, runningTasks: 0\n",
      "25/04/11 09:59:14 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 53) (macbookpro.lan, executor driver, partition 0, NODE_LOCAL, 9184 bytes) \n",
      "25/04/11 09:59:14 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY\n",
      "25/04/11 09:59:14 INFO Executor: Running task 0.0 in stage 67.0 (TID 53)\n",
      "25/04/11 09:59:14 DEBUG ExecutorMetricsPoller: stageTCMP: (67, 0) -> 1\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Getting local block broadcast_92\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Level for block broadcast_92 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/04/11 09:59:14 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 9\n",
      "25/04/11 09:59:14 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 9, mappers 0-1, partitions 0-200\n",
      "25/04/11 09:59:14 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647\n",
      "25/04/11 09:59:14 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/04/11 09:59:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/04/11 09:59:14 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_9_52_42_43,0)\n",
      "25/04/11 09:59:14 DEBUG BlockManager: Getting local shuffle block shuffle_9_52_42_43\n",
      "25/04/11 09:59:14 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 0 ms\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for null:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     if (true) {\n",
      "/* 033 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 034 */     } else {\n",
      "/* 035 */       mutableStateArray_0[0].write(0, -1.0);\n",
      "/* 036 */     }\n",
      "/* 037 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 038 */   }\n",
      "/* 039 */\n",
      "/* 040 */\n",
      "/* 041 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, int, true],input[2, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     int value_1 = isNull_1 ?\n",
      "/* 042 */     -1 : (i.getInt(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     int value_2 = isNull_2 ?\n",
      "/* 051 */     -1 : (i.getInt(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 53 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@7a3b787f\n",
      "25/04/11 09:59:14 DEBUG GenerateUnsafeProjection: code for input[0, double, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     double value_0 = isNull_0 ?\n",
      "/* 033 */     -1.0 : (i.getDouble(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 53 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@7a3b787f\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 53 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@7a3b787f\n",
      "25/04/11 09:59:14 DEBUG TaskMemoryManager: Task 53 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@7a3b787f\n",
      "25/04/11 09:59:14 INFO Executor: Finished task 0.0 in stage 67.0 (TID 53). 5162 bytes result sent to driver\n",
      "25/04/11 09:59:14 DEBUG ExecutorMetricsPoller: stageTCMP: (67, 0) -> 0\n",
      "25/04/11 09:59:14 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 53) in 9 ms on macbookpro.lan (executor driver) (1/1)\n",
      "25/04/11 09:59:14 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "25/04/11 09:59:14 INFO DAGScheduler: ResultStage 67 (showString at <unknown>:0) finished in 0.013 s\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: After removal of stage 67, remaining stages = 1\n",
      "25/04/11 09:59:14 DEBUG DAGScheduler: After removal of stage 66, remaining stages = 0\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/04/11 09:59:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished\n",
      "25/04/11 09:59:14 INFO DAGScheduler: Job 53 finished: showString at <unknown>:0, took 0.015028 s\n",
      "25/04/11 09:59:14 DEBUG AdaptiveSparkPlanExec: Final plan:\n",
      "*(2) HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[sum(saleamount#1043)], output=[toprettystring(year)#1264, toprettystring(quarter)#1265, toprettystring(month)#1266, toprettystring(sum(saleamount))#1267])\n",
      "+- AQEShuffleRead coalesced\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange hashpartitioning(year#1206, quarter#1217, month#1229, 200), ENSURE_REQUIREMENTS, [plan_id=1254]\n",
      "         +- *(1) HashAggregate(keys=[year#1206, quarter#1217, month#1229], functions=[partial_sum(saleamount#1043)], output=[year#1206, quarter#1217, month#1229, sum#1273])\n",
      "            +- *(1) Project [saleamount#1043, year(saledate#1162) AS year#1206, quarter(saledate#1162) AS quarter#1217, month(saledate#1162) AS month#1229]\n",
      "               +- *(1) Project [cast(gettimestamp(saledate#1038, yyyy-MM-dd, TimestampType, Some(America/Chicago), false) as date) AS saledate#1162, saleamount#1043]\n",
      "                  +- FileScan csv [saledate#1038,saleamount#1043] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/craigwilcox/Projects/smart-store-craigwilcox/Data/prepared..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<saledate:string,saleamount:double>\n",
      "\n",
      "25/04/11 09:59:14 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, input[3, string, false].toString, StructField(toprettystring(year),StringType,false), StructField(toprettystring(quarter),StringType,false), StructField(toprettystring(month),StringType,false), StructField(toprettystring(sum(saleamount)),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[4];\n",
      "/* 024 */     createExternalRow_0_0(i, values_0);\n",
      "/* 025 */     createExternalRow_0_1(i, values_0);\n",
      "/* 026 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 027 */     if (false) {\n",
      "/* 028 */       mutableRow.setNullAt(0);\n",
      "/* 029 */     } else {\n",
      "/* 030 */\n",
      "/* 031 */       mutableRow.update(0, value_0);\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     return mutableRow;\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */\n",
      "/* 038 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {\n",
      "/* 039 */\n",
      "/* 040 */     UTF8String value_8 = i.getUTF8String(3);\n",
      "/* 041 */     boolean isNull_7 = true;\n",
      "/* 042 */     java.lang.String value_7 = null;\n",
      "/* 043 */     isNull_7 = false;\n",
      "/* 044 */     if (!isNull_7) {\n",
      "/* 045 */\n",
      "/* 046 */       Object funcResult_3 = null;\n",
      "/* 047 */       funcResult_3 = value_8.toString();\n",
      "/* 048 */       value_7 = (java.lang.String) funcResult_3;\n",
      "/* 049 */\n",
      "/* 050 */     }\n",
      "/* 051 */     if (isNull_7) {\n",
      "/* 052 */       values_0[3] = null;\n",
      "/* 053 */     } else {\n",
      "/* 054 */       values_0[3] = value_7;\n",
      "/* 055 */     }\n",
      "/* 056 */\n",
      "/* 057 */   }\n",
      "/* 058 */\n",
      "/* 059 */\n",
      "/* 060 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {\n",
      "/* 061 */\n",
      "/* 062 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 063 */     boolean isNull_1 = true;\n",
      "/* 064 */     java.lang.String value_1 = null;\n",
      "/* 065 */     isNull_1 = false;\n",
      "/* 066 */     if (!isNull_1) {\n",
      "/* 067 */\n",
      "/* 068 */       Object funcResult_0 = null;\n",
      "/* 069 */       funcResult_0 = value_2.toString();\n",
      "/* 070 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 071 */\n",
      "/* 072 */     }\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       values_0[0] = null;\n",
      "/* 075 */     } else {\n",
      "/* 076 */       values_0[0] = value_1;\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 080 */     boolean isNull_3 = true;\n",
      "/* 081 */     java.lang.String value_3 = null;\n",
      "/* 082 */     isNull_3 = false;\n",
      "/* 083 */     if (!isNull_3) {\n",
      "/* 084 */\n",
      "/* 085 */       Object funcResult_1 = null;\n",
      "/* 086 */       funcResult_1 = value_4.toString();\n",
      "/* 087 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 088 */\n",
      "/* 089 */     }\n",
      "/* 090 */     if (isNull_3) {\n",
      "/* 091 */       values_0[1] = null;\n",
      "/* 092 */     } else {\n",
      "/* 093 */       values_0[1] = value_3;\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 097 */     boolean isNull_5 = true;\n",
      "/* 098 */     java.lang.String value_5 = null;\n",
      "/* 099 */     isNull_5 = false;\n",
      "/* 100 */     if (!isNull_5) {\n",
      "/* 101 */\n",
      "/* 102 */       Object funcResult_2 = null;\n",
      "/* 103 */       funcResult_2 = value_6.toString();\n",
      "/* 104 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 105 */\n",
      "/* 106 */     }\n",
      "/* 107 */     if (isNull_5) {\n",
      "/* 108 */       values_0[2] = null;\n",
      "/* 109 */     } else {\n",
      "/* 110 */       values_0[2] = value_5;\n",
      "/* 111 */     }\n",
      "/* 112 */\n",
      "/* 113 */   }\n",
      "/* 114 */\n",
      "/* 115 */ }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year, quarter, month\n",
    "\n",
    "# Step 1: Ensure the `saledate` is in date format if it's a string\n",
    "df_sales = df_sales.withColumn(\"saledate\", to_date(col(\"saledate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Step 2: Filter the data (Make sure `saledate` is in date format for comparison)\n",
    "df_filtered = df_sales.filter(df_sales.saledate >= \"2023-01-01\")\n",
    "\n",
    "# Step 3: Group by `billtype` and `storeid`, sum `amount`\n",
    "df_sales.groupby(\"billtype\", \"storeid\").sum(\"saleamount\").show()\n",
    "\n",
    "# Step 4: Extract `year`, `quarter`, and `month` from `saledate`\n",
    "df_sales = df_sales.withColumn(\"year\", year(df_sales.saledate))\n",
    "df_sales = df_sales.withColumn(\"quarter\", quarter(df_sales.saledate))\n",
    "df_sales = df_sales.withColumn(\"month\", month(df_sales.saledate))\n",
    "\n",
    "# Step 5: Group by `year`, `quarter`, and `month`, sum `saleamount`\n",
    "df_sales.groupby(\"year\", \"quarter\", \"month\").sum(\"saleamount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98a648b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Name', 'total_spent'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_top_customers_pd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df48d5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (50, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (67, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (52, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (61, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (54, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (53, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (56, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (65, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (47, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (45, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (49, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (46, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (64, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (48, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (58, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (44, 0) from stageTCMP\n",
      "25/04/11 09:59:15 DEBUG ExecutorMetricsPoller: removing (62, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "# All imports at the top, organized\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.barplot(data=df_top_customers_pd, x=\"Name\", y=\"total_spent\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
